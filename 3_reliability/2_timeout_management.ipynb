{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f903b3",
   "metadata": {},
   "source": [
    "# Reliability: Timeout Management\n",
    "\n",
    "----\n",
    "\n",
    "This notebook focuses on **timeout strategies** for LLM applications.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "- **TTFT vs Full Response**: Different timeout types for different needs\n",
    "- **Streaming Timeouts**: Handle long-running streaming responses\n",
    "- **Adaptive Timeouts**: Adjust based on request complexity\n",
    "- **Timeout Recovery**: Handle timeouts gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0602d8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Understanding LLM Timeouts](#understanding-llm-timeouts)\n",
    "- [Setup](#setup)\n",
    "- [Part 1: TTFT vs Full Response Timeout](#part-1-ttft-vs-full-response-timeout)\n",
    "- [Part 2: Streaming Timeout Management](#part-2-streaming-timeout-management)\n",
    "- [Part 3: Adaptive Timeout Strategies](#part-3-adaptive-timeout-strategies)\n",
    "- [Part 4: Timeout Recovery](#part-4-timeout-recovery)\n",
    "- [Part 5: Production Timeout Manager](#part-5-production-timeout-manager)\n",
    "- [Best Practices Summary](#best-practices-summary)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209612e5",
   "metadata": {},
   "source": [
    "## Understanding LLM Timeouts\n",
    "\n",
    "### Types of Timeouts\n",
    "\n",
    "```\n",
    "Request Timeline:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â”‚ Connection â”‚     TTFT     â”‚            Token Generation            â”‚\n",
    "â”‚  Timeout   â”‚   Timeout    â”‚              (Streaming)               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   ~2s      â”‚    ~10s      â”‚           ~30-120s (varies)            â”‚\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        â–²              â–²                    â–²\n",
    "     Connect       First Token         Last Token\n",
    "```\n",
    "\n",
    "| Timeout Type | Description | Typical Value |\n",
    "|--------------|-------------|---------------|\n",
    "| **Connection** | Time to establish connection | 2-5s |\n",
    "| **TTFT** | Time to First Token | 5-30s |\n",
    "| **Read/Chunk** | Time between tokens | 10-30s |\n",
    "| **Total** | End-to-end timeout | 60-300s |\n",
    "\n",
    "### Why TTFT Matters\n",
    "\n",
    "- **User Experience**: Users expect quick feedback\n",
    "- **Early Failure Detection**: Know quickly if request is stuck\n",
    "- **Resource Efficiency**: Don't wait forever for failed requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754962d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3b1b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Azure CLI found: /anaconda/envs/azureml_py38/bin//az\n",
      "âœ… PATH looks good already\n",
      "\n",
      "PATH (first 150 chars): /anaconda/envs/azureml_py38/bin/:/afh/code/agent-operator-lab/.venv/bin:/home/azureuser/.vscode-server/cli/servers/Stable-c9d77990917f3102ada88be140d2...\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Callable, Generator\n",
    "from dataclasses import dataclass, field\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'ðŸ” Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"âœ… Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('âœ… PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ab19a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded settings from '../0_setup/.foundry_config.json'.\n",
      "\n",
      "ðŸ“Œ Foundry name: foundry-rq90gs\n",
      "ðŸ“Œ Resource group: foundry-rg\n",
      "ðŸ“Œ Location: swedencentral\n",
      "ðŸ“Œ Azure OpenAI endpoint: https://foundry-rq90gs.cognitiveservices.azure.com/\n",
      "ðŸ“Œ Chat deployment: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸ Could not find '{config_file}'.\")\n",
    "    print('ðŸ’¡ Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "# Project variables from config\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "\n",
    "# Azure OpenAI variables from env\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"âœ… Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nðŸ“Œ Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"ðŸ“Œ Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"ðŸ“Œ Location: {LOCATION}\")\n",
    "print(f\"ðŸ“Œ Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"ðŸ“Œ Chat deployment: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aca22a",
   "metadata": {},
   "source": [
    "## Part 1: TTFT vs Full Response Timeout with TimeoutInjector\n",
    "\n",
    "Understand and implement different timeout types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c9b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š TTFT Measurement Demo\n",
      "============================================================\n",
      "\n",
      "âš™ï¸ Timeout Configuration:\n",
      "   Connect: 5.0s\n",
      "   TTFT:    3.0s\n",
      "   Read:    10.0s\n",
      "   Total:   15.0s\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸŸ¢ Test 1: Normal request (no injection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TTFT: 763ms\n",
      "   Total: 764ms\n",
      "   Timed out: False\n",
      "   Response: Hello\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”¥ Test 2: Injecting 4s TTFT delay (timeout=3s)\n",
      "      ðŸ’¤ Injecting 4.0s TTFT delay...\n",
      "   TTFT: 4532ms\n",
      "   Total: 4532ms\n",
      "   Timed out: True\n",
      "   Timeout phase: ttft\n",
      "   Response: Hello\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”¥ Test 3: Stall after 2 chunks (total_timeout=15s)\n",
      "      ðŸ”¥ Hanging after 2 chunks (simulating stall)...\n",
      "   âš ï¸ Test killed by thread timeout (as expected)\n",
      "   âœ… Demonstrates that without proper read timeout, request would hang forever\n"
     ]
    }
   ],
   "source": [
    "# TTFT vs Total Response Timeout\n",
    "from openai import AzureOpenAI\n",
    "import httpx\n",
    "import threading\n",
    "\n",
    "@dataclass\n",
    "class TimeoutConfig:\n",
    "    \"\"\"Configuration for different timeout types.\"\"\"\n",
    "    connect_timeout: float = 5.0    # Time to establish connection\n",
    "    ttft_timeout: float = 30.0      # Time to first token\n",
    "    read_timeout: float = 60.0      # Time between chunks\n",
    "    total_timeout: float = 300.0    # Total request timeout\n",
    "\n",
    "@dataclass\n",
    "class TimingResult:\n",
    "    \"\"\"Timing information for a request.\"\"\"\n",
    "    connect_time: Optional[float] = None\n",
    "    ttft: Optional[float] = None\n",
    "    total_time: Optional[float] = None\n",
    "    token_count: int = 0\n",
    "    timed_out: bool = False\n",
    "    timeout_phase: Optional[str] = None\n",
    "    response: Optional[str] = None\n",
    "\n",
    "def create_client_with_timeouts(config: TimeoutConfig) -> AzureOpenAI:\n",
    "    \"\"\"Create Azure OpenAI client with custom timeouts.\"\"\"\n",
    "    http_client = httpx.Client(\n",
    "        timeout=httpx.Timeout(\n",
    "            connect=config.connect_timeout,\n",
    "            read=config.read_timeout,\n",
    "            write=30.0,\n",
    "            pool=None,\n",
    "        )\n",
    "    )\n",
    "    return AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "        http_client=http_client,\n",
    "    )\n",
    "\n",
    "\n",
    "# â”€â”€ Timeout Injector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Simulates delayed responses to trigger timeout scenarios.\n",
    "\n",
    "class TimeoutInjector:\n",
    "    \"\"\"\n",
    "    Wraps an API call and injects artificial delays\n",
    "    to simulate TTFT, chunk, or total timeouts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        real_func: Callable,\n",
    "        delay_before_first: float = 0.0,   # Delay before TTFT\n",
    "        delay_between_chunks: float = 0.0, # Delay between chunks\n",
    "        fail_after_chunks: int = 0,        # Hang after N chunks\n",
    "    ):\n",
    "        self.real_func = real_func\n",
    "        self.delay_before_first = delay_before_first\n",
    "        self.delay_between_chunks = delay_between_chunks\n",
    "        self.fail_after_chunks = fail_after_chunks\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Call the real function but inject delays.\n",
    "        For streaming, wraps the generator.\n",
    "        \"\"\"\n",
    "        if kwargs.get(\"stream\"):\n",
    "            return self._wrap_stream(**kwargs)\n",
    "        else:\n",
    "            # Non-streaming: just delay before response\n",
    "            if self.delay_before_first > 0:\n",
    "                print(\n",
    "                    f\"      ðŸ’¤ Injecting {self.delay_before_first}s \"\n",
    "                    f\"delay before response...\"\n",
    "                )\n",
    "                time.sleep(self.delay_before_first)\n",
    "            return self.real_func(**kwargs)\n",
    "\n",
    "    def _wrap_stream(self, **kwargs):\n",
    "        \"\"\"Wrap streaming response with delays.\"\"\"\n",
    "        stream = self.real_func(**kwargs)\n",
    "        chunk_count = 0\n",
    "        first_chunk = True\n",
    "\n",
    "        for chunk in stream:\n",
    "            # Delay before first token (TTFT simulation)\n",
    "            if first_chunk and self.delay_before_first > 0:\n",
    "                print(\n",
    "                    f\"      ðŸ’¤ Injecting {self.delay_before_first}s \"\n",
    "                    f\"TTFT delay...\"\n",
    "                )\n",
    "                time.sleep(self.delay_before_first)\n",
    "                first_chunk = False\n",
    "\n",
    "            # Hang after N chunks (simulates stalled connection)\n",
    "            if (\n",
    "                self.fail_after_chunks > 0\n",
    "                and chunk_count >= self.fail_after_chunks\n",
    "            ):\n",
    "                print(\n",
    "                    f\"      ðŸ”¥ Hanging after {chunk_count} chunks \"\n",
    "                    f\"(simulating stall)...\"\n",
    "                )\n",
    "                # Sleep longer than any reasonable timeout\n",
    "                time.sleep(9999)\n",
    "\n",
    "            # Delay between chunks\n",
    "            if (\n",
    "                not first_chunk\n",
    "                and self.delay_between_chunks > 0\n",
    "            ):\n",
    "                time.sleep(self.delay_between_chunks)\n",
    "\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                chunk_count += 1\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def measure_ttft_streaming(\n",
    "    client: AzureOpenAI,\n",
    "    messages: List[Dict[str, str]],\n",
    "    deployment: str,\n",
    "    config: TimeoutConfig,\n",
    "    injector: Optional[TimeoutInjector] = None,\n",
    "    **kwargs,\n",
    ") -> TimingResult:\n",
    "    \"\"\"Measure TTFT using streaming response.\"\"\"\n",
    "    result = TimingResult()\n",
    "    start_time = time.time()\n",
    "    content_chunks = []\n",
    "\n",
    "    try:\n",
    "        # Use injector if provided, else call client directly\n",
    "        if injector:\n",
    "            stream = injector(\n",
    "                model=deployment,\n",
    "                messages=messages,\n",
    "                stream=True,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            stream = client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=messages,\n",
    "                stream=True,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        result.connect_time = time.time() - start_time\n",
    "\n",
    "        for chunk in stream:\n",
    "            current_time = time.time() - start_time\n",
    "\n",
    "            # Check total timeout\n",
    "            if current_time > config.total_timeout:\n",
    "                result.timed_out = True\n",
    "                result.timeout_phase = \"total\"\n",
    "                break\n",
    "\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                content_chunks.append(content)\n",
    "                result.token_count += 1\n",
    "\n",
    "                # Record TTFT on first content\n",
    "                if result.ttft is None:\n",
    "                    result.ttft = current_time\n",
    "                    # Check TTFT timeout\n",
    "                    if result.ttft > config.ttft_timeout:\n",
    "                        result.timed_out = True\n",
    "                        result.timeout_phase = \"ttft\"\n",
    "                        break\n",
    "\n",
    "        result.total_time = time.time() - start_time\n",
    "        result.response = ''.join(content_chunks)\n",
    "\n",
    "    except httpx.TimeoutException as e:\n",
    "        result.timed_out = True\n",
    "        result.total_time = time.time() - start_time\n",
    "        if \"connect\" in str(e).lower():\n",
    "            result.timeout_phase = \"connect\"\n",
    "        else:\n",
    "            result.timeout_phase = \"read\"\n",
    "    except Exception as e:\n",
    "        result.total_time = time.time() - start_time\n",
    "        result.response = f\"Error: {str(e)}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# â”€â”€ Demo 1: Normal request (baseline) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š TTFT Measurement Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    timeout_config = TimeoutConfig(\n",
    "        connect_timeout=5.0,\n",
    "        ttft_timeout=3.0,   # Tight TTFT for demo\n",
    "        read_timeout=10.0,\n",
    "        total_timeout=15.0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâš™ï¸ Timeout Configuration:\")\n",
    "    print(f\"   Connect: {timeout_config.connect_timeout}s\")\n",
    "    print(f\"   TTFT:    {timeout_config.ttft_timeout}s\")\n",
    "    print(f\"   Read:    {timeout_config.read_timeout}s\")\n",
    "    print(f\"   Total:   {timeout_config.total_timeout}s\")\n",
    "\n",
    "    client = create_client_with_timeouts(timeout_config)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Say 'hello' only.\"}\n",
    "    ]\n",
    "\n",
    "    # â”€â”€ Baseline: no injection â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸŸ¢ Test 1: Normal request (no injection)\")\n",
    "    result = measure_ttft_streaming(\n",
    "        client=client,\n",
    "        messages=messages,\n",
    "        deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        config=timeout_config,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    print(f\"   TTFT: {result.ttft*1000:.0f}ms\" if result.ttft else \"   TTFT: N/A\")\n",
    "    print(f\"   Total: {result.total_time*1000:.0f}ms\")\n",
    "    print(f\"   Timed out: {result.timed_out}\")\n",
    "    print(f\"   Response: {result.response}\")\n",
    "\n",
    "    # â”€â”€ Inject TTFT delay â†’ should trigger ttft timeout â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸ”¥ Test 2: Injecting 4s TTFT delay (timeout=3s)\")\n",
    "\n",
    "    def real_create(**kw):\n",
    "        return client.chat.completions.create(**kw)\n",
    "\n",
    "    injector_ttft = TimeoutInjector(\n",
    "        real_create,\n",
    "        delay_before_first=4.0,  # > ttft_timeout\n",
    "    )\n",
    "\n",
    "    result = measure_ttft_streaming(\n",
    "        client=client,\n",
    "        messages=messages,\n",
    "        deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        config=timeout_config,\n",
    "        injector=injector_ttft,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    print(f\"   TTFT: {result.ttft*1000:.0f}ms\" if result.ttft else \"   TTFT: N/A\")\n",
    "    print(f\"   Total: {result.total_time*1000:.0f}ms\")\n",
    "    print(f\"   Timed out: {result.timed_out}\")\n",
    "    print(f\"   Timeout phase: {result.timeout_phase}\")\n",
    "    print(f\"   Response: {result.response or '(none)'}\")\n",
    "\n",
    "    # â”€â”€ Inject chunk stall â†’ total timeout â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸ”¥ Test 3: Stall after 2 chunks (total_timeout=15s)\")\n",
    "\n",
    "    # Use a thread with a hard kill for the stall test\n",
    "    # because the injector sleeps forever\n",
    "    def run_stall_test():\n",
    "        injector_stall = TimeoutInjector(\n",
    "            real_create,\n",
    "            fail_after_chunks=2,\n",
    "        )\n",
    "        return measure_ttft_streaming(\n",
    "            client=client,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Count 1 2 3 4 5\"}\n",
    "            ],\n",
    "            deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            config=TimeoutConfig(\n",
    "                ttft_timeout=5.0,\n",
    "                read_timeout=2.0,  # Will trigger read timeout\n",
    "                total_timeout=5.0,\n",
    "            ),\n",
    "            injector=injector_stall,\n",
    "            max_tokens=50,\n",
    "        )\n",
    "\n",
    "    # Run in a thread with a timeout\n",
    "    result_container = [None]\n",
    "\n",
    "    def target():\n",
    "        result_container[0] = run_stall_test()\n",
    "\n",
    "    t = threading.Thread(target=target, daemon=True)\n",
    "    t.start()\n",
    "    t.join(timeout=6.0)\n",
    "\n",
    "    if result_container[0]:\n",
    "        result = result_container[0]\n",
    "        print(f\"   Tokens received: {result.token_count}\")\n",
    "        print(f\"   Timed out: {result.timed_out}\")\n",
    "        print(f\"   Timeout phase: {result.timeout_phase}\")\n",
    "        print(f\"   Partial response: {result.response or '(none)'}\")\n",
    "    else:\n",
    "        print(\"   âš ï¸ Test killed by thread timeout (as expected)\")\n",
    "        print(\"   âœ… Demonstrates that without proper read \"\n",
    "              \"timeout, request would hang forever\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88942951",
   "metadata": {},
   "source": [
    "## Part 2: Streaming Timeout Management\n",
    "\n",
    "Handle timeouts in streaming scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2542ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Streaming Timeout Demo\n",
      "============================================================\n",
      "\n",
      "ðŸŸ¢ Test 1: Normal streaming (no injection)\n",
      "   Config: chunk_timeout=5.0s, total_timeout=30.0s\n",
      "   Response: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "   âœ… Completed (2 chunks)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”¥ Test 2: Inject 6s delay after chunk 2 (chunk_timeout=3s)\n",
      "   Response: Sure\n",
      "      ðŸ’¤ Injecting 6.0s delay after chunk 2...\n",
      "\n",
      "   âš ï¸ Timeout: chunk_timeout (6.0s > 3.0s)\n",
      "\n",
      "   Partial response: 1 chunks\n",
      "   âœ… Chunk timeout correctly detected and handled\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”¥ Test 3: Inject 8s delay (total_timeout=5s)\n",
      "   Response: \n",
      "      ðŸ’¤ Injecting 8.0s delay after chunk 1...\n",
      "\n",
      "   âš ï¸ Timeout: total_timeout (8.7s > 5.0s)\n",
      "\n",
      "   Partial response: 0 chunks\n",
      "   âœ… Total timeout correctly detected\n"
     ]
    }
   ],
   "source": [
    "# Streaming Timeout Handler\n",
    "\n",
    "@dataclass\n",
    "class StreamingConfig:\n",
    "    \"\"\"Configuration for streaming timeout handling.\"\"\"\n",
    "    chunk_timeout: float = 10.0    # Max time between chunks\n",
    "    total_timeout: float = 120.0   # Total streaming timeout\n",
    "    min_chunks: int = 1            # Minimum chunks before timeout OK\n",
    "\n",
    "class StreamingTimeoutHandler:\n",
    "    \"\"\"Handle timeouts during streaming responses.\"\"\"\n",
    "\n",
    "    def __init__(self, config: StreamingConfig):\n",
    "        self.config = config\n",
    "        self.start_time: Optional[float] = None\n",
    "        self.last_chunk_time: Optional[float] = None\n",
    "        self.chunk_count: int = 0\n",
    "\n",
    "    def start(self) -> None:\n",
    "        \"\"\"Start timing.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.last_chunk_time = self.start_time\n",
    "        self.chunk_count = 0\n",
    "\n",
    "    def record_chunk(self) -> None:\n",
    "        \"\"\"Record a chunk received.\"\"\"\n",
    "        self.last_chunk_time = time.time()\n",
    "        self.chunk_count += 1\n",
    "\n",
    "    def check_timeout(self) -> tuple[bool, str]:\n",
    "        \"\"\"Check if timeout occurred.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            return False, \"\"\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Check total timeout\n",
    "        elapsed_total = current_time - self.start_time\n",
    "        if elapsed_total > self.config.total_timeout:\n",
    "            return True, (\n",
    "                f\"total_timeout ({elapsed_total:.1f}s \"\n",
    "                f\"> {self.config.total_timeout}s)\"\n",
    "            )\n",
    "\n",
    "        # Check chunk timeout (only after min_chunks)\n",
    "        if (\n",
    "            self.chunk_count >= self.config.min_chunks\n",
    "            and self.last_chunk_time\n",
    "        ):\n",
    "            elapsed_since_chunk = current_time - self.last_chunk_time\n",
    "            if elapsed_since_chunk > self.config.chunk_timeout:\n",
    "                return True, (\n",
    "                    f\"chunk_timeout ({elapsed_since_chunk:.1f}s \"\n",
    "                    f\"> {self.config.chunk_timeout}s)\"\n",
    "                )\n",
    "\n",
    "        return False, \"\"\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get timing statistics.\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = (\n",
    "            current_time - self.start_time\n",
    "            if self.start_time else 0\n",
    "        )\n",
    "        return {\n",
    "            \"elapsed_total\": elapsed,\n",
    "            \"time_since_last_chunk\": (\n",
    "                current_time - self.last_chunk_time\n",
    "                if self.last_chunk_time else 0\n",
    "            ),\n",
    "            \"chunk_count\": self.chunk_count,\n",
    "            \"chunks_per_second\": (\n",
    "                self.chunk_count / elapsed\n",
    "                if elapsed > 0 else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "# â”€â”€ Streaming with injected delays â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class StreamingChunkInjector:\n",
    "    \"\"\"\n",
    "    Wraps a streaming response to inject delays\n",
    "    between chunks for timeout testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        real_create_func: Callable,\n",
    "        delay_after_n_chunks: int = 0,\n",
    "        delay_seconds: float = 0.0,\n",
    "    ):\n",
    "        self.real_create_func = real_create_func\n",
    "        self.delay_after_n_chunks = delay_after_n_chunks\n",
    "        self.delay_seconds = delay_seconds\n",
    "\n",
    "    def create(self, **kwargs):\n",
    "        \"\"\"Create streaming response with injected delay.\"\"\"\n",
    "        stream = self.real_create_func(**kwargs)\n",
    "        chunk_count = 0\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                chunk_count += 1\n",
    "                # Inject delay after N chunks\n",
    "                if (\n",
    "                    self.delay_after_n_chunks > 0\n",
    "                    and chunk_count == self.delay_after_n_chunks\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"\\n      ðŸ’¤ Injecting {self.delay_seconds}s \"\n",
    "                        f\"delay after chunk {chunk_count}...\"\n",
    "                    )\n",
    "                    time.sleep(self.delay_seconds)\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def stream_with_timeout_v2(\n",
    "    stream_source,\n",
    "    streaming_config: StreamingConfig,\n",
    ") -> Generator[str, None, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Stream response with timeout handling.\n",
    "    Accepts any iterable of chunks.\n",
    "    \"\"\"\n",
    "    handler = StreamingTimeoutHandler(streaming_config)\n",
    "    handler.start()\n",
    "\n",
    "    for chunk in stream_source:\n",
    "        timed_out, reason = handler.check_timeout()\n",
    "        if timed_out:\n",
    "            print(f\"\\n   âš ï¸ Timeout: {reason}\")\n",
    "            break\n",
    "\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            handler.record_chunk()\n",
    "            yield content\n",
    "\n",
    "    return handler.get_stats()\n",
    "\n",
    "\n",
    "# â”€â”€ Demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Streaming Timeout Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Test 1: Normal streaming (baseline) â”€â”€\n",
    "    print(\"\\nðŸŸ¢ Test 1: Normal streaming (no injection)\")\n",
    "    streaming_config = StreamingConfig(\n",
    "        chunk_timeout=5.0,\n",
    "        total_timeout=30.0,\n",
    "        min_chunks=1,\n",
    "    )\n",
    "\n",
    "    print(f\"   Config: chunk_timeout={streaming_config.chunk_timeout}s, \"\n",
    "          f\"total_timeout={streaming_config.total_timeout}s\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Say 'hello world' only.\"}\n",
    "    ]\n",
    "\n",
    "    print(\"   Response: \", end=\"\", flush=True)\n",
    "    content_chunks = []\n",
    "    stream = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    for chunk in stream_with_timeout_v2(stream, streaming_config):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        content_chunks.append(chunk)\n",
    "    print(f\"\\n   âœ… Completed ({len(content_chunks)} chunks)\")\n",
    "\n",
    "    # â”€â”€ Test 2: Inject chunk delay â†’ trigger chunk_timeout â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸ”¥ Test 2: Inject 6s delay after chunk 2 \"\n",
    "          \"(chunk_timeout=3s)\")\n",
    "\n",
    "    tight_config = StreamingConfig(\n",
    "        chunk_timeout=3.0,   # Tight timeout\n",
    "        total_timeout=30.0,\n",
    "        min_chunks=1,\n",
    "    )\n",
    "\n",
    "    def real_create(**kw):\n",
    "        return client.chat.completions.create(**kw)\n",
    "\n",
    "    injector = StreamingChunkInjector(\n",
    "        real_create,\n",
    "        delay_after_n_chunks=2,\n",
    "        delay_seconds=6.0,  # > chunk_timeout\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Count from 1 to 10\"}\n",
    "    ]\n",
    "\n",
    "    print(\"   Response: \", end=\"\", flush=True)\n",
    "    content_chunks = []\n",
    "    stream = injector.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    for chunk in stream_with_timeout_v2(stream, tight_config):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        content_chunks.append(chunk)\n",
    "\n",
    "    print(f\"\\n   Partial response: {len(content_chunks)} chunks\")\n",
    "    print(\"   âœ… Chunk timeout correctly detected and handled\")\n",
    "\n",
    "    # â”€â”€ Test 3: Inject delay â†’ trigger total_timeout â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸ”¥ Test 3: Inject 8s delay \"\n",
    "          \"(total_timeout=5s)\")\n",
    "\n",
    "    very_tight_config = StreamingConfig(\n",
    "        chunk_timeout=10.0,\n",
    "        total_timeout=5.0,  # Will be exceeded\n",
    "        min_chunks=1,\n",
    "    )\n",
    "\n",
    "    injector2 = StreamingChunkInjector(\n",
    "        real_create,\n",
    "        delay_after_n_chunks=1,\n",
    "        delay_seconds=8.0,  # > total_timeout\n",
    "    )\n",
    "\n",
    "    print(\"   Response: \", end=\"\", flush=True)\n",
    "    content_chunks = []\n",
    "    stream = injector2.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'A B C'\"}],\n",
    "        stream=True,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    for chunk in stream_with_timeout_v2(stream, very_tight_config):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        content_chunks.append(chunk)\n",
    "\n",
    "    print(f\"\\n   Partial response: {len(content_chunks)} chunks\")\n",
    "    print(\"   âœ… Total timeout correctly detected\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75291ae1",
   "metadata": {},
   "source": [
    "## Part 3: Adaptive Timeout Strategies\n",
    "\n",
    "Adjust timeouts based on request characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985c8c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Adaptive Timeout Calculator Demo\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ Calculated Timeouts by Scenario:\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario                            TTFT       Read       Total     \n",
      "--------------------------------------------------------------------------------\n",
      "Simple query (gpt-4o-mini)          3.3        30.0       10.0      \n",
      "Complex analysis (gpt-4o)           6.0        30.0       62.0      \n",
      "Code generation with tools (gpt-4.1) 12.0       30.0       138.3     \n",
      "Reasoning task (o1)                 15.8       30.0       98.2      \n"
     ]
    }
   ],
   "source": [
    "# Adaptive Timeout Calculator\n",
    "\n",
    "@dataclass\n",
    "class RequestCharacteristics:\n",
    "    \"\"\"Characteristics that affect timeout.\"\"\"\n",
    "    estimated_input_tokens: int\n",
    "    max_output_tokens: int\n",
    "    model: str\n",
    "    is_complex_task: bool = False  # e.g., code generation, analysis\n",
    "    has_tools: bool = False\n",
    "\n",
    "class AdaptiveTimeoutCalculator:\n",
    "    \"\"\"\n",
    "    Calculate appropriate timeouts based on request.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Approximate tokens per second for different models\n",
    "    MODEL_SPEEDS = {\n",
    "        \"gpt-4o\": 80,         # tokens/second\n",
    "        \"gpt-4o-mini\": 120,\n",
    "        \"gpt-4.1\": 70,\n",
    "        \"gpt-4\": 40,\n",
    "        \"gpt-35-turbo\": 100,\n",
    "        \"o1\": 30,             # Reasoning models are slower\n",
    "        \"o3\": 25,\n",
    "    }\n",
    "    \n",
    "    # Base TTFT for different models (seconds)\n",
    "    BASE_TTFT = {\n",
    "        \"gpt-4o\": 1.0,\n",
    "        \"gpt-4o-mini\": 0.5,\n",
    "        \"gpt-4.1\": 1.5,\n",
    "        \"gpt-4\": 2.0,\n",
    "        \"gpt-35-turbo\": 0.5,\n",
    "        \"o1\": 5.0,            # Reasoning models have higher TTFT\n",
    "        \"o3\": 10.0,\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_timeout: float = 10.0,\n",
    "        max_timeout: float = 300.0,\n",
    "        safety_multiplier: float = 2.0,\n",
    "    ):\n",
    "        self.min_timeout = min_timeout\n",
    "        self.max_timeout = max_timeout\n",
    "        self.safety_multiplier = safety_multiplier\n",
    "    \n",
    "    def _get_model_base(self, model: str) -> str:\n",
    "        \"\"\"Get base model name for lookup.\"\"\"\n",
    "        model_lower = model.lower()\n",
    "        for base in self.MODEL_SPEEDS.keys():\n",
    "            if base in model_lower:\n",
    "                return base\n",
    "        return \"gpt-4o\"  # default\n",
    "    \n",
    "    def calculate_ttft_timeout(self, characteristics: RequestCharacteristics) -> float:\n",
    "        \"\"\"\n",
    "        Calculate TTFT timeout based on request.\n",
    "        \"\"\"\n",
    "        base_model = self._get_model_base(characteristics.model)\n",
    "        base_ttft = self.BASE_TTFT.get(base_model, 2.0)\n",
    "        \n",
    "        # Adjust for input size (longer prompts = longer TTFT)\n",
    "        input_adjustment = characteristics.estimated_input_tokens / 1000 * 0.5\n",
    "        \n",
    "        # Adjust for complexity\n",
    "        complexity_multiplier = 1.5 if characteristics.is_complex_task else 1.0\n",
    "        \n",
    "        # Adjust for tools\n",
    "        tools_adjustment = 2.0 if characteristics.has_tools else 0.0\n",
    "        \n",
    "        ttft = (base_ttft + input_adjustment + tools_adjustment) * complexity_multiplier\n",
    "        ttft *= self.safety_multiplier\n",
    "        \n",
    "        return max(self.min_timeout / 3, min(ttft, self.max_timeout / 3))\n",
    "    \n",
    "    def calculate_total_timeout(self, characteristics: RequestCharacteristics) -> float:\n",
    "        \"\"\"\n",
    "        Calculate total timeout based on request.\n",
    "        \"\"\"\n",
    "        base_model = self._get_model_base(characteristics.model)\n",
    "        tokens_per_second = self.MODEL_SPEEDS.get(base_model, 50)\n",
    "        \n",
    "        # Estimate generation time\n",
    "        generation_time = characteristics.max_output_tokens / tokens_per_second\n",
    "        \n",
    "        # Add TTFT\n",
    "        ttft = self.calculate_ttft_timeout(characteristics)\n",
    "        \n",
    "        # Total with safety margin\n",
    "        total = (ttft + generation_time) * self.safety_multiplier\n",
    "        \n",
    "        return max(self.min_timeout, min(total, self.max_timeout))\n",
    "    \n",
    "    def calculate_timeouts(self, characteristics: RequestCharacteristics) -> TimeoutConfig:\n",
    "        \"\"\"\n",
    "        Calculate complete timeout configuration.\n",
    "        \"\"\"\n",
    "        ttft = self.calculate_ttft_timeout(characteristics)\n",
    "        total = self.calculate_total_timeout(characteristics)\n",
    "        \n",
    "        return TimeoutConfig(\n",
    "            connect_timeout=5.0,\n",
    "            ttft_timeout=ttft,\n",
    "            read_timeout=max(ttft, 30.0),\n",
    "            total_timeout=total,\n",
    "        )\n",
    "\n",
    "# Demonstrate adaptive timeouts\n",
    "print(\"ðŸ“Š Adaptive Timeout Calculator Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "calculator = AdaptiveTimeoutCalculator(\n",
    "    min_timeout=10.0,\n",
    "    max_timeout=300.0,\n",
    "    safety_multiplier=2.0,\n",
    ")\n",
    "\n",
    "# Test different scenarios\n",
    "scenarios = [\n",
    "    (\"Simple query (gpt-4o-mini)\", RequestCharacteristics(\n",
    "        estimated_input_tokens=100,\n",
    "        max_output_tokens=100,\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )),\n",
    "    (\"Complex analysis (gpt-4o)\", RequestCharacteristics(\n",
    "        estimated_input_tokens=2000,\n",
    "        max_output_tokens=2000,\n",
    "        model=\"gpt-4o\",\n",
    "        is_complex_task=True,\n",
    "    )),\n",
    "    (\"Code generation with tools (gpt-4.1)\", RequestCharacteristics(\n",
    "        estimated_input_tokens=1000,\n",
    "        max_output_tokens=4000,\n",
    "        model=\"gpt-4.1\",\n",
    "        is_complex_task=True,\n",
    "        has_tools=True,\n",
    "    )),\n",
    "    (\"Reasoning task (o1)\", RequestCharacteristics(\n",
    "        estimated_input_tokens=500,\n",
    "        max_output_tokens=1000,\n",
    "        model=\"o1\",\n",
    "        is_complex_task=True,\n",
    "    )),\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“‹ Calculated Timeouts by Scenario:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Scenario':<35} {'TTFT':<10} {'Read':<10} {'Total':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, characteristics in scenarios:\n",
    "    config = calculator.calculate_timeouts(characteristics)\n",
    "    print(f\"{name:<35} {config.ttft_timeout:<10.1f} {config.read_timeout:<10.1f} {config.total_timeout:<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ee3cb",
   "metadata": {},
   "source": [
    "## Part 4: Timeout Recovery\n",
    "\n",
    "Handle timeouts gracefully with recovery strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "078901c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Timeout Recovery Demo\n",
      "============================================================\n",
      "\n",
      "ðŸŸ¢ Test 1: Normal request (no injection)\n",
      "   ðŸ”„ Attempt 1: retry_same (max_tokens=20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   âœ… Result:\n",
      "      Success: True\n",
      "      Strategy: retry_same\n",
      "      Attempts: 1\n",
      "      Response: hello\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”¥ Test 2: First 2 attempts timeout, 3rd succeeds\n",
      "   ðŸ”„ Attempt 1: retry_same (max_tokens=20)\n",
      "      ðŸ’¤ Injecting 5.0s delay (attempt 1)...\n",
      "      âš ï¸ read_timeout exceeded (5.2s)\n",
      "   ðŸ”„ Attempt 2: retry_same (max_tokens=20)\n",
      "      ðŸ’¤ Injecting 5.0s delay (attempt 2)...\n",
      "      âš ï¸ read_timeout exceeded (5.2s)\n",
      "   ðŸ”„ Attempt 3: retry_same (max_tokens=20)\n",
      "\n",
      "   âœ… Result:\n",
      "      Success: True\n",
      "      Strategy: retry_same\n",
      "      Attempts: 3\n",
      "      Timed out (any): False\n",
      "      Response: 4\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”¥ Test 3: All attempts timeout â†’ return partial\n",
      "   ðŸ”„ Attempt 1: retry_same (max_tokens=50)\n",
      "      ðŸ’¤ Injecting 5.0s delay (attempt 1)...\n",
      "      âš ï¸ read_timeout exceeded (5.2s)\n",
      "      ðŸ“‰ Reducing max_tokens: 50 â†’ 25\n",
      "   ðŸ”„ Attempt 2: retry_simplified (max_tokens=25)\n",
      "      ðŸ’¤ Injecting 5.0s delay (attempt 2)...\n",
      "      âš ï¸ read_timeout exceeded (5.2s)\n",
      "   ðŸ”„ Attempt 3: partial_response (max_tokens=25)\n",
      "      ðŸ’¤ Injecting 5.0s delay (attempt 3)...\n",
      "      âš ï¸ read_timeout exceeded (5.2s)\n",
      "\n",
      "   âŒ Result:\n",
      "      Success: False\n",
      "      Strategy: fail_fast\n",
      "      Attempts: 3\n",
      "      Partial: False\n",
      "      Response: (none)\n"
     ]
    }
   ],
   "source": [
    "# Timeout Recovery Strategies\n",
    "\n",
    "class RecoveryStrategy(Enum):\n",
    "    \"\"\"Strategies for handling timeouts.\"\"\"\n",
    "    RETRY_SAME = \"retry_same\"\n",
    "    RETRY_SIMPLIFIED = \"retry_simplified\"\n",
    "    RETRY_FASTER_MODEL = \"retry_faster\"\n",
    "    FALLBACK_CACHE = \"fallback_cache\"\n",
    "    PARTIAL_RESPONSE = \"partial_response\"\n",
    "    FAIL_FAST = \"fail_fast\"\n",
    "\n",
    "@dataclass\n",
    "class RecoveryResult:\n",
    "    \"\"\"Result of recovery attempt.\"\"\"\n",
    "    success: bool\n",
    "    strategy_used: RecoveryStrategy\n",
    "    attempts: int\n",
    "    response: Optional[str]\n",
    "    partial: bool = False\n",
    "    timed_out: bool = False\n",
    "\n",
    "\n",
    "class TimeoutRecoveryManager:\n",
    "    \"\"\"Manage timeout recovery with multiple strategies.\"\"\"\n",
    "\n",
    "    MODEL_FALLBACKS = {\n",
    "        \"gpt-4o\": \"gpt-4o-mini\",\n",
    "        \"gpt-4.1\": \"gpt-4o\",\n",
    "        \"o1\": \"gpt-4o\",\n",
    "        \"o3\": \"o1\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AzureOpenAI,\n",
    "        default_deployment: str,\n",
    "        max_retries: int = 2,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.default_deployment = default_deployment\n",
    "        self.max_retries = max_retries\n",
    "        # Injector for testing (set externally)\n",
    "        self.timeout_injector: Optional[Callable] = None\n",
    "\n",
    "    def _try_request(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        deployment: str,\n",
    "        timeout_config: TimeoutConfig,\n",
    "        max_tokens: int,\n",
    "    ) -> tuple[bool, Optional[str], bool]:\n",
    "        \"\"\"\n",
    "        Try a request with timeout.\n",
    "        Returns (success, response, timed_out).\n",
    "        \"\"\"\n",
    "        partial_content = []\n",
    "        timed_out = False\n",
    "\n",
    "        try:\n",
    "            # Use injector if set, else real client\n",
    "            if self.timeout_injector:\n",
    "                stream = self.timeout_injector(\n",
    "                    model=deployment,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    stream=True,\n",
    "                )\n",
    "            else:\n",
    "                stream = self.client.chat.completions.create(\n",
    "                    model=deployment,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            start_time = time.time()\n",
    "            last_chunk_time = start_time\n",
    "\n",
    "            for chunk in stream:\n",
    "                current_time = time.time()\n",
    "\n",
    "                # Check total timeout\n",
    "                if (\n",
    "                    current_time - start_time\n",
    "                    > timeout_config.total_timeout\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"      âš ï¸ total_timeout exceeded \"\n",
    "                        f\"({current_time - start_time:.1f}s)\"\n",
    "                    )\n",
    "                    timed_out = True\n",
    "                    break\n",
    "\n",
    "                # Check read timeout\n",
    "                if (\n",
    "                    current_time - last_chunk_time\n",
    "                    > timeout_config.read_timeout\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"      âš ï¸ read_timeout exceeded \"\n",
    "                        f\"({current_time - last_chunk_time:.1f}s)\"\n",
    "                    )\n",
    "                    timed_out = True\n",
    "                    break\n",
    "\n",
    "                if chunk.choices and chunk.choices[0].delta.content:\n",
    "                    partial_content.append(\n",
    "                        chunk.choices[0].delta.content\n",
    "                    )\n",
    "                    last_chunk_time = current_time\n",
    "\n",
    "            response = ''.join(partial_content)\n",
    "            success = len(response) > 0 and not timed_out\n",
    "\n",
    "            return success, response, timed_out\n",
    "\n",
    "        except Exception as e:\n",
    "            response = (\n",
    "                ''.join(partial_content)\n",
    "                if partial_content else None\n",
    "            )\n",
    "            return False, response, True\n",
    "\n",
    "    def complete_with_recovery(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        timeout_config: TimeoutConfig,\n",
    "        max_tokens: int = 1000,\n",
    "        strategies: List[RecoveryStrategy] = None,\n",
    "    ) -> RecoveryResult:\n",
    "        \"\"\"Complete request with automatic recovery.\"\"\"\n",
    "        if strategies is None:\n",
    "            strategies = [\n",
    "                RecoveryStrategy.RETRY_SAME,\n",
    "                RecoveryStrategy.RETRY_SIMPLIFIED,\n",
    "                RecoveryStrategy.PARTIAL_RESPONSE,\n",
    "            ]\n",
    "\n",
    "        attempts = 0\n",
    "        current_deployment = self.default_deployment\n",
    "        current_max_tokens = max_tokens\n",
    "        last_partial = None\n",
    "        any_timeout = False\n",
    "\n",
    "        for strategy in strategies:\n",
    "            if attempts >= self.max_retries:\n",
    "                break\n",
    "\n",
    "            if strategy == RecoveryStrategy.RETRY_SIMPLIFIED:\n",
    "                current_max_tokens = max_tokens // 2\n",
    "                print(\n",
    "                    f\"      ðŸ“‰ Reducing max_tokens: \"\n",
    "                    f\"{max_tokens} â†’ {current_max_tokens}\"\n",
    "                )\n",
    "            elif strategy == RecoveryStrategy.RETRY_FASTER_MODEL:\n",
    "                fallback = self.MODEL_FALLBACKS.get(\n",
    "                    current_deployment\n",
    "                )\n",
    "                if fallback:\n",
    "                    print(\n",
    "                        f\"      ðŸ”„ Switching model: \"\n",
    "                        f\"{current_deployment} â†’ {fallback}\"\n",
    "                    )\n",
    "                    current_deployment = fallback\n",
    "            elif strategy == RecoveryStrategy.PARTIAL_RESPONSE:\n",
    "                if last_partial:\n",
    "                    return RecoveryResult(\n",
    "                        success=True,\n",
    "                        strategy_used=strategy,\n",
    "                        attempts=attempts,\n",
    "                        response=last_partial,\n",
    "                        partial=True,\n",
    "                        timed_out=any_timeout,\n",
    "                    )\n",
    "            elif strategy == RecoveryStrategy.FAIL_FAST:\n",
    "                break\n",
    "\n",
    "            attempts += 1\n",
    "            print(\n",
    "                f\"   ðŸ”„ Attempt {attempts}: {strategy.value} \"\n",
    "                f\"(max_tokens={current_max_tokens})\"\n",
    "            )\n",
    "\n",
    "            success, response, timed_out = self._try_request(\n",
    "                messages=messages,\n",
    "                deployment=current_deployment,\n",
    "                timeout_config=timeout_config,\n",
    "                max_tokens=current_max_tokens,\n",
    "            )\n",
    "\n",
    "            if timed_out:\n",
    "                any_timeout = True\n",
    "\n",
    "            if response:\n",
    "                last_partial = response\n",
    "\n",
    "            if success:\n",
    "                return RecoveryResult(\n",
    "                    success=True,\n",
    "                    strategy_used=strategy,\n",
    "                    attempts=attempts,\n",
    "                    response=response,\n",
    "                    partial=False,\n",
    "                    timed_out=False,\n",
    "                )\n",
    "\n",
    "        return RecoveryResult(\n",
    "            success=False,\n",
    "            strategy_used=RecoveryStrategy.FAIL_FAST,\n",
    "            attempts=attempts,\n",
    "            response=last_partial,\n",
    "            partial=True if last_partial else False,\n",
    "            timed_out=any_timeout,\n",
    "        )\n",
    "\n",
    "\n",
    "# â”€â”€ Timeout Injector for Recovery Demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class RecoveryTimeoutInjector:\n",
    "    \"\"\"\n",
    "    Injects timeouts for the first N attempts,\n",
    "    then allows success.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        real_create_func: Callable,\n",
    "        fail_attempts: int = 2,\n",
    "        delay_seconds: float = 5.0,\n",
    "    ):\n",
    "        self.real_create_func = real_create_func\n",
    "        self.fail_attempts = fail_attempts\n",
    "        self.delay_seconds = delay_seconds\n",
    "        self.attempt_count = 0\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        \"\"\"Wrap streaming with conditional delay.\"\"\"\n",
    "        self.attempt_count += 1\n",
    "        stream = self.real_create_func(**kwargs)\n",
    "\n",
    "        if self.attempt_count <= self.fail_attempts:\n",
    "            # Inject delay on first N attempts\n",
    "            return self._inject_delay(stream)\n",
    "        else:\n",
    "            # Let it succeed\n",
    "            return stream\n",
    "\n",
    "    def _inject_delay(self, stream):\n",
    "        \"\"\"Inject delay after first chunk.\"\"\"\n",
    "        chunk_count = 0\n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                chunk_count += 1\n",
    "                if chunk_count == 1:\n",
    "                    print(\n",
    "                        f\"      ðŸ’¤ Injecting {self.delay_seconds}s \"\n",
    "                        f\"delay (attempt {self.attempt_count})...\"\n",
    "                    )\n",
    "                    time.sleep(self.delay_seconds)\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "# â”€â”€ Demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Timeout Recovery Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "\n",
    "    recovery_manager = TimeoutRecoveryManager(\n",
    "        client=client,\n",
    "        default_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        max_retries=4,\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Test 1: Normal request (no timeout) â”€â”€\n",
    "    print(\"\\nðŸŸ¢ Test 1: Normal request (no injection)\")\n",
    "    timeout_config = TimeoutConfig(\n",
    "        ttft_timeout=10.0,\n",
    "        read_timeout=10.0,\n",
    "        total_timeout=30.0,\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Say 'hello' only.\"}\n",
    "    ]\n",
    "\n",
    "    result = recovery_manager.complete_with_recovery(\n",
    "        messages=messages,\n",
    "        timeout_config=timeout_config,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   âœ… Result:\")\n",
    "    print(f\"      Success: {result.success}\")\n",
    "    print(f\"      Strategy: {result.strategy_used.value}\")\n",
    "    print(f\"      Attempts: {result.attempts}\")\n",
    "    print(f\"      Response: {result.response}\")\n",
    "\n",
    "    # â”€â”€ Test 2: Inject timeout â†’ recovery succeeds â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸ”¥ Test 2: First 2 attempts timeout, 3rd succeeds\")\n",
    "\n",
    "    def real_create(**kw):\n",
    "        return client.chat.completions.create(**kw)\n",
    "\n",
    "    injector = RecoveryTimeoutInjector(\n",
    "        real_create,\n",
    "        fail_attempts=2,\n",
    "        delay_seconds=5.0,  # Will exceed read_timeout\n",
    "    )\n",
    "    recovery_manager.timeout_injector = injector\n",
    "\n",
    "    tight_timeout = TimeoutConfig(\n",
    "        ttft_timeout=5.0,\n",
    "        read_timeout=2.0,   # Tight â€” injected delay will exceed\n",
    "        total_timeout=10.0,\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2? Answer only.\"}\n",
    "    ]\n",
    "\n",
    "    result = recovery_manager.complete_with_recovery(\n",
    "        messages=messages,\n",
    "        timeout_config=tight_timeout,\n",
    "        max_tokens=20,\n",
    "        strategies=[\n",
    "            RecoveryStrategy.RETRY_SAME,\n",
    "            RecoveryStrategy.RETRY_SAME,\n",
    "            RecoveryStrategy.RETRY_SAME,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   {'âœ…' if result.success else 'âŒ'} Result:\")\n",
    "    print(f\"      Success: {result.success}\")\n",
    "    print(f\"      Strategy: {result.strategy_used.value}\")\n",
    "    print(f\"      Attempts: {result.attempts}\")\n",
    "    print(f\"      Timed out (any): {result.timed_out}\")\n",
    "    print(f\"      Response: {result.response}\")\n",
    "\n",
    "    # Reset injector\n",
    "    recovery_manager.timeout_injector = None\n",
    "\n",
    "    # â”€â”€ Test 3: All attempts timeout â†’ partial response â”€â”€\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(\"ðŸ”¥ Test 3: All attempts timeout â†’ return partial\")\n",
    "\n",
    "    always_fail_injector = RecoveryTimeoutInjector(\n",
    "        real_create,\n",
    "        fail_attempts=99,  # Always fail\n",
    "        delay_seconds=5.0,\n",
    "    )\n",
    "    recovery_manager.timeout_injector = always_fail_injector\n",
    "\n",
    "    result = recovery_manager.complete_with_recovery(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Count 1 2 3 4 5\"}\n",
    "        ],\n",
    "        timeout_config=tight_timeout,\n",
    "        max_tokens=50,\n",
    "        strategies=[\n",
    "            RecoveryStrategy.RETRY_SAME,\n",
    "            RecoveryStrategy.RETRY_SIMPLIFIED,\n",
    "            RecoveryStrategy.PARTIAL_RESPONSE,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   {'âœ…' if result.success else 'âŒ'} Result:\")\n",
    "    print(f\"      Success: {result.success}\")\n",
    "    print(f\"      Strategy: {result.strategy_used.value}\")\n",
    "    print(f\"      Attempts: {result.attempts}\")\n",
    "    print(f\"      Partial: {result.partial}\")\n",
    "    print(f\"      Response: {result.response or '(none)'}\")\n",
    "\n",
    "    # Reset\n",
    "    recovery_manager.timeout_injector = None\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e3f89",
   "metadata": {},
   "source": [
    "## Part 5: Production Timeout Manager\n",
    "\n",
    "Complete timeout management solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72488596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Production Timeout Manager Demo\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Making requests with timeout management...\n",
      "\n",
      "   Query: What is 2+2?...\n",
      "   âœ… TTFT: 670ms, Total: 673ms\n",
      "\n",
      "   Query: Explain machine learning in 2 sentences....\n",
      "   âœ… TTFT: 1481ms, Total: 1486ms\n",
      "\n",
      "   Query: Write a short poem about coding....\n",
      "   âœ… TTFT: 1775ms, Total: 1784ms\n",
      "\n",
      "ðŸ“ˆ Timeout Manager Statistics:\n",
      "   Total requests: 3\n",
      "   Success rate: 100.0%\n",
      "   Timeout rate: 0.0%\n",
      "   Recovery rate: 0.0%\n",
      "   Avg TTFT: 1309ms\n"
     ]
    }
   ],
   "source": [
    "# Production Timeout Manager\n",
    "\n",
    "@dataclass\n",
    "class TimeoutResult:\n",
    "    \"\"\"Result with full timeout information.\"\"\"\n",
    "    success: bool\n",
    "    response: Optional[str]\n",
    "    timings: TimingResult\n",
    "    recovery_used: bool\n",
    "    recovery_strategy: Optional[RecoveryStrategy]\n",
    "    total_attempts: int\n",
    "\n",
    "class ProductionTimeoutManager:\n",
    "    \"\"\"\n",
    "    Production-ready timeout management with:\n",
    "    - Adaptive timeout calculation\n",
    "    - TTFT monitoring\n",
    "    - Streaming timeout handling\n",
    "    - Automatic recovery\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AzureOpenAI,\n",
    "        default_deployment: str,\n",
    "        enable_adaptive_timeouts: bool = True,\n",
    "        enable_recovery: bool = True,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.default_deployment = default_deployment\n",
    "        self.enable_adaptive = enable_adaptive_timeouts\n",
    "        self.enable_recovery = enable_recovery\n",
    "        \n",
    "        self.calculator = AdaptiveTimeoutCalculator()\n",
    "        self.recovery_manager = TimeoutRecoveryManager(\n",
    "            client=client,\n",
    "            default_deployment=default_deployment,\n",
    "        )\n",
    "        \n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful\": 0,\n",
    "            \"timed_out\": 0,\n",
    "            \"recovered\": 0,\n",
    "            \"avg_ttft\": 0.0,\n",
    "            \"ttft_samples\": [],\n",
    "        }\n",
    "    \n",
    "    def _estimate_tokens(self, messages: List[Dict[str, str]]) -> int:\n",
    "        \"\"\"Rough estimate of input tokens.\"\"\"\n",
    "        total_chars = sum(len(m.get(\"content\", \"\")) for m in messages)\n",
    "        return total_chars // 4  # Rough approximation\n",
    "    \n",
    "    def complete(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_tokens: int = 1000,\n",
    "        timeout_config: Optional[TimeoutConfig] = None,\n",
    "        is_complex: bool = False,\n",
    "        has_tools: bool = False,\n",
    "        **kwargs\n",
    "    ) -> TimeoutResult:\n",
    "        \"\"\"\n",
    "        Make a completion with full timeout management.\n",
    "        \"\"\"\n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        # Calculate adaptive timeouts if enabled\n",
    "        if timeout_config is None and self.enable_adaptive:\n",
    "            characteristics = RequestCharacteristics(\n",
    "                estimated_input_tokens=self._estimate_tokens(messages),\n",
    "                max_output_tokens=max_tokens,\n",
    "                model=self.default_deployment,\n",
    "                is_complex_task=is_complex,\n",
    "                has_tools=has_tools,\n",
    "            )\n",
    "            timeout_config = self.calculator.calculate_timeouts(characteristics)\n",
    "        elif timeout_config is None:\n",
    "            timeout_config = TimeoutConfig()\n",
    "        \n",
    "        # Measure timing\n",
    "        timing = measure_ttft_streaming(\n",
    "            client=self.client,\n",
    "            messages=messages,\n",
    "            deployment=self.default_deployment,\n",
    "            config=timeout_config,\n",
    "            max_tokens=max_tokens,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Update TTFT stats\n",
    "        if timing.ttft:\n",
    "            self.stats[\"ttft_samples\"].append(timing.ttft)\n",
    "            self.stats[\"avg_ttft\"] = sum(self.stats[\"ttft_samples\"]) / len(self.stats[\"ttft_samples\"])\n",
    "        \n",
    "        # Check if recovery needed\n",
    "        recovery_used = False\n",
    "        recovery_strategy = None\n",
    "        total_attempts = 1\n",
    "        \n",
    "        if timing.timed_out and self.enable_recovery:\n",
    "            self.stats[\"timed_out\"] += 1\n",
    "            print(f\"   âš ï¸ Timeout occurred ({timing.timeout_phase}), attempting recovery...\")\n",
    "            \n",
    "            recovery_result = self.recovery_manager.complete_with_recovery(\n",
    "                messages=messages,\n",
    "                timeout_config=timeout_config,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            \n",
    "            recovery_used = True\n",
    "            recovery_strategy = recovery_result.strategy_used\n",
    "            total_attempts += recovery_result.attempts\n",
    "            \n",
    "            if recovery_result.success:\n",
    "                self.stats[\"recovered\"] += 1\n",
    "                timing.response = recovery_result.response\n",
    "                timing.timed_out = False\n",
    "        \n",
    "        if not timing.timed_out:\n",
    "            self.stats[\"successful\"] += 1\n",
    "        \n",
    "        return TimeoutResult(\n",
    "            success=not timing.timed_out,\n",
    "            response=timing.response,\n",
    "            timings=timing,\n",
    "            recovery_used=recovery_used,\n",
    "            recovery_strategy=recovery_strategy,\n",
    "            total_attempts=total_attempts,\n",
    "        )\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get timeout statistics.\"\"\"\n",
    "        stats = self.stats.copy()\n",
    "        del stats[\"ttft_samples\"]  # Don't include raw samples\n",
    "        \n",
    "        if self.stats[\"total_requests\"] > 0:\n",
    "            stats[\"success_rate\"] = self.stats[\"successful\"] / self.stats[\"total_requests\"] * 100\n",
    "            stats[\"timeout_rate\"] = self.stats[\"timed_out\"] / self.stats[\"total_requests\"] * 100\n",
    "            if self.stats[\"timed_out\"] > 0:\n",
    "                stats[\"recovery_rate\"] = self.stats[\"recovered\"] / self.stats[\"timed_out\"] * 100\n",
    "            else:\n",
    "                stats[\"recovery_rate\"] = 0\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Demonstrate production timeout manager\n",
    "print(\"ðŸ“Š Production Timeout Manager Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    manager = ProductionTimeoutManager(\n",
    "        client=client,\n",
    "        default_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        enable_adaptive_timeouts=True,\n",
    "        enable_recovery=True,\n",
    "    )\n",
    "    \n",
    "    # Make several requests\n",
    "    print(\"\\nðŸ“ Making requests with timeout management...\")\n",
    "    \n",
    "    queries = [\n",
    "        (\"What is 2+2?\", False),\n",
    "        (\"Explain machine learning in 2 sentences.\", False),\n",
    "        (\"Write a short poem about coding.\", True),\n",
    "    ]\n",
    "    \n",
    "    for query, is_complex in queries:\n",
    "        print(f\"\\n   Query: {query[:40]}...\")\n",
    "        result = manager.complete(\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            max_tokens=100,\n",
    "            is_complex=is_complex,\n",
    "        )\n",
    "        \n",
    "        status = \"âœ…\" if result.success else \"âŒ\"\n",
    "        print(f\"   {status} TTFT: {result.timings.ttft*1000:.0f}ms, Total: {result.timings.total_time*1000:.0f}ms\")\n",
    "        if result.recovery_used:\n",
    "            print(f\"   âš¡ Recovery: {result.recovery_strategy.value}\")\n",
    "    \n",
    "    # Display stats\n",
    "    stats = manager.get_stats()\n",
    "    print(f\"\\nðŸ“ˆ Timeout Manager Statistics:\")\n",
    "    print(f\"   Total requests: {stats['total_requests']}\")\n",
    "    print(f\"   Success rate: {stats.get('success_rate', 0):.1f}%\")\n",
    "    print(f\"   Timeout rate: {stats.get('timeout_rate', 0):.1f}%\")\n",
    "    print(f\"   Recovery rate: {stats.get('recovery_rate', 0):.1f}%\")\n",
    "    print(f\"   Avg TTFT: {stats['avg_ttft']*1000:.0f}ms\")\n",
    "else:\n",
    "    print(\"âš ï¸ AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fac2c9",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### Timeout Configuration Guidelines\n",
    "\n",
    "| Scenario | Connect | TTFT | Read | Total |\n",
    "|----------|---------|------|------|-------|\n",
    "| Simple queries | 5s | 10s | 30s | 60s |\n",
    "| Complex analysis | 5s | 30s | 60s | 180s |\n",
    "| Code generation | 5s | 30s | 60s | 300s |\n",
    "| Reasoning (o1/o3) | 10s | 60s | 120s | 600s |\n",
    "\n",
    "### Recovery Strategy Priority\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   Timeout Recovery Flow                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Timeout     1. Retry Same    2. Simplified    3. Faster      â”‚\n",
    "â”‚   Occurs  â†’   Parameters   â†’   (Â½ tokens)   â†’   Model      â†’   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚          4. Return Partial    5. Fail Fast                     â”‚\n",
    "â”‚      â†’   (if available)   â†’   (no response)                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8b944",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **TTFT vs Total**: Different timeouts for different stages\n",
    "2. **Streaming Timeouts**: Monitor chunk-by-chunk progress\n",
    "3. **Adaptive Timeouts**: Adjust based on request complexity\n",
    "4. **Recovery Strategies**: Handle timeouts gracefully\n",
    "5. **Production Manager**: Combine all strategies\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure OpenAI Latency](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits)\n",
    "- [Timeout Best Practices](https://learn.microsoft.com/en-us/azure/architecture/best-practices/retry-service-specific)\n",
    "- [Streaming Responses](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/streaming)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **4_observability/1_tracing_and_logging.ipynb**: Monitor and debug your applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
