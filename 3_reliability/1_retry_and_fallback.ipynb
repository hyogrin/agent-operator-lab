{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ac94de",
   "metadata": {},
   "source": [
    "# Reliability: Retry and Fallback Strategies\n",
    "\n",
    "----\n",
    "\n",
    "This notebook focuses on **retry and fallback patterns** for building resilient AI applications.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "- **Exponential Backoff**: Properly handle transient failures with increasing delays\n",
    "- **429 Rate Limit Handling**: Respect Retry-After headers and manage quota\n",
    "- **5xx Error Handling**: Handle server errors gracefully\n",
    "- **Multi-Backend Fallback**: Route to backup deployments on failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132396c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Why Retry Strategies Matter](#why-retry-strategies-matter)\n",
    "- [Setup](#setup)\n",
    "- [Part 1: Exponential Backoff](#part-1-exponential-backoff)\n",
    "- [Part 2: 429 Rate Limit Handling](#part-2-429-rate-limit-handling)\n",
    "- [Part 3: 5xx Error Handling](#part-3-5xx-error-handling)\n",
    "- [Part 4: Multi-Backend Fallback](#part-4-multi-backend-fallback)\n",
    "- [Part 5: Complete Resilient Client](#part-5-complete-resilient-client)\n",
    "- [Best Practices Summary](#best-practices-summary)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e66fd",
   "metadata": {},
   "source": [
    "## Why Retry Strategies Matter\n",
    "\n",
    "### Common Failure Scenarios\n",
    "\n",
    "| Error | Cause | Retryable? | Strategy |\n",
    "|-------|-------|------------|----------|\n",
    "| **429 Too Many Requests** | Rate limit exceeded | âœ… Yes | Wait for Retry-After |\n",
    "| **500 Internal Server Error** | Backend issue | âœ… Yes | Exponential backoff |\n",
    "| **502 Bad Gateway** | Gateway issue | âœ… Yes | Exponential backoff |\n",
    "| **503 Service Unavailable** | Overloaded | âœ… Yes | Exponential backoff |\n",
    "| **504 Gateway Timeout** | Timeout | âœ… Yes | Exponential backoff |\n",
    "| **400 Bad Request** | Invalid input | âŒ No | Fix request |\n",
    "| **401 Unauthorized** | Invalid credentials | âŒ No | Fix auth |\n",
    "\n",
    "### The Cost of No Retries\n",
    "\n",
    "```\n",
    "Without Retries:           With Smart Retries:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Request â†’ 429 â†’ FAIL      Request â†’ 429 â†’ Wait â†’ Retry â†’ Success\n",
    "User sees error           User gets response (delayed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9154c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5413c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Azure CLI found: /anaconda/envs/azureml_py38/bin//az\n",
      "âœ… PATH looks good already\n",
      "\n",
      "PATH (first 150 chars): /anaconda/envs/azureml_py38/bin/:/afh/code/agent-operator-lab/.venv/bin:/home/azureuser/.vscode-server/cli/servers/Stable-c9d77990917f3102ada88be140d2...\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'ğŸ” Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"âœ… Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('âœ… PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05a6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded settings from '../0_setup/.foundry_config.json'.\n",
      "\n",
      "ğŸ“Œ Foundry name: foundry-rq90gs\n",
      "ğŸ“Œ Resource group: foundry-rg\n",
      "ğŸ“Œ Location: swedencentral\n",
      "ğŸ“Œ Azure OpenAI endpoint: https://foundry-rq90gs.cognitiveservices.azure.com/\n",
      "ğŸ“Œ Chat deployment: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸ Could not find '{config_file}'.\")\n",
    "    print('ğŸ’¡ Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "# Project variables from config\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "\n",
    "# Azure OpenAI variables from env\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"âœ… Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nğŸ“Œ Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"ğŸ“Œ Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"ğŸ“Œ Location: {LOCATION}\")\n",
    "print(f\"ğŸ“Œ Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"ğŸ“Œ Chat deployment: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe037b7",
   "metadata": {},
   "source": [
    "## Part 1: Exponential Backoff\n",
    "\n",
    "Exponential backoff increases the delay between retries to avoid overwhelming the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6525d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Exponential Backoff Demonstration\n",
      "============================================================\n",
      "\n",
      "âš™ï¸ Configuration:\n",
      "   Base delay: 1.0s\n",
      "   Max delay: 60.0s\n",
      "   Exponential base: 2.0\n",
      "   Jitter: True\n",
      "\n",
      "ğŸ“ˆ Delay progression (without jitter):\n",
      "   Attempt 0:   1.0s â”‚ \n",
      "   Attempt 1:   2.0s â”‚ â–ˆ\n",
      "   Attempt 2:   4.0s â”‚ â–ˆâ–ˆ\n",
      "   Attempt 3:   8.0s â”‚ â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   Attempt 4:  16.0s â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   Attempt 5:  32.0s â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ğŸ“ˆ Delay progression (with jitter):\n",
      "   Attempt 0:   0.9s â”‚ \n",
      "   Attempt 1:   1.6s â”‚ \n",
      "   Attempt 2:   4.0s â”‚ â–ˆ\n",
      "   Attempt 3:   6.2s â”‚ â–ˆâ–ˆâ–ˆ\n",
      "   Attempt 4:  15.0s â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   Attempt 5:  24.1s â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n"
     ]
    }
   ],
   "source": [
    "# Exponential Backoff implementation\n",
    "\n",
    "@dataclass\n",
    "class RetryConfig:\n",
    "    \"\"\"Configuration for retry behavior.\"\"\"\n",
    "    max_retries: int = 3\n",
    "    base_delay: float = 1.0  # seconds\n",
    "    max_delay: float = 60.0  # seconds\n",
    "    exponential_base: float = 2.0\n",
    "    jitter: bool = True  # Add randomness to prevent thundering herd\n",
    "\n",
    "def calculate_backoff_delay(\n",
    "    attempt: int,\n",
    "    config: RetryConfig,\n",
    "    retry_after: Optional[float] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate delay before next retry using exponential backoff.\n",
    "    \n",
    "    Formula: min(base_delay * (exponential_base ^ attempt), max_delay)\n",
    "    With jitter: delay * (0.5 + random(0, 0.5))\n",
    "    \"\"\"\n",
    "    # If Retry-After header provided, use it\n",
    "    if retry_after is not None:\n",
    "        return retry_after\n",
    "    \n",
    "    # Calculate exponential delay\n",
    "    delay = config.base_delay * (config.exponential_base ** attempt)\n",
    "    delay = min(delay, config.max_delay)\n",
    "    \n",
    "    # Add jitter to prevent thundering herd\n",
    "    if config.jitter:\n",
    "        jitter_factor = 0.5 + random.random() * 0.5  # 0.5 to 1.0\n",
    "        delay *= jitter_factor\n",
    "    \n",
    "    return delay\n",
    "\n",
    "# Demonstrate exponential backoff delays\n",
    "print(\"ğŸ“Š Exponential Backoff Demonstration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config = RetryConfig(max_retries=5, base_delay=1.0, max_delay=60.0)\n",
    "\n",
    "print(f\"\\nâš™ï¸ Configuration:\")\n",
    "print(f\"   Base delay: {config.base_delay}s\")\n",
    "print(f\"   Max delay: {config.max_delay}s\")\n",
    "print(f\"   Exponential base: {config.exponential_base}\")\n",
    "print(f\"   Jitter: {config.jitter}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Delay progression (without jitter):\")\n",
    "config_no_jitter = RetryConfig(jitter=False)\n",
    "for attempt in range(6):\n",
    "    delay = calculate_backoff_delay(attempt, config_no_jitter)\n",
    "    bar = \"â–ˆ\" * int(delay / 2)\n",
    "    print(f\"   Attempt {attempt}: {delay:5.1f}s â”‚ {bar}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Delay progression (with jitter):\")\n",
    "for attempt in range(6):\n",
    "    delay = calculate_backoff_delay(attempt, config)\n",
    "    bar = \"â–ˆ\" * int(delay / 2)\n",
    "    print(f\"   Attempt {attempt}: {delay:5.1f}s â”‚ {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280bc2bc",
   "metadata": {},
   "source": [
    "## Part 2: 429 Rate Limit Handling with fake_response\n",
    "\n",
    "Handle rate limits properly by respecting Retry-After headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e5cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Retry Demo â€“ forced 429 errors then real success\n",
      "============================================================\n",
      "   â³ Retry 1/3 after 1.0s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â³ Retry 2/3 after 1.0s...\n",
      "\n",
      "âœ… Result:\n",
      "   Success:     True\n",
      "   Response:    ok\n",
      "   Attempts:    3\n",
      "   Total delay: 2.0s\n",
      "   Errors hit:  2\n",
      "      â€¢ Attempt 1: RateLimitError: [Simulated] Rate limit hit (call 1)\n",
      "      â€¢ Attempt 2: RateLimitError: [Simulated] Rate limit hit (call 2)\n"
     ]
    }
   ],
   "source": [
    "# 429 Rate Limit Handler\n",
    "from openai import AzureOpenAI, RateLimitError, APIStatusError\n",
    "import httpx\n",
    "\n",
    "@dataclass\n",
    "class RetryResult:\n",
    "    \"\"\"Result of a retry operation.\"\"\"\n",
    "    success: bool\n",
    "    response: Optional[str]\n",
    "    attempts: int\n",
    "    total_delay: float\n",
    "    errors: List[str]\n",
    "\n",
    "def extract_retry_after(error: Exception) -> Optional[float]:\n",
    "    \"\"\"Extract Retry-After value from error response.\"\"\"\n",
    "    if hasattr(error, 'response') and error.response is not None:\n",
    "        headers = getattr(error.response, 'headers', {})\n",
    "        retry_after = (\n",
    "            headers.get('Retry-After')\n",
    "            or headers.get('retry-after')\n",
    "        )\n",
    "        if retry_after:\n",
    "            try:\n",
    "                return float(retry_after)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def is_retryable_error(error: Exception) -> bool:\n",
    "    \"\"\"Check if an error is retryable.\"\"\"\n",
    "    if isinstance(error, RateLimitError):\n",
    "        return True\n",
    "    if isinstance(error, APIStatusError):\n",
    "        status_code = getattr(error, 'status_code', 0)\n",
    "        return status_code in [429, 500, 502, 503, 504]\n",
    "    return False\n",
    "\n",
    "def call_with_retry(\n",
    "    func: Callable,\n",
    "    config: RetryConfig = RetryConfig()\n",
    ") -> RetryResult:\n",
    "    \"\"\"Execute a function with retry logic.\"\"\"\n",
    "    attempts = 0\n",
    "    total_delay = 0\n",
    "    errors = []\n",
    "\n",
    "    while attempts <= config.max_retries:\n",
    "        try:\n",
    "            result = func()\n",
    "            return RetryResult(\n",
    "                success=True,\n",
    "                response=result,\n",
    "                attempts=attempts + 1,\n",
    "                total_delay=total_delay,\n",
    "                errors=errors,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            errors.append(\n",
    "                f\"Attempt {attempts + 1}: \"\n",
    "                f\"{type(e).__name__}: {str(e)[:100]}\"\n",
    "            )\n",
    "\n",
    "            if not is_retryable_error(e):\n",
    "                return RetryResult(\n",
    "                    success=False,\n",
    "                    response=None,\n",
    "                    attempts=attempts + 1,\n",
    "                    total_delay=total_delay,\n",
    "                    errors=errors,\n",
    "                )\n",
    "\n",
    "            if attempts >= config.max_retries:\n",
    "                break\n",
    "\n",
    "            retry_after = extract_retry_after(e)\n",
    "            delay = calculate_backoff_delay(\n",
    "                attempts, config, retry_after\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"   â³ Retry {attempts + 1}\"\n",
    "                f\"/{config.max_retries} \"\n",
    "                f\"after {delay:.1f}s...\"\n",
    "            )\n",
    "            time.sleep(delay)\n",
    "            total_delay += delay\n",
    "            attempts += 1\n",
    "\n",
    "    return RetryResult(\n",
    "        success=False,\n",
    "        response=None,\n",
    "        attempts=attempts,\n",
    "        total_delay=total_delay,\n",
    "        errors=errors,\n",
    "    )\n",
    "\n",
    "\n",
    "# â”€â”€ Forced-failure demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# We inject a counter so the first N calls throw a *real*\n",
    "# RateLimitError, then the final call goes to the actual API.\n",
    "\n",
    "class FaultInjector:\n",
    "    \"\"\"Wraps an API call and forces failures for the\n",
    "    first `fail_count` invocations.\"\"\"\n",
    "\n",
    "    def __init__(self, real_func: Callable, fail_count: int = 2):\n",
    "        self.real_func = real_func\n",
    "        self.fail_count = fail_count\n",
    "        self.call_number = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.call_number += 1\n",
    "        if self.call_number <= self.fail_count:\n",
    "            # Build a realistic 429 response object\n",
    "            fake_response = httpx.Response(\n",
    "                status_code=429,\n",
    "                headers={\"Retry-After\": \"1\"},\n",
    "                json={\n",
    "                    \"error\": {\n",
    "                        \"code\": \"429\",\n",
    "                        \"message\": \"Rate limit exceeded \"\n",
    "                        \"(simulated)\",\n",
    "                    }\n",
    "                },\n",
    "                request=httpx.Request(\"POST\", \"https://fake\"),\n",
    "            )\n",
    "            raise RateLimitError(\n",
    "                message=(\n",
    "                    f\"[Simulated] Rate limit hit \"\n",
    "                    f\"(call {self.call_number})\"\n",
    "                ),\n",
    "                response=fake_response,\n",
    "                body=None,\n",
    "            )\n",
    "        # After enough failures, call the real API\n",
    "        return self.real_func()\n",
    "\n",
    "\n",
    "print(\"ğŸ“Š Retry Demo â€“ forced 429 errors then real success\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "\n",
    "    def real_request():\n",
    "        resp = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Say 'ok'\"}\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    # First 2 calls â†’ simulated 429, 3rd call â†’ real API\n",
    "    injector = FaultInjector(real_request, fail_count=2)\n",
    "    result = call_with_retry(\n",
    "        injector,\n",
    "        RetryConfig(max_retries=3, base_delay=0.5),\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'âœ…' if result.success else 'âŒ'} Result:\")\n",
    "    print(f\"   Success:     {result.success}\")\n",
    "    print(f\"   Response:    {result.response}\")\n",
    "    print(f\"   Attempts:    {result.attempts}\")\n",
    "    print(f\"   Total delay: {result.total_delay:.1f}s\")\n",
    "    if result.errors:\n",
    "        print(f\"   Errors hit:  {len(result.errors)}\")\n",
    "        for e in result.errors:\n",
    "            print(f\"      â€¢ {e}\")\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d3ca6",
   "metadata": {},
   "source": [
    "## Part 3: 5xx Error Handling with fake 5xx errors\n",
    "\n",
    "Handle server-side errors with appropriate retry strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb74890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š 5xx Error Classification + Retry Demo\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Error Types and Retry Strategies:\n",
      "------------------------------------------------------------\n",
      "   RATE_LIMIT      â”‚ retries=5  base=1.0s  max=60.0s\n",
      "   SERVER_ERROR    â”‚ retries=3  base=2.0s  max=30.0s\n",
      "   CLIENT_ERROR    â”‚ âŒ No retry (fix the request)\n",
      "   TIMEOUT         â”‚ retries=2  base=1.0s  max=10.0s\n",
      "   UNKNOWN         â”‚ retries=1  base=1.0s  max=60.0s\n",
      "\n",
      "\n",
      "ğŸ”¥ Live run: simulating 502 â†’ 503 â†’ real call\n",
      "------------------------------------------------------------\n",
      "   â³ Retry 1/3 after 0.3s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â³ Retry 2/3 after 0.5s...\n",
      "\n",
      "âœ… Result:\n",
      "   Success:     True\n",
      "   Response:    recovered!\n",
      "   Attempts:    3\n",
      "   Total delay: 0.8s\n",
      "   Errors hit:\n",
      "      â€¢ Attempt 1: APIStatusError: [Simulated] 502 error\n",
      "      â€¢ Attempt 2: APIStatusError: [Simulated] 503 error\n"
     ]
    }
   ],
   "source": [
    "# 5xx Error Handler with classification\n",
    "\n",
    "class ErrorType(Enum):\n",
    "    \"\"\"Classification of API errors.\"\"\"\n",
    "    RATE_LIMIT = \"rate_limit\"      # 429\n",
    "    SERVER_ERROR = \"server_error\"  # 500, 502, 503, 504\n",
    "    CLIENT_ERROR = \"client_error\"  # 400, 401, 403\n",
    "    TIMEOUT = \"timeout\"            # Connection/read timeout\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "def classify_error(error: Exception) -> ErrorType:\n",
    "    \"\"\"Classify an error for appropriate handling.\"\"\"\n",
    "    if isinstance(error, RateLimitError):\n",
    "        return ErrorType.RATE_LIMIT\n",
    "    if isinstance(error, APIStatusError):\n",
    "        status = getattr(error, 'status_code', 0)\n",
    "        if status == 429:\n",
    "            return ErrorType.RATE_LIMIT\n",
    "        elif status >= 500:\n",
    "            return ErrorType.SERVER_ERROR\n",
    "        elif status >= 400:\n",
    "            return ErrorType.CLIENT_ERROR\n",
    "    if 'timeout' in str(error).lower():\n",
    "        return ErrorType.TIMEOUT\n",
    "    return ErrorType.UNKNOWN\n",
    "\n",
    "def get_retry_config_for_error(\n",
    "    error_type: ErrorType,\n",
    ") -> Optional[RetryConfig]:\n",
    "    \"\"\"Get appropriate retry config for each error type.\"\"\"\n",
    "    configs = {\n",
    "        ErrorType.RATE_LIMIT: RetryConfig(\n",
    "            max_retries=5, base_delay=1.0, max_delay=60.0,\n",
    "        ),\n",
    "        ErrorType.SERVER_ERROR: RetryConfig(\n",
    "            max_retries=3, base_delay=2.0, max_delay=30.0,\n",
    "        ),\n",
    "        ErrorType.TIMEOUT: RetryConfig(\n",
    "            max_retries=2, base_delay=1.0, max_delay=10.0,\n",
    "        ),\n",
    "        ErrorType.CLIENT_ERROR: None,\n",
    "        ErrorType.UNKNOWN: RetryConfig(\n",
    "            max_retries=1, base_delay=1.0,\n",
    "        ),\n",
    "    }\n",
    "    return configs.get(error_type)\n",
    "\n",
    "\n",
    "# â”€â”€ Live demo: force different error types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class Multi5xxInjector:\n",
    "    \"\"\"Simulates a sequence of 5xx errors followed by\n",
    "    a real successful API call.\"\"\"\n",
    "\n",
    "    ERROR_SEQUENCE = [502, 503, 500]  # rotate through these\n",
    "\n",
    "    def __init__(self, real_func: Callable, fail_count: int = 2):\n",
    "        self.real_func = real_func\n",
    "        self.fail_count = fail_count\n",
    "        self.call_number = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.call_number += 1\n",
    "        if self.call_number <= self.fail_count:\n",
    "            code = self.ERROR_SEQUENCE[\n",
    "                (self.call_number - 1) % len(self.ERROR_SEQUENCE)\n",
    "            ]\n",
    "            fake = httpx.Response(\n",
    "                status_code=code,\n",
    "                json={\n",
    "                    \"error\": {\n",
    "                        \"code\": str(code),\n",
    "                        \"message\": f\"Simulated {code}\",\n",
    "                    }\n",
    "                },\n",
    "                request=httpx.Request(\"POST\", \"https://fake\"),\n",
    "            )\n",
    "            raise APIStatusError(\n",
    "                message=f\"[Simulated] {code} error\",\n",
    "                response=fake,\n",
    "                body=None,\n",
    "            )\n",
    "        return self.real_func()\n",
    "\n",
    "\n",
    "print(\"ğŸ“Š 5xx Error Classification + Retry Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1ï¸âƒ£  Show error classification table\n",
    "print(\"\\nğŸ“‹ Error Types and Retry Strategies:\")\n",
    "print(\"-\" * 60)\n",
    "for error_type in ErrorType:\n",
    "    cfg = get_retry_config_for_error(error_type)\n",
    "    if cfg:\n",
    "        print(\n",
    "            f\"   {error_type.value.upper():15s} â”‚ \"\n",
    "            f\"retries={cfg.max_retries}  \"\n",
    "            f\"base={cfg.base_delay}s  \"\n",
    "            f\"max={cfg.max_delay}s\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"   {error_type.value.upper():15s} â”‚ \"\n",
    "            f\"âŒ No retry (fix the request)\"\n",
    "        )\n",
    "\n",
    "# 2ï¸âƒ£  Live run: inject 502 â†’ 503 â†’ then real call\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    print(\"\\n\\nğŸ”¥ Live run: simulating 502 â†’ 503 â†’ real call\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    def real_5xx_request():\n",
    "        resp = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Say 'recovered!'\",\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    injector_5xx = Multi5xxInjector(\n",
    "        real_5xx_request, fail_count=2\n",
    "    )\n",
    "\n",
    "    server_cfg = get_retry_config_for_error(\n",
    "        ErrorType.SERVER_ERROR\n",
    "    )\n",
    "    server_cfg.base_delay = 0.3  # speed up for demo\n",
    "\n",
    "    result = call_with_retry(injector_5xx, server_cfg)\n",
    "\n",
    "    print(f\"\\n{'âœ…' if result.success else 'âŒ'} Result:\")\n",
    "    print(f\"   Success:     {result.success}\")\n",
    "    print(f\"   Response:    {result.response}\")\n",
    "    print(f\"   Attempts:    {result.attempts}\")\n",
    "    print(f\"   Total delay: {result.total_delay:.1f}s\")\n",
    "    if result.errors:\n",
    "        print(f\"   Errors hit:\")\n",
    "        for e in result.errors:\n",
    "            # Classify each error for demonstration\n",
    "            print(f\"      â€¢ {e}\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\nâš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c961a",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Backend Fallback with non-existent-deployment-xyz\n",
    "\n",
    "Route to backup deployments when primary fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c040bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Multi-Backend Fallback Demo\n",
      "============================================================\n",
      "Scenario: primary backend has a BAD deployment name â†’ should fallback to secondary\n",
      "\n",
      "   ğŸ”„ Trying backend: primary-BROKEN\n",
      "   âŒ Backend primary-BROKEN failed after 1 attempts\n",
      "      â€¢ Attempt 1: NotFoundError: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this r\n",
      "   ğŸ”„ Trying backend: secondary-OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Result:\n",
      "   Success:        True\n",
      "   Response:       hello from fallback\n",
      "   Backend used:   secondary-OK\n",
      "   Backends tried: ['primary-BROKEN', 'secondary-OK']\n",
      "   Total attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# Multi-Backend Fallback implementation\n",
    "\n",
    "@dataclass\n",
    "class BackendConfig:\n",
    "    \"\"\"Configuration for a backend deployment.\"\"\"\n",
    "    name: str\n",
    "    endpoint: str\n",
    "    api_key: str\n",
    "    deployment: str\n",
    "    priority: int = 1  # Lower = higher priority\n",
    "\n",
    "@dataclass\n",
    "class FallbackResult:\n",
    "    \"\"\"Result of a fallback operation.\"\"\"\n",
    "    success: bool\n",
    "    response: Optional[str]\n",
    "    backend_used: str\n",
    "    backends_tried: List[str]\n",
    "    total_attempts: int\n",
    "\n",
    "class FallbackClient:\n",
    "    \"\"\"Client with multi-backend fallback support.\"\"\"\n",
    "\n",
    "    def __init__(self, backends: List[BackendConfig]):\n",
    "        self.backends = sorted(\n",
    "            backends, key=lambda b: b.priority\n",
    "        )\n",
    "        self.clients: Dict[str, AzureOpenAI] = {}\n",
    "        for backend in self.backends:\n",
    "            self.clients[backend.name] = AzureOpenAI(\n",
    "                azure_endpoint=backend.endpoint,\n",
    "                api_key=backend.api_key,\n",
    "                api_version=AZURE_OPENAI_API_VERSION,\n",
    "            )\n",
    "\n",
    "    def complete(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        retry_config: RetryConfig = RetryConfig(),\n",
    "        **kwargs,\n",
    "    ) -> FallbackResult:\n",
    "        \"\"\"Make a completion call with fallback support.\"\"\"\n",
    "        backends_tried = []\n",
    "        total_attempts = 0\n",
    "\n",
    "        for backend in self.backends:\n",
    "            backends_tried.append(backend.name)\n",
    "            client = self.clients[backend.name]\n",
    "\n",
    "            print(f\"   ğŸ”„ Trying backend: {backend.name}\")\n",
    "\n",
    "            def make_request(\n",
    "                _client=client, _backend=backend\n",
    "            ):\n",
    "                return _client.chat.completions.create(\n",
    "                    model=_backend.deployment,\n",
    "                    messages=messages,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            result = call_with_retry(\n",
    "                make_request, retry_config\n",
    "            )\n",
    "            total_attempts += result.attempts\n",
    "\n",
    "            if result.success:\n",
    "                return FallbackResult(\n",
    "                    success=True,\n",
    "                    response=(\n",
    "                        result.response.choices[0]\n",
    "                        .message.content\n",
    "                    ),\n",
    "                    backend_used=backend.name,\n",
    "                    backends_tried=backends_tried,\n",
    "                    total_attempts=total_attempts,\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"   âŒ Backend {backend.name} failed \"\n",
    "                f\"after {result.attempts} attempts\"\n",
    "            )\n",
    "            for err in result.errors:\n",
    "                print(f\"      â€¢ {err}\")\n",
    "\n",
    "        return FallbackResult(\n",
    "            success=False,\n",
    "            response=None,\n",
    "            backend_used=\"none\",\n",
    "            backends_tried=backends_tried,\n",
    "            total_attempts=total_attempts,\n",
    "        )\n",
    "\n",
    "\n",
    "# â”€â”€ Demo: first backend is DELIBERATELY broken â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“Š Multi-Backend Fallback Demo\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    \"Scenario: primary backend has a BAD deployment name \"\n",
    "    \"â†’ should fallback to secondary\\n\"\n",
    ")\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    backends = [\n",
    "        # âŒ Primary â€“ wrong deployment â†’ will fail with 404\n",
    "        BackendConfig(\n",
    "            name=\"primary-BROKEN\",\n",
    "            endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=AZURE_OPENAI_API_KEY,\n",
    "            deployment=\"non-existent-deployment-xyz\",\n",
    "            priority=1,\n",
    "        ),\n",
    "        # âœ… Secondary â€“ correct deployment\n",
    "        BackendConfig(\n",
    "            name=\"secondary-OK\",\n",
    "            endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=AZURE_OPENAI_API_KEY,\n",
    "            deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            priority=2,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    fallback_client = FallbackClient(backends)\n",
    "\n",
    "    result = fallback_client.complete(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Say 'hello from fallback'\",\n",
    "            }\n",
    "        ],\n",
    "        retry_config=RetryConfig(\n",
    "            max_retries=1, base_delay=0.3\n",
    "        ),\n",
    "        max_tokens=20,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'âœ…' if result.success else 'âŒ'} Result:\")\n",
    "    print(f\"   Success:        {result.success}\")\n",
    "    print(f\"   Response:       {result.response}\")\n",
    "    print(f\"   Backend used:   {result.backend_used}\")\n",
    "    print(f\"   Backends tried: {result.backends_tried}\")\n",
    "    print(f\"   Total attempts: {result.total_attempts}\")\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405fcec",
   "metadata": {},
   "source": [
    "## Part 5: Complete Resilient Client with fault-injected 429 / 502 errors\n",
    "\n",
    "Combine all strategies into a production-ready client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1b4cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Resilient Client â€“ Mixed Failure Demo\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Sending 5 queries (some will be fault-injected)â€¦\n",
      "\n",
      "â”€â”€ Query 1: \"What is 2+2?\"  (ğŸŸ¢ clean call)\n",
      "   ğŸ”„ Trying backend: primary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… attempts=1  â†’ 2 + 2 = 4\n",
      "\n",
      "â”€â”€ Query 2: \"Capital of France?\"  (ğŸ”¥ 2 injected fault(s))\n",
      "   â³ Retry 1/3 after 1.0s...\n",
      "   â³ Retry 2/3 after 1.0s...\n",
      "   âœ… attempts=3  â†’ The capital of France is **Paris**.\n",
      "\n",
      "â”€â”€ Query 3: \"Say 'test'\"  (ğŸŸ¢ clean call)\n",
      "   ğŸ”„ Trying backend: primary\n",
      "   âœ… attempts=1  â†’ test\n",
      "\n",
      "â”€â”€ Query 4: \"Color of the sky?\"  (ğŸ”¥ 1 injected fault(s))\n",
      "   â³ Retry 1/3 after 1.0s...\n",
      "   âœ… attempts=2  â†’ The color of the sky is **usually blue** during th\n",
      "\n",
      "â”€â”€ Query 5: \"Say 'done'\"  (ğŸŸ¢ clean call)\n",
      "   ğŸ”„ Trying backend: primary\n",
      "   âœ… attempts=1  â†’ done\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ Client Statistics:\n",
      "   Total requests:  5\n",
      "   Successful:      5\n",
      "   Failed:          0\n",
      "   Success rate:    100.0%\n",
      "   Total retries:   3\n",
      "   Fallbacks used:  0\n"
     ]
    }
   ],
   "source": [
    "# Complete Resilient Client\n",
    "\n",
    "class ResilientOpenAIClient:\n",
    "    \"\"\"\n",
    "    Production-ready OpenAI client with:\n",
    "    - Exponential backoff\n",
    "    - Error classification\n",
    "    - Multi-backend fallback\n",
    "    - Comprehensive logging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backends: List[BackendConfig],\n",
    "        default_retry_config: RetryConfig = RetryConfig(),\n",
    "    ):\n",
    "        self.fallback_client = FallbackClient(backends)\n",
    "        self.default_retry_config = default_retry_config\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"total_retries\": 0,\n",
    "            \"fallbacks_used\": 0,\n",
    "        }\n",
    "\n",
    "    def complete(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        **kwargs,\n",
    "    ) -> FallbackResult:\n",
    "        \"\"\"Make a resilient completion call.\"\"\"\n",
    "        self.stats[\"total_requests\"] += 1\n",
    "\n",
    "        result = self.fallback_client.complete(\n",
    "            messages=messages,\n",
    "            retry_config=self.default_retry_config,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            self.stats[\"successful_requests\"] += 1\n",
    "        else:\n",
    "            self.stats[\"failed_requests\"] += 1\n",
    "        self.stats[\"total_retries\"] += (\n",
    "            result.total_attempts - 1\n",
    "        )\n",
    "        if len(result.backends_tried) > 1:\n",
    "            self.stats[\"fallbacks_used\"] += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get client statistics.\"\"\"\n",
    "        stats = self.stats.copy()\n",
    "        if stats[\"total_requests\"] > 0:\n",
    "            stats[\"success_rate\"] = (\n",
    "                stats[\"successful_requests\"]\n",
    "                / stats[\"total_requests\"]\n",
    "                * 100\n",
    "            )\n",
    "            stats[\"avg_retries\"] = (\n",
    "                stats[\"total_retries\"]\n",
    "                / stats[\"total_requests\"]\n",
    "            )\n",
    "        return stats\n",
    "\n",
    "\n",
    "# â”€â”€ Demo: mixed failure scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# We send 5 queries through the resilient client.\n",
    "# Queries 2 and 4 get fault-injected 429 errors that\n",
    "# require retries, while the others succeed on first try.\n",
    "\n",
    "print(\"ğŸ“Š Resilient Client â€“ Mixed Failure Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    backends = [\n",
    "        BackendConfig(\n",
    "            name=\"primary\",\n",
    "            endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=AZURE_OPENAI_API_KEY,\n",
    "            deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            priority=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    resilient_client = ResilientOpenAIClient(\n",
    "        backends=backends,\n",
    "        default_retry_config=RetryConfig(\n",
    "            max_retries=3, base_delay=0.3\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    queries = [\n",
    "        (\"What is 2+2?\", 0),      # clean\n",
    "        (\"Capital of France?\", 2), # 2Ã— forced 429\n",
    "        (\"Say 'test'\", 0),         # clean\n",
    "        (\"Color of the sky?\", 1),  # 1Ã— forced 502\n",
    "        (\"Say 'done'\", 0),         # clean\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ“ Sending 5 queries (some will be \"\n",
    "          \"fault-injected)â€¦\\n\")\n",
    "\n",
    "    for idx, (query, n_faults) in enumerate(queries, 1):\n",
    "        label = (\n",
    "            f\"ğŸ”¥ {n_faults} injected fault(s)\"\n",
    "            if n_faults\n",
    "            else \"ğŸŸ¢ clean call\"\n",
    "        )\n",
    "        print(f\"â”€â”€ Query {idx}: \\\"{query}\\\"  ({label})\")\n",
    "\n",
    "        # Monkey-patch the fallback client so its\n",
    "        # internal make_request goes through the injector\n",
    "        original_complete = (\n",
    "            resilient_client.fallback_client.complete\n",
    "        )\n",
    "\n",
    "        if n_faults:\n",
    "            # Wrap with injector\n",
    "            def patched_complete(\n",
    "                messages,\n",
    "                retry_config=RetryConfig(),\n",
    "                _nf=n_faults,\n",
    "                **kw,\n",
    "            ):\n",
    "                backend = (\n",
    "                    resilient_client.fallback_client\n",
    "                    .backends[0]\n",
    "                )\n",
    "                _client = (\n",
    "                    resilient_client.fallback_client\n",
    "                    .clients[backend.name]\n",
    "                )\n",
    "\n",
    "                def real_fn():\n",
    "                    return _client.chat.completions.create(\n",
    "                        model=backend.deployment,\n",
    "                        messages=messages,\n",
    "                        **kw,\n",
    "                    )\n",
    "\n",
    "                injector = FaultInjector(\n",
    "                    real_fn, fail_count=_nf\n",
    "                )\n",
    "                r = call_with_retry(injector, retry_config)\n",
    "                return FallbackResult(\n",
    "                    success=r.success,\n",
    "                    response=(\n",
    "                        r.response.choices[0]\n",
    "                        .message.content\n",
    "                        if r.success else None\n",
    "                    ),\n",
    "                    backend_used=backend.name,\n",
    "                    backends_tried=[backend.name],\n",
    "                    total_attempts=r.attempts,\n",
    "                )\n",
    "\n",
    "            resilient_client.fallback_client.complete = (\n",
    "                patched_complete\n",
    "            )\n",
    "        else:\n",
    "            resilient_client.fallback_client.complete = (\n",
    "                original_complete\n",
    "            )\n",
    "\n",
    "        result = resilient_client.complete(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "        )\n",
    "\n",
    "        # Restore original\n",
    "        resilient_client.fallback_client.complete = (\n",
    "            original_complete\n",
    "        )\n",
    "\n",
    "        icon = \"âœ…\" if result.success else \"âŒ\"\n",
    "        snippet = (\n",
    "            result.response[:50]\n",
    "            if result.response else \"N/A\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   {icon} attempts={result.total_attempts}  \"\n",
    "            f\"â†’ {snippet}\\n\"\n",
    "        )\n",
    "\n",
    "    # Display stats\n",
    "    stats = resilient_client.get_stats()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“ˆ Client Statistics:\")\n",
    "    print(f\"   Total requests:  {stats['total_requests']}\")\n",
    "    print(f\"   Successful:      \"\n",
    "          f\"{stats['successful_requests']}\")\n",
    "    print(f\"   Failed:          {stats['failed_requests']}\")\n",
    "    print(f\"   Success rate:    \"\n",
    "          f\"{stats.get('success_rate', 0):.1f}%\")\n",
    "    print(f\"   Total retries:   {stats['total_retries']}\")\n",
    "    print(f\"   Fallbacks used:  {stats['fallbacks_used']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"âš ï¸ AZURE_OPENAI_ENDPOINT or \"\n",
    "        \"AZURE_OPENAI_API_KEY not set\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791101c1",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### Retry Strategy Guidelines\n",
    "\n",
    "| Scenario | Max Retries | Base Delay | Strategy |\n",
    "|----------|-------------|------------|----------|\n",
    "| Rate limits (429) | 5 | 1s | Exponential + Retry-After |\n",
    "| Server errors (5xx) | 3 | 2s | Exponential + jitter |\n",
    "| Timeouts | 2 | 1s | Quick retry |\n",
    "| Client errors (4xx) | 0 | N/A | Don't retry, fix request |\n",
    "\n",
    "### Code-Level Fallback vs API Gateway Routing\n",
    "\n",
    "| Aspect | Code-Level Fallback (this notebook) | API Gateway Routing (e.g. APIM) |\n",
    "|--------|--------------------------------------|--------------------------------|\n",
    "| **Where it lives** | Inside application code | Infrastructure (gateway) layer |\n",
    "| **Deployment changes** | Code change + redeploy required | Policy update â€” instant, no redeploy |\n",
    "| **Language dependency** | Must implement per language (Python, JS, â€¦) | Language-agnostic â€” transparent to all clients |\n",
    "| **Fine-grained control** | âœ… Custom logic per request | âš ï¸ Applied uniformly via policy rules |\n",
    "| **Observability** | Application logs only | Centralized dashboard & metrics |\n",
    "| **Complexity at scale** | Risk of duplicated logic across services | Single place for consistent policy management |\n",
    "| **Latency overhead** | Direct call â€” no extra hop | Gateway hop â€” slight added latency |\n",
    "| **Cost** | No additional infrastructure | Gateway service cost (e.g. APIM tier) |\n",
    "| **Operational ownership** | Owned by each dev team | Managed centrally by platform team |\n",
    "\n",
    "> **Recommendation**: For prototypes or single-service apps, **Code-Level** fallback is fast and simple.\n",
    "> In production, where multiple services share the same AI backends, centralizing retry/fallback policies in an **API Gateway** (e.g. Azure API Management) is more operationally efficient.\n",
    "> You can also **combine both** â€” let the gateway handle primary retry/routing, and keep application-level fallback for business-logic-specific scenarios.\n",
    "\n",
    "### Architecture Recommendations\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Resilient AI Client                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Error Classification (retryable vs non-retryable)          â”‚\n",
    "â”‚  2. Exponential Backoff with Jitter                            â”‚\n",
    "â”‚  3. Retry-After Header Respect                                  â”‚\n",
    "â”‚  4. Multi-Backend Fallback (Primary â†’ Secondary â†’ Tertiary)    â”‚\n",
    "â”‚  5. Comprehensive Metrics/Logging                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c7241",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Exponential Backoff**: Implement proper retry delays with jitter\n",
    "2. **429 Handling**: Respect rate limits and Retry-After headers\n",
    "3. **5xx Handling**: Classify and handle server errors appropriately\n",
    "4. **Multi-Backend Fallback**: Route to backup deployments on failure\n",
    "5. **Resilient Client**: Combine strategies for production use\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure OpenAI Rate Limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits)\n",
    "- [Retry Pattern - Cloud Design Patterns](https://learn.microsoft.com/en-us/azure/architecture/patterns/retry)\n",
    "- [Circuit Breaker Pattern](https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673bbbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
