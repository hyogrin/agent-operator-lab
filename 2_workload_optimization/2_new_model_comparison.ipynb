{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd09c4",
   "metadata": {},
   "source": [
    "# Optimization: Model Comparison & Benchmark\n",
    "----\n",
    "\n",
    "This notebook focuses on **Workload Optimization** through model comparison and benchmarking. You will learn how to systematically compare **two models you select** (e.g., GPT-4o vs GPT-5.1) with the **Azure OpenAI Responses API** and measure:\n",
    "\n",
    "- **Latency**: Response time for different scenarios\n",
    "- **Token Usage**: Input, output, and cached tokens\n",
    "- **Cost Efficiency**: Price per request with prompt caching\n",
    "- **Accuracy**: Answer correctness for enterprise scenarios\n",
    "\n",
    "## Key Features\n",
    "- Fair 7-dimension aligned comparison (same API, cache, padding, etc.)\n",
    "- Two-model selection via variables (no hard-coded model pair)\n",
    "- Prompt caching support with Azure OpenAI\n",
    "- Enterprise scenario coverage (intent, sentiment, RAG, code, customer service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c90e39",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Why Model Comparison Matters](#why-model-comparison-matters)\n",
    "- [Setup](#setup)\n",
    "- [Step 1: Configure Pricing](#step-1-configure-pricing)\n",
    "- [Step 2: Define Test Scenarios](#step-2-define-test-scenarios)\n",
    "- [Step 3: Create Static Padding for Cache Eligibility](#step-3-create-static-padding-for-cache-eligibility)\n",
    "- [Step 4: Implement Core Benchmark Functions](#step-4-implement-core-benchmark-functions)\n",
    "- [Step 5: Cache Warmup](#step-5-cache-warmup)\n",
    "- [Step 6: Run Benchmark](#step-6-run-benchmark)\n",
    "- [Step 7: Analyze Results](#step-7-analyze-results)\n",
    "- [Additional Resources](#additional-resources)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1374a",
   "metadata": {},
   "source": [
    "## Why Model Comparison Matters\n",
    "\n",
    "### Enterprise Migration Decision Support\n",
    "\n",
    "When migrating between model versions, organizations need to consider:\n",
    "\n",
    "| Dimension | Description |\n",
    "|-----------|-------------|\n",
    "| **Performance** | Response latency, time-to-first-token (TTFT) |\n",
    "| **Cost** | Input/output token pricing, cache hit savings |\n",
    "| **Quality** | Accuracy for specific use cases |\n",
    "| **Scalability** | Behavior under different workloads |\n",
    "\n",
    "### Fair Comparison Methodology\n",
    "\n",
    "This benchmark ensures fairness through:\n",
    "\n",
    "1. **Same API**: Using Responses API for both models\n",
    "2. **Same Questions**: Identical test scenarios\n",
    "3. **Same Cache Key**: Consistent `prompt_cache_key` for cache routing\n",
    "4. **Warmup Phase**: Populate cache before measurement\n",
    "5. **Multiple Runs**: Statistical significance through repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c16f96",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d770c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI package imported successfully\n",
      "‚úÖ Loaded settings from '../0_setup/.foundry_config.json'\n",
      "\n",
      "üìå Azure OpenAI Endpoint: https://foundry-rq90gs.openai.azure.com...\n",
      "üìå API Key: ‚úÖ Set\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Verify required packages\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    print(\"‚úÖ Azure OpenAI package imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå ERROR: openai package not installed.\")\n",
    "    print(\"   Run: pip install openai>=1.60.0\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Load Foundry project settings\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"‚úÖ Loaded settings from '{config_file}'\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'.\")\n",
    "    print('üí° Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "# Extract configuration values\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_API_VERSION = \"2025-04-01-preview\"\n",
    "\n",
    "print(f\"\\nüìå Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT[:50]}...\" if AZURE_OPENAI_ENDPOINT else \"‚ö†Ô∏è AZURE_OPENAI_ENDPOINT not set\")\n",
    "print(f\"üìå API Key: {'‚úÖ Set' if AZURE_OPENAI_API_KEY else '‚ö†Ô∏è Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c71317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All required environment variables are set\n",
      "‚úÖ Azure OpenAI client initialized\n",
      "   Base URL: https://foundry-rq90gs.openai.azure.com/openai/v1/\n"
     ]
    }
   ],
   "source": [
    "# Validate required environment variables\n",
    "if not AZURE_OPENAI_ENDPOINT or not AZURE_OPENAI_API_KEY:\n",
    "    print(\"‚ùå ERROR: Missing required environment variables!\")\n",
    "    print(\"\\nPlease set:\")\n",
    "    print(\"  export AZURE_OPENAI_ENDPOINT='https://your-resource.openai.azure.com/'\")\n",
    "    print(\"  export AZURE_OPENAI_API_KEY='your-api-key'\")\n",
    "    print(\"\\nOr add them to your .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are set\")\n",
    "    \n",
    "# Initialize Azure OpenAI client with Responses API base URL\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "print(f\"‚úÖ Azure OpenAI client initialized\")\n",
    "print(f\"   Base URL: {AZURE_OPENAI_ENDPOINT.rstrip('/')}/openai/v1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725cf91",
   "metadata": {},
   "source": [
    "## Step 1: Configure Pricing\n",
    "\n",
    "Define the pricing structure for each model. This allows us to calculate costs accurately.\n",
    "\n",
    "**Azure OpenAI Pricing** (per 1M tokens):\n",
    "- Prices are based on official Azure OpenAI pricing\n",
    "- Cached input tokens are significantly cheaper than uncached tokens\n",
    "- Source: [Azure OpenAI Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e140073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Pricing (per 1M tokens)\n",
      "============================================================\n",
      "Model               Input    Cached Input       Output\n",
      "------------------------------------------------------------\n",
      "gpt-4o       $      2.50 $         1.25 $     10.00\n",
      "gpt-5.1      $      1.25 $         0.13 $     10.00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Pricing configuration (per 1M tokens) - Azure OpenAI Official Pricing\n",
    "# Source: https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/\n",
    "\n",
    "PRICING: Dict[str, Dict[str, float]] = {\n",
    "    \"gpt-4o\": {\n",
    "        \"input\": 2.50,           # $2.50 per 1M input tokens\n",
    "        \"cached_input\": 1.25,    # $1.25 per 1M cached input tokens (50% discount)\n",
    "        \"output\": 10.00,         # $10.00 per 1M output tokens\n",
    "    },\n",
    "    \"gpt-5.1\": {\n",
    "        \"input\": 1.25,           # $1.25 per 1M input tokens\n",
    "        \"cached_input\": 0.13,    # $0.13 per 1M cached input tokens (~90% discount)\n",
    "        \"output\": 10.00,         # $10.00 per 1M output tokens\n",
    "    },\n",
    "}\n",
    "\n",
    "# Display pricing table\n",
    "print(\"üìä Model Pricing (per 1M tokens)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<12} {'Input':>12} {'Cached Input':>15} {'Output':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for model, prices in PRICING.items():\n",
    "    print(f\"{model:<12} ${prices['input']:>10.2f} ${prices['cached_input']:>13.2f} ${prices['output']:>10.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8814cd",
   "metadata": {},
   "source": [
    "## Step 2: Define Test Scenarios\n",
    "\n",
    "We define various test scenarios covering different enterprise use cases:\n",
    "\n",
    "| Category | Description | Expected Response |\n",
    "|----------|-------------|-------------------|\n",
    "| **Short** | Intent classification, sentiment analysis | Single word/phrase |\n",
    "| **Medium** | RAG Q&A, code explanation, fact extraction | 1-3 sentences |\n",
    "| **Long** | Customer service replies, content generation | Paragraph (50-100 words) |\n",
    "\n",
    "Each scenario includes:\n",
    "- `category`: Response length category\n",
    "- `name`: Descriptive name\n",
    "- `question`: The prompt to send\n",
    "- `answer_variants`: Expected keywords in the response (for accuracy check)\n",
    "- `language`: EN (English) or ZH (Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a439c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Test Scenarios Summary\n",
      "============================================================\n",
      "1. [Short] Intent Classification (EN)\n",
      "2. [Short] Sentiment Analysis (EN)\n",
      "3. [Medium] RAG Number Extraction (EN)\n",
      "4. [Medium] RAG Fact Extraction (EN)\n",
      "5. [Medium] Code Explanation (EN)\n",
      "6. [Long] Customer Service Reply (EN)\n",
      "7. [Long] Product Description (EN)\n",
      "\n",
      "‚úÖ Total scenarios: 7\n"
     ]
    }
   ],
   "source": [
    "# Test Scenarios - Enterprise use cases for migration validation\n",
    "\n",
    "TEST_SCENARIOS: List[Dict[str, Any]] = [\n",
    "    # =========================================================================\n",
    "    # SHORT RESPONSE SCENARIOS (Intent Classification, Sentiment)\n",
    "    # =========================================================================\n",
    "    {\n",
    "        \"category\": \"Short\",\n",
    "        \"name\": \"Intent Classification\",\n",
    "        \"question\": \"Classify the intent (complaint/inquiry/praise/request): 'My delivery is 2 hours late!' Answer with ONE word.\",\n",
    "        \"answer_variants\": [\"complaint\", \"Complaint\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Short\",\n",
    "        \"name\": \"Sentiment Analysis\",\n",
    "        \"question\": \"What is the sentiment of 'The service was amazing!' (positive/negative/neutral)? ONE word.\",\n",
    "        \"answer_variants\": [\"positive\", \"Positive\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MEDIUM RESPONSE SCENARIOS (RAG Q&A, Code Explanation)\n",
    "    # =========================================================================\n",
    "    {\n",
    "        \"category\": \"Medium\",\n",
    "        \"name\": \"RAG Number Extraction\",\n",
    "        \"question\": \"Based on: 'Company ABC reported revenue of $2.76 billion in 2023, a 26% increase.' What was the revenue? Growth rate? Brief answer.\",\n",
    "        \"answer_variants\": [\"2.76\", \"26\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Medium\",\n",
    "        \"name\": \"RAG Fact Extraction\",\n",
    "        \"question\": \"Based on: 'TechCorp was founded in 2018 in Seattle by Sarah Chen.' When and where was it founded? Brief answer.\",\n",
    "        \"answer_variants\": [\"2018\", \"Seattle\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Medium\",\n",
    "        \"name\": \"Code Explanation\",\n",
    "        \"question\": \"Explain this code: def f(n): return n if n<=1 else f(n-1)+f(n-2). Answer in 2 sentences.\",\n",
    "        \"answer_variants\": [\"fibonacci\", \"recursive\", \"Fibonacci\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LONG RESPONSE SCENARIOS (Content Generation, Customer Service)\n",
    "    # =========================================================================\n",
    "    {\n",
    "        \"category\": \"Long\",\n",
    "        \"name\": \"Customer Service Reply\",\n",
    "        \"question\": \"You are a customer service agent. User says: 'My order is 2 hours late!' Generate an apology and solution in about 50 words.\",\n",
    "        \"answer_variants\": [\"sorry\", \"apologize\", \"refund\", \"compensation\", \"delay\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Long\",\n",
    "        \"name\": \"Product Description\",\n",
    "        \"question\": \"Write a 50-word description for a smart water bottle with hydration tracking features.\",\n",
    "        \"answer_variants\": [\"hydration\", \"track\", \"smart\", \"water\"],\n",
    "        \"language\": \"EN\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Display test scenarios summary\n",
    "print(\"üìã Test Scenarios Summary\")\n",
    "print(\"=\" * 60)\n",
    "for i, scenario in enumerate(TEST_SCENARIOS, 1):\n",
    "    print(f\"{i}. [{scenario['category']}] {scenario['name']} ({scenario['language']})\")\n",
    "print(f\"\\n‚úÖ Total scenarios: {len(TEST_SCENARIOS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e5445",
   "metadata": {},
   "source": [
    "## Step 3: Create Static Padding for Cache Eligibility\n",
    "\n",
    "Azure OpenAI's prompt caching requires a **minimum of 1024 tokens** in the prompt prefix to be cache-eligible.\n",
    "\n",
    "We create a realistic enterprise knowledge base document (~1030 tokens) that:\n",
    "- Acts as context for all test scenarios\n",
    "- Enables prompt caching for both models\n",
    "- Simulates real-world RAG (Retrieval-Augmented Generation) scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2015bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Static Padding Created\n",
      "   Character count: 5,345\n",
      "   Estimated tokens: ~1,336\n",
      "   Cache eligible: ‚úÖ Yes (>1024 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Static Padding - Must be >= 1024 tokens for Azure prompt cache eligibility\n",
    "# This simulates a realistic enterprise knowledge base\n",
    "\n",
    "STATIC_PADDING = \"\"\"[ENTERPRISE KNOWLEDGE BASE - VERSION 2024.12]\n",
    "================================================================================\n",
    "\n",
    "SECTION 1: CUSTOMER SERVICE PROTOCOLS\n",
    "================================================================================\n",
    "\n",
    "1.1 Response Time Standards:\n",
    "- Priority 1 (Order Issues): Response within 2 minutes, resolution within 15 minutes\n",
    "- Priority 2 (Payment Issues): Response within 5 minutes, resolution within 30 minutes\n",
    "- Priority 3 (General Inquiry): Response within 10 minutes, resolution within 2 hours\n",
    "- Priority 4 (Feedback/Suggestions): Response within 24 hours\n",
    "\n",
    "1.2 Compensation Guidelines:\n",
    "- Delivery delay 30-60 minutes: $5 coupon\n",
    "- Delivery delay over 60 minutes: $10 coupon + free delivery next order\n",
    "- Wrong item delivered: Full refund + replacement + $15 coupon\n",
    "- Food quality issue: Full refund + $20 coupon\n",
    "\n",
    "1.3 Escalation Procedures:\n",
    "- Level 1: Frontline agent handles standard issues\n",
    "- Level 2: Senior agent handles complex complaints\n",
    "- Level 3: Supervisor handles escalated disputes\n",
    "- Level 4: Quality assurance team handles legal/PR issues\n",
    "\n",
    "================================================================================\n",
    "\n",
    "SECTION 2: E-COMMERCE OPERATIONS\n",
    "================================================================================\n",
    "\n",
    "2.1 Order Lifecycle States:\n",
    "- PENDING: Order placed, awaiting merchant confirmation\n",
    "- CONFIRMED: Merchant accepted, preparing order\n",
    "- PREPARING: Kitchen/warehouse processing\n",
    "- READY: Order ready for pickup by rider\n",
    "- DISPATCHED: Rider picked up, in transit\n",
    "- ARRIVING: Rider within 500m of delivery address\n",
    "- DELIVERED: Order handed to customer\n",
    "- COMPLETED: Customer confirmed receipt\n",
    "- CANCELLED: Order cancelled (with reason code)\n",
    "- REFUNDED: Refund processed\n",
    "\n",
    "2.2 Merchant Categories:\n",
    "- Restaurant (Chinese, Western, Japanese, Korean, Fast Food)\n",
    "- Grocery (Fresh Produce, Dairy, Snacks, Beverages)\n",
    "- Pharmacy (OTC, Prescription, Health Products)\n",
    "- Convenience Store (24/7 essentials)\n",
    "- Specialty Shops (Bakery, Desserts, Coffee)\n",
    "\n",
    "2.3 Delivery Optimization:\n",
    "- Smart routing algorithm considers traffic, weather, rider capacity\n",
    "- Batching orders from same merchant to nearby addresses\n",
    "- Peak hour surge pricing and rider incentives\n",
    "- Quality metrics: On-time rate, customer rating, order accuracy\n",
    "\n",
    "================================================================================\n",
    "\n",
    "SECTION 3: DATA ANALYTICS FRAMEWORK\n",
    "================================================================================\n",
    "\n",
    "3.1 Key Performance Indicators:\n",
    "- GMV (Gross Merchandise Value): Total transaction value\n",
    "- Take Rate: Revenue as percentage of GMV\n",
    "- Order Frequency: Orders per user per month\n",
    "- Customer Acquisition Cost (CAC): Marketing spend per new user\n",
    "- Customer Lifetime Value (LTV): Predicted total revenue per user\n",
    "- Rider Efficiency: Orders delivered per hour per rider\n",
    "\n",
    "3.2 Reporting Cadence:\n",
    "- Real-time: Order volume, active riders, system health\n",
    "- Hourly: Regional performance, surge status\n",
    "- Daily: Revenue, costs, profit margins\n",
    "- Weekly: Trend analysis, anomaly detection\n",
    "- Monthly: Executive summary, strategic metrics\n",
    "- Quarterly: Investor reports, market analysis\n",
    "\n",
    "3.3 Data Quality Standards:\n",
    "- Completeness: All required fields populated\n",
    "- Accuracy: Values within expected ranges\n",
    "- Timeliness: Data available within SLA\n",
    "- Consistency: No conflicting records\n",
    "\n",
    "================================================================================\n",
    "\n",
    "SECTION 4: TECHNICAL ARCHITECTURE\n",
    "================================================================================\n",
    "\n",
    "4.1 Microservices:\n",
    "- Order Service: Order creation, modification, cancellation\n",
    "- User Service: Authentication, profile management\n",
    "- Merchant Service: Menu management, inventory, hours\n",
    "- Rider Service: Assignment, tracking, earnings\n",
    "- Payment Service: Transactions, refunds, settlements\n",
    "- Notification Service: Push, SMS, in-app messages\n",
    "\n",
    "4.2 Infrastructure:\n",
    "- Multi-region deployment for high availability\n",
    "- Kubernetes clusters with auto-scaling\n",
    "- Redis clusters for caching and session management\n",
    "- MySQL clusters with read replicas\n",
    "- Kafka for event streaming\n",
    "- Elasticsearch for search and analytics\n",
    "\n",
    "4.3 API Standards:\n",
    "- RESTful design with versioned endpoints\n",
    "- OAuth 2.0 authentication\n",
    "- Rate limiting: 1000 requests per minute per client\n",
    "- Response format: JSON with standard error codes\n",
    "- Pagination: Cursor-based for large datasets\n",
    "\n",
    "================================================================================\n",
    "\n",
    "SECTION 5: COMPLIANCE AND SECURITY\n",
    "================================================================================\n",
    "\n",
    "5.1 Data Protection:\n",
    "- PII encryption at rest (AES-256), TLS 1.3 for data in transit\n",
    "- Data retention: 3 years for transactions, 1 year for logs\n",
    "- Right to deletion: Process within 30 days\n",
    "\n",
    "5.2 Food Safety:\n",
    "- Merchant license verification\n",
    "- Regular hygiene inspections\n",
    "- Temperature monitoring for cold chain\n",
    "- Allergen information disclosure\n",
    "\n",
    "5.3 Financial Compliance:\n",
    "- Anti-money laundering (AML) monitoring\n",
    "- Transaction limits per user per day\n",
    "- Fraud detection and prevention\n",
    "- Regular audit trails\n",
    "\n",
    "================================================================================\n",
    "[END OF KNOWLEDGE BASE]\n",
    "================================================================================\n",
    "\n",
    "Based on the above context, please answer the following question:\n",
    "\"\"\"\n",
    "\n",
    "# Estimate token count (rough approximation: ~4 chars per token)\n",
    "estimated_tokens = len(STATIC_PADDING) // 4\n",
    "print(f\"üìÑ Static Padding Created\")\n",
    "print(f\"   Character count: {len(STATIC_PADDING):,}\")\n",
    "print(f\"   Estimated tokens: ~{estimated_tokens:,}\")\n",
    "print(f\"   Cache eligible: {'‚úÖ Yes (>1024 tokens)' if estimated_tokens >= 1024 else '‚ùå No (<1024 tokens)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28754bd",
   "metadata": {},
   "source": [
    "## Step 4: Implement Core Benchmark Functions\n",
    "\n",
    "We implement three core functions:\n",
    "\n",
    "1. **`calculate_cost()`**: Computes the cost in USD based on token usage\n",
    "2. **`check_answer()`**: Validates if the response contains expected keywords\n",
    "3. **`test_with_cache_key()`**: Executes a single benchmark request with cache support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5bffc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined:\n",
      "   - calculate_cost(model, input_tokens, output_tokens, cached_tokens)\n",
      "   - check_answer(response, correct_variants)\n",
      "\n",
      "üìä Example cost calculation (gpt-4o, 1000 input, 100 output, 500 cached):\n",
      "   Cost: $0.002875\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(model: str, input_tokens: int, output_tokens: int, \n",
    "                   cached_tokens: int = 0) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the request cost in USD based on token usage.\n",
    "    \n",
    "    Parameters:\n",
    "        model (str): Model name (e.g., 'gpt-4o', 'gpt-5.1')\n",
    "        input_tokens (int): Total input tokens\n",
    "        output_tokens (int): Total output tokens\n",
    "        cached_tokens (int): Number of cached input tokens (default: 0)\n",
    "    \n",
    "    Returns:\n",
    "        float: Cost in USD\n",
    "    \"\"\"\n",
    "    pricing = PRICING[model]\n",
    "    \n",
    "    # Uncached tokens = total input - cached\n",
    "    uncached_tokens = input_tokens - cached_tokens\n",
    "    \n",
    "    # Calculate cost: (uncached * input_price + cached * cached_price + output * output_price)\n",
    "    cost = (\n",
    "        uncached_tokens * pricing[\"input\"] +\n",
    "        cached_tokens * pricing[\"cached_input\"] +\n",
    "        output_tokens * pricing[\"output\"]\n",
    "    ) / 1_000_000  # Convert from per-1M to actual\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def check_answer(response: str, correct_variants: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the response contains any of the expected answer variants.\n",
    "    \n",
    "    Parameters:\n",
    "        response (str): The model's response text\n",
    "        correct_variants (List[str]): List of acceptable answer keywords\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if any variant is found in the response\n",
    "    \"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    for variant in correct_variants:\n",
    "        if variant.lower() in response_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Test the helper functions\n",
    "print(\"‚úÖ Helper functions defined:\")\n",
    "print(\"   - calculate_cost(model, input_tokens, output_tokens, cached_tokens)\")\n",
    "print(\"   - check_answer(response, correct_variants)\")\n",
    "\n",
    "# Example cost calculation\n",
    "example_cost = calculate_cost(\"gpt-4o\", input_tokens=1000, output_tokens=100, cached_tokens=500)\n",
    "print(f\"\\nüìä Example cost calculation (gpt-4o, 1000 input, 100 output, 500 cached):\")\n",
    "print(f\"   Cost: ${example_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7693f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_with_cache_key() function defined\n",
      "   - Supports both streaming and non-streaming modes\n",
      "   - Tracks latency, token usage, and cache hits\n",
      "   - Returns structured result dictionary\n"
     ]
    }
   ],
   "source": [
    "def test_with_cache_key(\n",
    "    client: AzureOpenAI,\n",
    "    model: str,\n",
    "    instructions: str,\n",
    "    question: str,\n",
    "    cache_key: str,\n",
    "    reasoning_effort: Optional[str] = None,\n",
    "    stream: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test a model using Responses API with prompt_cache_key.\n",
    "    \n",
    "    Parameters:\n",
    "        client: OpenAI client configured for Responses API\n",
    "        model (str): Model name (gpt-4o or gpt-5.1)\n",
    "        instructions (str): System instructions (should be >1024 tokens for caching)\n",
    "        question (str): User question\n",
    "        cache_key (str): Prompt cache key for cache routing\n",
    "        reasoning_effort (str): For GPT-5.1, set to \"none\", \"low\", \"medium\", or \"high\"\n",
    "        stream (bool): Whether to use streaming mode\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains latency, tokens, content, and success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build request parameters\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"instructions\": instructions,\n",
    "            \"input\": question,\n",
    "            \"max_output_tokens\": 100,\n",
    "            \"extra_body\": {\"prompt_cache_key\": cache_key}\n",
    "        }\n",
    "        \n",
    "        # Add reasoning effort for GPT-5.1 (controls thinking depth)\n",
    "        if reasoning_effort and \"5.1\" in model:\n",
    "            params[\"reasoning\"] = {\"effort\": reasoning_effort}\n",
    "        \n",
    "        # Add streaming flag\n",
    "        if stream:\n",
    "            params[\"stream\"] = True\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if stream:\n",
    "            # =====================\n",
    "            # STREAMING MODE\n",
    "            # =====================\n",
    "            response_stream = client.responses.create(**params)\n",
    "            content = \"\"\n",
    "            input_tokens = 0\n",
    "            output_tokens = 0\n",
    "            cached_tokens = 0\n",
    "            first_token_time = None\n",
    "            \n",
    "            for event in response_stream:\n",
    "                # Record time to first token\n",
    "                if first_token_time is None and hasattr(event, 'type'):\n",
    "                    if event.type in ['response.output_item.added', \n",
    "                                      'response.content_part.added', \n",
    "                                      'response.output_text.delta']:\n",
    "                        first_token_time = time.time() - start_time\n",
    "                \n",
    "                # Extract text from delta events\n",
    "                if hasattr(event, 'type') and event.type == 'response.output_text.delta':\n",
    "                    if hasattr(event, 'delta'):\n",
    "                        content += event.delta\n",
    "                \n",
    "                # Extract usage from completed event\n",
    "                if hasattr(event, 'type') and event.type == 'response.completed':\n",
    "                    if hasattr(event, 'response') and hasattr(event.response, 'usage'):\n",
    "                        usage = event.response.usage\n",
    "                        input_tokens = usage.input_tokens\n",
    "                        output_tokens = usage.output_tokens\n",
    "                        cached_details = getattr(usage, 'input_tokens_details', None)\n",
    "                        if cached_details:\n",
    "                            cached_tokens = getattr(cached_details, 'cached_tokens', 0)\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"latency\": latency,\n",
    "                \"first_token_time\": first_token_time or latency,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"cached_tokens\": cached_tokens,\n",
    "                \"content\": content,\n",
    "                \"stream\": True\n",
    "            }\n",
    "        else:\n",
    "            # =====================\n",
    "            # NON-STREAMING MODE\n",
    "            # =====================\n",
    "            response = client.responses.create(**params)\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Extract token usage\n",
    "            usage = response.usage\n",
    "            input_tokens = usage.input_tokens\n",
    "            output_tokens = usage.output_tokens\n",
    "            cached_details = getattr(usage, 'input_tokens_details', {})\n",
    "            cached_tokens = getattr(cached_details, 'cached_tokens', 0) if cached_details else 0\n",
    "            \n",
    "            # Extract response content\n",
    "            content = \"\"\n",
    "            if response.output:\n",
    "                for item in response.output:\n",
    "                    if hasattr(item, 'content'):\n",
    "                        for c in item.content:\n",
    "                            if hasattr(c, 'text'):\n",
    "                                content += c.text\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"latency\": latency,\n",
    "                \"first_token_time\": latency,  # Same as latency for non-streaming\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"cached_tokens\": cached_tokens,\n",
    "                \"content\": content,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Return error result\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"latency\": 0,\n",
    "            \"first_token_time\": 0,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"cached_tokens\": 0,\n",
    "            \"content\": \"\",\n",
    "            \"stream\": stream\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ test_with_cache_key() function defined\")\n",
    "print(\"   - Supports both streaming and non-streaming modes\")\n",
    "print(\"   - Tracks latency, token usage, and cache hits\")\n",
    "print(\"   - Returns structured result dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32681c85",
   "metadata": {},
   "source": [
    "## Step 5: Cache Warmup\n",
    "\n",
    "Before running the actual benchmark, we need to **warm up the cache** for both models.\n",
    "\n",
    "**Why warmup matters:**\n",
    "- First requests always miss the cache (cold start)\n",
    "- Subsequent requests benefit from cached prompts\n",
    "- Warmup ensures fair measurement of cache effectiveness\n",
    "\n",
    "We send 3 warmup requests per model with a simple question to populate the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "563bf976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Benchmark Configuration\n",
      "============================================================\n",
      "   Runs per scenario: 3\n",
      "   Streaming mode: ‚ùå Disabled\n",
      "   Cache key: benchmark_migration_v2\n",
      "   Total scenarios: 7\n",
      "   Instructions length: ~1,369 tokens\n"
     ]
    }
   ],
   "source": [
    "# Benchmark configuration\n",
    "NUM_RUNS = 3  # Number of runs per scenario (adjust for more accuracy)\n",
    "STREAM_MODE = False  # Set to True to enable streaming mode\n",
    "CACHE_KEY = \"benchmark_migration_v2\"  # Consistent cache key for all requests\n",
    "\n",
    "# Build full instructions (static padding + task instruction)\n",
    "INSTRUCTIONS = STATIC_PADDING + \"\"\"\n",
    "You are a helpful assistant. Answer concisely and directly. \n",
    "For questions requiring a specific format, follow the format exactly.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß Benchmark Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Runs per scenario: {NUM_RUNS}\")\n",
    "print(f\"   Streaming mode: {'‚úÖ Enabled' if STREAM_MODE else '‚ùå Disabled'}\")\n",
    "print(f\"   Cache key: {CACHE_KEY}\")\n",
    "print(f\"   Total scenarios: {len(TEST_SCENARIOS)}\")\n",
    "print(f\"   Instructions length: ~{len(INSTRUCTIONS)//4:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38df1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " PHASE 1: CACHE WARMUP\n",
      "================================================================================\n",
      "\n",
      "üß™ Models Selected\n",
      "------------------------------------------------------------\n",
      "  ‚Ä¢ gpt-4o: api_model=gpt-4o price_key=gpt-4o\n",
      "  ‚Ä¢ gpt-5.1: api_model=gpt-5.1 price_key=gpt-5.1 (reasoning_effort=none)\n",
      "------------------------------------------------------------\n",
      "\n",
      "  üî• Warming up gpt-4o...... done\n",
      "\n",
      "  üî• Warming up gpt-5.1...... done\n",
      "\n",
      "  ‚è≥ Waiting 2 seconds for cache to stabilize...\n",
      "  ‚úÖ Cache warmup complete!\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Cache Warmup\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" PHASE 1: CACHE WARMUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Select TWO models to compare\n",
    "# - label: Used in tables/keys (must be unique)\n",
    "# - api_model: Value passed into client.responses.create(model=...)\n",
    "# - price_key: Key used for PRICING / calculate_cost()\n",
    "# - reasoning_effort: Only applies to GPT-5.x reasoning models (optional)\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL_A = {\n",
    "    \"label\": \"gpt-4o\",\n",
    "    \"api_model\": \"gpt-4o\",\n",
    "    \"price_key\": \"gpt-4o\",\n",
    "    \"reasoning_effort\": None,\n",
    "}\n",
    "MODEL_B = {\n",
    "    \"label\": \"gpt-5.1\",\n",
    "    \"api_model\": \"gpt-5.1\",\n",
    "    \"price_key\": \"gpt-5.1\",\n",
    "    \"reasoning_effort\": \"none\",  # none/low/medium/high (only for GPT-5.x)\n",
    "}\n",
    "\n",
    "MODELS_TO_TEST = [MODEL_A, MODEL_B]\n",
    "MODEL_LABELS = [m[\"label\"] for m in MODELS_TO_TEST]\n",
    "\n",
    "# Basic validation\n",
    "if len(MODELS_TO_TEST) != 2:\n",
    "    raise ValueError(\"This notebook expects exactly TWO models in MODELS_TO_TEST\")\n",
    "if len(set(MODEL_LABELS)) != 2:\n",
    "    raise ValueError(\"MODEL_A/ MODEL_B labels must be unique\")\n",
    "for m in MODELS_TO_TEST:\n",
    "    if m[\"price_key\"] not in PRICING:\n",
    "        raise KeyError(f\"Missing PRICING entry for price_key='{m['price_key']}'\")\n",
    "\n",
    "print(\"\\nüß™ Models Selected\")\n",
    "print(\"-\" * 60)\n",
    "for m in MODELS_TO_TEST:\n",
    "    effort = m.get(\"reasoning_effort\")\n",
    "    effort_label = f\" (reasoning_effort={effort})\" if effort else \"\"\n",
    "    print(f\"  ‚Ä¢ {m['label']}: api_model={m['api_model']} price_key={m['price_key']}{effort_label}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "warmup_question = \"What is 2+2?\"\n",
    "warmup_runs = 3\n",
    "\n",
    "for m in MODELS_TO_TEST:\n",
    "    label = m[\"label\"]\n",
    "    api_model = m[\"api_model\"]\n",
    "    effort = m.get(\"reasoning_effort\")\n",
    "\n",
    "    print(f\"\\n  üî• Warming up {label}...\", end=\"\", flush=True)\n",
    "    for i in range(warmup_runs):\n",
    "        result = test_with_cache_key(\n",
    "            client=client,\n",
    "            model=api_model,\n",
    "            instructions=INSTRUCTIONS,\n",
    "            question=warmup_question,\n",
    "            cache_key=CACHE_KEY,\n",
    "            reasoning_effort=effort,\n",
    "        )\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\"x\", end=\"\", flush=True)\n",
    "\n",
    "        time.sleep(0.3)  # Brief pause between requests\n",
    "    print(\" done\")\n",
    "\n",
    "# Wait for cache to stabilize\n",
    "print(\"\\n  ‚è≥ Waiting 2 seconds for cache to stabilize...\")\n",
    "time.sleep(2)\n",
    "print(\"  ‚úÖ Cache warmup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5cec71",
   "metadata": {},
   "source": [
    "## Step 6: Run Benchmark\n",
    "\n",
    "Now we run the actual benchmark across all test scenarios. For each scenario:\n",
    "\n",
    "1. Test two selected models (configured in `MODEL_A` and `MODEL_B`)\n",
    "2. Run multiple times (configurable via `NUM_RUNS`)\n",
    "3. Collect latency, token usage, and accuracy metrics\n",
    "4. Calculate costs using prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa51c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " PHASE 2: BENCHMARK MEASUREMENT\n",
      "================================================================================\n",
      "\n",
      "  [1/7] Short - Intent Classification (EN)\n",
      "      gpt-4o: 0.655s | in:1085 out:2 cache:94.4% | acc:100% ‚úÖ\n",
      "      gpt-5.1 (effort=none): 1.461s | in:1084 out:12 cache:94.5% | acc:100% ‚úÖ\n",
      "\n",
      "  [2/7] Short - Sentiment Analysis (EN)\n",
      "      gpt-4o: 0.985s | in:1079 out:2 cache:63.3% | acc:100% ‚úÖ\n",
      "      gpt-5.1 (effort=none): 1.810s | in:1078 out:11 cache:95.0% | acc:100% ‚úÖ\n",
      "\n",
      "  [3/7] Medium - RAG Number Extraction (EN)\n",
      "      gpt-4o: 0.835s | in:1094 out:16 cache:93.6% | acc:100% ‚úÖ\n",
      "      gpt-5.1 (effort=none): 1.495s | in:1093 out:30 cache:93.7% | acc:100% ‚úÖ\n",
      "\n",
      "  [4/7] Medium - RAG Fact Extraction (EN)\n",
      "      gpt-4o: 0.607s | in:1086 out:12 cache:94.3% | acc:100% ‚úÖ\n",
      "      gpt-5.1 (effort=none): 1.610s | in:1085 out:20 cache:94.4% | acc:100% ‚úÖ\n",
      "\n",
      "  [5/7] Medium - Code Explanation (EN)\n",
      "      gpt-4o: 1.750s | in:1089 out:66 cache:94.0% | acc:100% ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Benchmark Measurement\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" PHASE 2: BENCHMARK MEASUREMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize results storage\n",
    "scenario_results = []\n",
    "results = {\n",
    "    m[\"label\"]: {\n",
    "        \"latency\": [],\n",
    "        \"first_token_time\": [],\n",
    "        \"input_tokens\": [],\n",
    "        \"output_tokens\": [],\n",
    "        \"cached_tokens\": [],\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "        \"cost\": 0,\n",
    "    }\n",
    "    for m in MODELS_TO_TEST\n",
    "}\n",
    "\n",
    "# Convenience for 2-model reporting\n",
    "MODEL_1_LABEL = MODEL_A[\"label\"]\n",
    "MODEL_2_LABEL = MODEL_B[\"label\"]\n",
    "\n",
    "# Run benchmark for each scenario\n",
    "for scenario_idx, scenario in enumerate(TEST_SCENARIOS):\n",
    "    category = scenario[\"category\"]\n",
    "    name = scenario[\"name\"]\n",
    "    question = scenario[\"question\"]\n",
    "    language = scenario[\"language\"]\n",
    "\n",
    "    print(f\"\\n  [{scenario_idx + 1}/{len(TEST_SCENARIOS)}] {category} - {name} ({language})\")\n",
    "\n",
    "    # Store scenario-level data\n",
    "    scenario_data = {\n",
    "        \"id\": scenario_idx + 1,\n",
    "        \"category\": category,\n",
    "        \"name\": name,\n",
    "        \"language\": language,\n",
    "    }\n",
    "    for m in MODELS_TO_TEST:\n",
    "        scenario_data[m[\"label\"]] = {}\n",
    "\n",
    "    # Test each model\n",
    "    for m in MODELS_TO_TEST:\n",
    "        display_name = m[\"label\"]\n",
    "        actual_model = m[\"api_model\"]\n",
    "        effort = m.get(\"reasoning_effort\")\n",
    "        price_key = m[\"price_key\"]\n",
    "\n",
    "        run_latencies = []\n",
    "        run_first_token = []\n",
    "        run_input = []\n",
    "        run_output = []\n",
    "        run_cached = []\n",
    "        correct = 0\n",
    "\n",
    "        # Multiple runs per scenario\n",
    "        for run in range(NUM_RUNS):\n",
    "            result = test_with_cache_key(\n",
    "                client=client,\n",
    "                model=actual_model,\n",
    "                instructions=INSTRUCTIONS,\n",
    "                question=question,\n",
    "                cache_key=CACHE_KEY,\n",
    "                reasoning_effort=effort,\n",
    "                stream=STREAM_MODE,\n",
    "            )\n",
    "\n",
    "            if result[\"success\"]:\n",
    "                # Collect metrics\n",
    "                run_latencies.append(result[\"latency\"])\n",
    "                run_first_token.append(result.get(\"first_token_time\", result[\"latency\"]))\n",
    "                run_input.append(result[\"input_tokens\"])\n",
    "                run_output.append(result[\"output_tokens\"])\n",
    "                run_cached.append(result[\"cached_tokens\"])\n",
    "\n",
    "                # Aggregate results\n",
    "                results[display_name][\"latency\"].append(result[\"latency\"])\n",
    "                results[display_name][\"first_token_time\"].append(result.get(\"first_token_time\", result[\"latency\"]))\n",
    "                results[display_name][\"input_tokens\"].append(result[\"input_tokens\"])\n",
    "                results[display_name][\"output_tokens\"].append(result[\"output_tokens\"])\n",
    "                results[display_name][\"cached_tokens\"].append(result[\"cached_tokens\"])\n",
    "\n",
    "                # Check accuracy\n",
    "                if check_answer(result[\"content\"], scenario[\"answer_variants\"]):\n",
    "                    correct += 1\n",
    "                    results[display_name][\"correct\"] += 1\n",
    "\n",
    "                results[display_name][\"total\"] += 1\n",
    "                time.sleep(0.2)  # Brief pause between requests\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è Error: {result.get('error', '')}\")\n",
    "\n",
    "        # Calculate scenario metrics for this model\n",
    "        avg_latency = sum(run_latencies) / len(run_latencies) if run_latencies else 0\n",
    "        avg_first_token = sum(run_first_token) / len(run_first_token) if run_first_token else 0\n",
    "        avg_input = sum(run_input) / len(run_input) if run_input else 0\n",
    "        avg_output = sum(run_output) / len(run_output) if run_output else 0\n",
    "        avg_cached = sum(run_cached) / len(run_cached) if run_cached else 0\n",
    "        cache_pct = (avg_cached / avg_input * 100) if avg_input > 0 else 0\n",
    "        accuracy = (correct / NUM_RUNS * 100) if NUM_RUNS > 0 else 0\n",
    "\n",
    "        # Calculate cost for this scenario\n",
    "        scenario_cost = calculate_cost(price_key, sum(run_input), sum(run_output), sum(run_cached))\n",
    "        results[display_name][\"cost\"] += scenario_cost\n",
    "\n",
    "        # Store scenario data\n",
    "        scenario_data[display_name] = {\n",
    "            \"avg_latency\": round(avg_latency, 3),\n",
    "            \"avg_first_token_time\": round(avg_first_token, 3),\n",
    "            \"avg_input_tokens\": round(avg_input, 0),\n",
    "            \"avg_output_tokens\": round(avg_output, 0),\n",
    "            \"avg_cached_tokens\": round(avg_cached, 0),\n",
    "            \"cache_hit_pct\": round(cache_pct, 1),\n",
    "            \"accuracy\": round(accuracy, 1),\n",
    "            \"cost\": round(scenario_cost * 1000, 4),  # Cost in milli-dollars\n",
    "        }\n",
    "\n",
    "        # Display results\n",
    "        status = \"‚úÖ\" if accuracy == 100 else \"‚ö†Ô∏è\" if accuracy >= 50 else \"‚ùå\"\n",
    "        effort_label = f\" (effort={effort})\" if effort else \"\"\n",
    "        stream_label = \" [stream]\" if STREAM_MODE else \"\"\n",
    "        ttft_info = f\" TTFT:{avg_first_token:.3f}s\" if STREAM_MODE else \"\"\n",
    "\n",
    "        print(\n",
    "            f\"      {display_name}{effort_label}{stream_label}: {avg_latency:.3f}s{ttft_info} | \"\n",
    "            f\"in:{avg_input:.0f} out:{avg_output:.0f} cache:{cache_pct:.1f}% | \"\n",
    "            f\"acc:{accuracy:.0f}% {status}\"\n",
    "        )\n",
    "\n",
    "    scenario_results.append(scenario_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96645e90",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results\n",
    "\n",
    "Now we aggregate the results and generate a comprehensive comparison report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318153d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Calculate Aggregated Metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" PHASE 3: ANALYZING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate aggregated metrics for each selected model\n",
    "for model in MODEL_LABELS:\n",
    "    r = results[model]\n",
    "\n",
    "    if r[\"latency\"]:\n",
    "        r[\"avg_latency\"] = sum(r[\"latency\"]) / len(r[\"latency\"])\n",
    "        r[\"avg_first_token\"] = (\n",
    "            sum(r[\"first_token_time\"]) / len(r[\"first_token_time\"])\n",
    "            if r[\"first_token_time\"]\n",
    "            else r[\"avg_latency\"]\n",
    "        )\n",
    "        r[\"avg_input\"] = sum(r[\"input_tokens\"]) / len(r[\"input_tokens\"])\n",
    "        r[\"avg_output\"] = sum(r[\"output_tokens\"]) / len(r[\"output_tokens\"])\n",
    "        r[\"avg_cached\"] = sum(r[\"cached_tokens\"]) / len(r[\"cached_tokens\"])\n",
    "        r[\"cache_pct\"] = (r[\"avg_cached\"] / r[\"avg_input\"] * 100) if r[\"avg_input\"] > 0 else 0\n",
    "        r[\"accuracy\"] = (r[\"correct\"] / r[\"total\"] * 100) if r[\"total\"] > 0 else 0\n",
    "        r[\"total_input\"] = sum(r[\"input_tokens\"])\n",
    "        r[\"total_output\"] = sum(r[\"output_tokens\"])\n",
    "        r[\"total_cached\"] = sum(r[\"cached_tokens\"])\n",
    "\n",
    "# Shorthand references for 2-model comparisons\n",
    "r1 = results[MODEL_1_LABEL]\n",
    "r2 = results[MODEL_2_LABEL]\n",
    "\n",
    "# Calculate differences (Model 2 vs Model 1)\n",
    "if r1.get(\"avg_latency\", 0) > 0:\n",
    "    latency_diff = (r2.get(\"avg_latency\", 0) - r1[\"avg_latency\"]) / r1[\"avg_latency\"] * 100\n",
    "else:\n",
    "    latency_diff = 0\n",
    "\n",
    "if r1.get(\"cost\", 0) > 0:\n",
    "    cost_savings = (r1[\"cost\"] - r2.get(\"cost\", 0)) / r1[\"cost\"] * 100\n",
    "else:\n",
    "    cost_savings = 0\n",
    "\n",
    "print(\"\\n‚úÖ Aggregation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Executive Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Performance Comparison\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<20} {MODEL_1_LABEL:>18} {MODEL_2_LABEL:>18} {'Difference':>18}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Latency\n",
    "print(\n",
    "    f\"{'Avg Latency':<20} \"\n",
    "    f\"{r1.get('avg_latency', 0):>17.3f}s \"\n",
    "    f\"{r2.get('avg_latency', 0):>17.3f}s \"\n",
    "    f\"{latency_diff:>+17.1f}%\"\n",
    " )\n",
    "\n",
    "# TTFT (if streaming)\n",
    "if STREAM_MODE:\n",
    "    print(\n",
    "        f\"{'Avg TTFT':<20} \"\n",
    "        f\"{r1.get('avg_first_token', 0):>17.3f}s \"\n",
    "        f\"{r2.get('avg_first_token', 0):>17.3f}s\"\n",
    "    )\n",
    "\n",
    "# Accuracy\n",
    "acc_diff = r2.get('accuracy', 0) - r1.get('accuracy', 0)\n",
    "print(\n",
    "    f\"{'Accuracy':<20} \"\n",
    "    f\"{r1.get('accuracy', 0):>17.1f}% \"\n",
    "    f\"{r2.get('accuracy', 0):>17.1f}% \"\n",
    "    f\"{acc_diff:>+17.1f}%\"\n",
    " )\n",
    "\n",
    "# Cache Hit Rate\n",
    "cache_diff = r2.get('cache_pct', 0) - r1.get('cache_pct', 0)\n",
    "print(\n",
    "    f\"{'Cache Hit Rate':<20} \"\n",
    "    f\"{r1.get('cache_pct', 0):>17.1f}% \"\n",
    "    f\"{r2.get('cache_pct', 0):>17.1f}% \"\n",
    "    f\"{cache_diff:>+17.1f}%\"\n",
    " )\n",
    "\n",
    "# Total Cost\n",
    "print(\n",
    "    f\"{'Total Cost':<20} \"\n",
    "    f\"${r1.get('cost', 0):>16.6f} \"\n",
    "    f\"${r2.get('cost', 0):>16.6f} \"\n",
    "    f\"{-cost_savings:>+17.1f}%\"\n",
    " )\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'üíµ Cost Savings (Model2 vs Model1)':<40} {cost_savings:>38.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Token Usage Summary\n",
    "print(\"\\nüì¶ Token Usage Summary\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<25} {MODEL_1_LABEL:>18} {MODEL_2_LABEL:>18}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Total Input Tokens':<25} {r1.get('total_input', 0):>18,.0f} {r2.get('total_input', 0):>18,.0f}\")\n",
    "print(f\"{'Total Output Tokens':<25} {r1.get('total_output', 0):>18,.0f} {r2.get('total_output', 0):>18,.0f}\")\n",
    "print(f\"{'Total Cached Tokens':<25} {r1.get('total_cached', 0):>18,.0f} {r2.get('total_cached', 0):>18,.0f}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Avg Input/Request':<25} {r1.get('avg_input', 0):>18,.0f} {r2.get('avg_input', 0):>18,.0f}\")\n",
    "print(f\"{'Avg Output/Request':<25} {r1.get('avg_output', 0):>18,.0f} {r2.get('avg_output', 0):>18,.0f}\")\n",
    "print(f\"{'Avg Cached/Request':<25} {r1.get('avg_cached', 0):>18,.0f} {r2.get('avg_cached', 0):>18,.0f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Detailed Results by Scenario\n",
    "print(\"\\nüìã Detailed Results by Scenario\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Group scenarios by category\n",
    "categories = {}\n",
    "for s in scenario_results:\n",
    "    cat = s[\"category\"]\n",
    "    if cat not in categories:\n",
    "        categories[cat] = []\n",
    "    categories[cat].append(s)\n",
    "\n",
    "for cat, scenarios in categories.items():\n",
    "    print(f\"\\n### {cat} Response Scenarios\")\n",
    "    print(\"-\" * 120)\n",
    "    print(\n",
    "        f\"{'#':<3} {'Scenario':<30} {'Lang':<5} \"\n",
    "        f\"{MODEL_1_LABEL + ' Lat':>12} {MODEL_2_LABEL + ' Lat':>12} \"\n",
    "        f\"{MODEL_1_LABEL + ' Cache':>14} {MODEL_2_LABEL + ' Cache':>14} \"\n",
    "        f\"{MODEL_1_LABEL + ' Acc':>12} {MODEL_2_LABEL + ' Acc':>12}\"\n",
    "    )\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    for s in scenarios:\n",
    "        m1 = s.get(MODEL_1_LABEL, {})\n",
    "        m2 = s.get(MODEL_2_LABEL, {})\n",
    "\n",
    "        acc1 = m1.get(\"accuracy\", 0)\n",
    "        acc2 = m2.get(\"accuracy\", 0)\n",
    "        acc1_icon = \"‚úÖ\" if acc1 == 100 else \"‚ö†Ô∏è\" if acc1 >= 50 else \"‚ùå\"\n",
    "        acc2_icon = \"‚úÖ\" if acc2 == 100 else \"‚ö†Ô∏è\" if acc2 >= 50 else \"‚ùå\"\n",
    "\n",
    "        print(\n",
    "            f\"{s['id']:<3} {s['name'][:28]:<30} {s['language']:<5} \"\n",
    "            f\"{m1.get('avg_latency', 0):>11.3f}s {m2.get('avg_latency', 0):>11.3f}s \"\n",
    "            f\"{m1.get('cache_hit_pct', 0):>13.1f}% {m2.get('cache_hit_pct', 0):>13.1f}% \"\n",
    "            f\"{acc1:>10.0f}%{acc1_icon} {acc2:>10.0f}%{acc2_icon}\"\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6950b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Recommendation\n",
    "print(\"\\nüéØ RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Performance analysis\n",
    "if r2.get(\"avg_latency\", float(\"inf\")) <= r1.get(\"avg_latency\", float(\"inf\")):\n",
    "    latency_winner = MODEL_2_LABEL\n",
    "    latency_verdict = \"faster or equal\"\n",
    "else:\n",
    "    latency_winner = MODEL_1_LABEL\n",
    "    latency_verdict = \"faster\"\n",
    "\n",
    "# Accuracy analysis\n",
    "accuracy_gap = abs(r1.get(\"accuracy\", 0) - r2.get(\"accuracy\", 0))\n",
    "if accuracy_gap < 1:\n",
    "    accuracy_verdict = \"Both models perform equally\"\n",
    "elif r1.get(\"accuracy\", 0) > r2.get(\"accuracy\", 0):\n",
    "    accuracy_verdict = f\"{MODEL_1_LABEL} has {accuracy_gap:.1f}% higher accuracy\"\n",
    "else:\n",
    "    accuracy_verdict = f\"{MODEL_2_LABEL} has {accuracy_gap:.1f}% higher accuracy\"\n",
    "\n",
    "print(f\"\\n  üìä Performance:\")\n",
    "print(f\"     ‚Ä¢ Latency: {latency_winner} is {latency_verdict} by {abs(latency_diff):.1f}%\")\n",
    "print(f\"     ‚Ä¢ Accuracy: {accuracy_verdict}\")\n",
    "\n",
    "print(f\"\\n  üí∞ Cost Efficiency:\")\n",
    "print(f\"     ‚Ä¢ {MODEL_2_LABEL} saves {cost_savings:.1f}% compared to {MODEL_1_LABEL}\")\n",
    "print(f\"     ‚Ä¢ Prompt caching effective: {r1.get('cache_pct', 0):.1f}% ({MODEL_1_LABEL}) / {r2.get('cache_pct', 0):.1f}% ({MODEL_2_LABEL})\")\n",
    "\n",
    "# Final recommendation (simple heuristic)\n",
    "if cost_savings > 50 and r2.get(\"accuracy\", 0) >= r1.get(\"accuracy\", 0) - 5:\n",
    "    print(f\"\\n  ‚úÖ **{MODEL_2_LABEL} recommended** for cost-sensitive workloads with acceptable latency trade-off\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ö†Ô∏è Evaluate based on your latency, quality, and cost requirements\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\n‚úÖ Benchmark completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb70f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "output_data = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"padding_tokens\": len(STATIC_PADDING) // 4,\n",
    "        \"cache_key\": CACHE_KEY,\n",
    "        \"runs_per_scenario\": NUM_RUNS,\n",
    "        \"total_scenarios\": len(TEST_SCENARIOS),\n",
    "        \"streaming\": STREAM_MODE,\n",
    "        \"models\": MODELS_TO_TEST,\n",
    "    },\n",
    "    \"summary\": {\n",
    "        MODEL_1_LABEL: {\n",
    "            \"avg_latency\": r1.get(\"avg_latency\", 0),\n",
    "            \"accuracy\": r1.get(\"accuracy\", 0),\n",
    "            \"cache_pct\": r1.get(\"cache_pct\", 0),\n",
    "            \"total_cost\": r1.get(\"cost\", 0),\n",
    "        },\n",
    "        MODEL_2_LABEL: {\n",
    "            \"avg_latency\": r2.get(\"avg_latency\", 0),\n",
    "            \"accuracy\": r2.get(\"accuracy\", 0),\n",
    "            \"cache_pct\": r2.get(\"cache_pct\", 0),\n",
    "            \"total_cost\": r2.get(\"cost\", 0),\n",
    "        },\n",
    "    },\n",
    "    \"scenarios\": scenario_results,\n",
    "}\n",
    "\n",
    "output_filename = f\"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d6611",
   "metadata": {},
   "source": [
    "## Pricing Reference\n",
    "\n",
    "Pricing is intentionally kept as a simple dictionary in this notebook so you can extend it easily.\n",
    "\n",
    "- If you change `MODEL_A` / `MODEL_B`, make sure their `price_key` values exist in `PRICING`.\n",
    "- If your org uses custom pricing/discounts, update `PRICING` to match your contract.\n",
    "\n",
    "| Model Key (price_key) | Input (per 1M) | Cached Input (per 1M) | Output (per 1M) |\n",
    "|-------|----------------|----------------------|-----------------|\n",
    "| gpt-4o | $2.50 | $1.25 | $10.00 |\n",
    "| gpt-5.1 | $1.25 | $0.13 | $10.00 |\n",
    "\n",
    "**Tip**: Cached input pricing can dominate total cost for repeated workloads; always benchmark with prompt caching enabled for an apples-to-apples migration comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017ed74",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Azure OpenAI Service Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/)\n",
    "- [Azure OpenAI Prompt Caching](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/prompt-caching)\n",
    "- [Azure OpenAI Responses API](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/responses)\n",
    "- [Model Migration Best Practices](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/model-versions)\n",
    "- [Azure AI Foundry Cost Optimization](https://learn.microsoft.com/en-us/azure/ai-foundry/control-plane/how-to-optimize-cost-performance?view=foundry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1965841",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "In this notebook, you learned how to benchmark and compare AI models for enterprise migration decisions.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Fair Comparison Methodology**\n",
    "   - Use identical API (Responses API) for both models\n",
    "   - Consistent cache keys and warmup procedures\n",
    "   - Multiple runs for statistical significance\n",
    "\n",
    "2. **Prompt Caching Benefits**\n",
    "   - Requires ‚â•1024 tokens in prompt prefix\n",
    "   - Dramatically reduces costs for repeated queries\n",
    "   - GPT-5.1 offers ~90% discount on cached tokens vs ~50% for GPT-4o\n",
    "\n",
    "3. **Migration Decision Factors**\n",
    "   - **Latency**: Measure actual response times for your workloads\n",
    "   - **Accuracy**: Validate with your specific use cases\n",
    "   - **Cost**: Calculate total cost including cache benefits\n",
    "   - **Trade-offs**: Balance performance vs cost for your requirements\n",
    "\n",
    "### Suggested Next Steps\n",
    "\n",
    "1. **Customize Test Scenarios**: Add scenarios specific to your use cases\n",
    "2. **Increase Runs**: Run more iterations for production-grade benchmarks\n",
    "3. **Enable Streaming**: Test TTFT (Time-to-First-Token) with `STREAM_MODE = True`\n",
    "4. **Compare More Models**: Extend `MODELS_TO_TEST` and `PRICING` for other models\n",
    "5. **Integrate with CI/CD**: Automate benchmarks as part of deployment pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
