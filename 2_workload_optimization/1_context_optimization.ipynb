{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd09c4",
   "metadata": {},
   "source": [
    "# Context Optimization with MCP and Code Execution\n",
    "\n",
    "----\n",
    "\n",
    "This notebook explores **context optimization strategies** for AI agents. We compare **anti-patterns** (inefficient approaches) with **best practices** to demonstrate how to build **more efficient and scalable AI agents**.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "- **Part 1: Warming Up** - MCP fundamentals and understanding your model's context window\n",
    "- **Part 2: Context Optimization** - Anti-patterns vs Best practices across four key areas:\n",
    "  1. Single Code Execution (full data vs sandbox)\n",
    "  2. Multi-Step Workflows (multiple trips vs single pipeline)\n",
    "  3. MCP Tool Management (schema bloat vs tool filtering)\n",
    "  4. Context Compression (growing history vs compressed context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a9664",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [What is MCP?](#what-is-mcp)\n",
    "- [The Problem: Context Window Overload](#the-problem-context-window-overload)\n",
    "- [Setup](#setup)\n",
    "- [Part 1: Warming Up](#part-1-warming-up)\n",
    "  - 1.1 MCPStdioTool - Local MCP Server\n",
    "  - 1.2 MCPStreamableHTTPTool - Remote HTTP Server\n",
    "  - 1.3 Understanding Context Window Limits\n",
    "- [Part 2: Context Optimization](#part-2-context-optimization)\n",
    "  - **Section 1: Single Code Execution**\n",
    "    - 1.1 [Anti-Pattern] Full Data in Context\n",
    "    - 1.2 [Best Practice] Data Generation in Sandbox\n",
    "  - **Section 2: Multi-Step Code Execution**\n",
    "    - 2.1 [Anti-Pattern] Complex Workflows (4 Round Trips)\n",
    "    - 2.2 [Best Practice] Multi-Step Pipeline in Single Execution\n",
    "  - **Section 3: MCP Optimization**\n",
    "    - 3.1 [Anti-Pattern] Remote MCP Schema Bloat\n",
    "    - 3.2 [Best Practice] Tool Filtering + History Limit\n",
    "  - **Section 4: Context Compression**\n",
    "    - 4.1 [Anti-Pattern] Inappropriate Large Context\n",
    "    - 4.2 [Best Practice] Context Compression\n",
    "- [Best Practices Summary](#best-practices-summary)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61df06e",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard that enables AI agents to seamlessly connect to external systems, tools, and data sources. Think of it as a **universal adapter** for AI agents.\n",
    "\n",
    "```\n",
    "MCP = Standardized Protocol for AI ‚Üî External Systems\n",
    "```\n",
    "\n",
    "### What MCP Connects To\n",
    "\n",
    "| Category | Examples |\n",
    "|----------|----------|\n",
    "| Cloud Services | Google Drive, Salesforce, AWS, Azure |\n",
    "| Databases | PostgreSQL, MongoDB, Redis, SQLite |\n",
    "| APIs | REST APIs, GraphQL endpoints |\n",
    "| File Systems | Local and remote file storage |\n",
    "| Development Tools | GitHub, Jira, Slack |\n",
    "\n",
    "### MCP Tool Types in Microsoft Agent Framework\n",
    "\n",
    "| Tool Type | Connection Method | Use Case |\n",
    "|-----------|------------------|----------|\n",
    "| `MCPStdioTool` | Standard I/O (local process) | Local MCP servers (e.g., calculator, filesystem) |\n",
    "| `MCPStreamableHTTPTool` | HTTP with SSE | Remote HTTP-based MCP servers |\n",
    "| `MCPWebsocketTool` | WebSocket | Real-time bidirectional communication |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b93d3",
   "metadata": {},
   "source": [
    "## The Problem: Context Window Overload\n",
    "\n",
    "### Challenge 1: Tool Definition Bloat\n",
    "\n",
    "When an AI agent connects to multiple tools via MCP, **all tool definitions must be loaded** into the model's context window.\n",
    "\n",
    "```\n",
    "Agent Context Window (128K tokens):\n",
    "‚îú‚îÄ‚îÄ Tool Definitions: 25,000 tokens (50 tools √ó 500 tokens avg)\n",
    "‚îú‚îÄ‚îÄ Conversation History: 15,000 tokens\n",
    "‚îú‚îÄ‚îÄ System Instructions: 5,000 tokens\n",
    "‚îî‚îÄ‚îÄ Available for Work: 83,000 tokens (only 65% of capacity)\n",
    "```\n",
    "\n",
    "### Challenge 2: Excessive Token Consumption\n",
    "\n",
    "Every intermediate result from tool calls must pass through the model's context window.\n",
    "\n",
    "**Example: Processing sales data**\n",
    "\n",
    "| Approach | Steps | Tokens |\n",
    "|----------|-------|--------|\n",
    "| Traditional | Download spreadsheet ‚Üí 50K, Process ‚Üí 30K, Update ‚Üí 5K | **85,000 tokens** |\n",
    "| Code Execution | Execute filtered query ‚Üí 5K (only filtered results returned) | **5,000 tokens** |\n",
    "\n",
    "**Result: 94% token reduction with code execution approach**\n",
    "\n",
    "### The Solution: Code Execution with MCP\n",
    "\n",
    "Code execution allows agents to:\n",
    "1. **Execute code** to process data locally (in sandbox)\n",
    "2. **Filter and transform** results before returning\n",
    "3. **Maintain state** across operations\n",
    "4. **Build reusable functions** and workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d13a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'üîç Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"‚úÖ Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('‚úÖ PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9821998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'.\")\n",
    "    print('üí° Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "# project variables from config\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "PROJECT_NAME = config.get('PROJECT_NAME', 'proj-default')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "AZURE_AI_MODEL_DEPLOYMENT_NAME = config.get('AZURE_AI_MODEL_DEPLOYMENT_NAME')\n",
    "\n",
    "# Azure Open AI variables from env\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"‚úÖ Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nüìå Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"üìå Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"üìå Location: {LOCATION}\")\n",
    "print(f\"üìå Azure AI project endpoint: {AZURE_AI_PROJECT_ENDPOINT}\")\n",
    "print(f\"üìå Azure Open AI endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"üìå Model deployment: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")\n",
    "print(f\"üìå Azure Open AI API version: {AZURE_OPENAI_API_VERSION}\")\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca231914",
   "metadata": {},
   "source": [
    "## Part 1: Warming Up\n",
    "\n",
    "Before diving into context optimization, let's get familiar with **MCP (Model Context Protocol)** basics and understand how to connect to MCP servers using the Microsoft Agent Framework.\n",
    "\n",
    "### MCP Tool Types in Microsoft Agent Framework\n",
    "\n",
    "The Microsoft Agent Framework supports three connection types for MCP:\n",
    "\n",
    "| Tool Class | Transport | Best For |\n",
    "|------------|-----------|----------|\n",
    "| `MCPStdioTool` | Standard I/O | Local process-based servers |\n",
    "| `MCPStreamableHTTPTool` | HTTP + SSE | Remote HTTP endpoints |\n",
    "| `MCPWebsocketTool` | WebSocket | Bidirectional real-time comm |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482d3e4",
   "metadata": {},
   "source": [
    "### 1.1 MCPStdioTool - Local MCP Server (Calculator)\n",
    "\n",
    "`MCPStdioTool` connects to MCP servers that run as **local processes** using standard input/output. This is the simplest way to use MCP tools.\n",
    "\n",
    "**Popular Local MCP Servers:**\n",
    "- `uvx mcp-server-calculator` - Mathematical computations\n",
    "- `uvx mcp-server-filesystem` - File system operations\n",
    "- `uvx mcp-server-sqlite` - Database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c2e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç MCP Calculator found at: /afh/code/agent-operator-lab/.venv/bin/mcp-server-calculator\n",
      "============================================================\n",
      "üßÆ Traditional MCP Tool Call: Calculator Example\n",
      "============================================================\n",
      "\n",
      "üìù User Query: What is 15 * 23 + 45?\n",
      "\n",
      "‚úÖ Agent Response: 390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agent_framework._types.AgentRunResponse at 0x78391ffafe00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1.1: Basic MCP Tool Call with MCPStdioTool\n",
    "# This demonstrates the traditional approach where tool definitions are loaded and called\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from agent_framework import MCPStdioTool\n",
    "from agent_framework.azure import AzureAIClient\n",
    "\n",
    "# Portable: Auto-detect MCP server path from current venv (works on any machine after `uv sync`)\n",
    "def get_mcp_server_path(server_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the path to an MCP server executable in the current virtual environment.\n",
    "    \n",
    "    This is the recommended portable approach:\n",
    "    1. First, try to find it in the same directory as the Python interpreter (venv/bin)\n",
    "    2. Fallback to shutil.which() to search PATH\n",
    "    \n",
    "    Args:\n",
    "        server_name: Name of the MCP server executable (e.g., 'mcp-server-calculator')\n",
    "    \n",
    "    Returns:\n",
    "        Absolute path to the MCP server executable\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the server is not found\n",
    "    \"\"\"\n",
    "    # Method 1: Look in the venv bin directory (most reliable)\n",
    "    venv_bin_dir = os.path.dirname(sys.executable)\n",
    "    venv_path = os.path.join(venv_bin_dir, server_name)\n",
    "    \n",
    "    if os.path.isfile(venv_path):\n",
    "        return venv_path\n",
    "    \n",
    "    # Method 2: Fallback to PATH search\n",
    "    path_result = shutil.which(server_name)\n",
    "    if path_result:\n",
    "        return path_result\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå '{server_name}' not found.\\n\"\n",
    "        f\"   Checked: {venv_path}\\n\"\n",
    "        f\"   Run: uv sync  (to install dependencies from pyproject.toml)\"\n",
    "    )\n",
    "\n",
    "# Get the calculator MCP server path\n",
    "MCP_CALCULATOR_PATH = get_mcp_server_path(\"mcp-server-calculator\")\n",
    "print(f\"üìç MCP Calculator found at: {MCP_CALCULATOR_PATH}\")\n",
    "\n",
    "async def basic_mcp_calculator_example():\n",
    "    \"\"\"\n",
    "    Traditional MCP approach: Load calculator tool and perform computation.\n",
    "    \n",
    "    In this approach:\n",
    "    - All tool definitions are loaded into context\n",
    "    - Each tool call returns full results to the model\n",
    "    - Token usage scales with data size\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üßÆ Traditional MCP Tool Call: Calculator Example\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    async with (\n",
    "        MCPStdioTool(\n",
    "            name=\"calculator\",\n",
    "            command=MCP_CALCULATOR_PATH,\n",
    "            args=[]\n",
    "        ) as mcp_calculator,\n",
    "        AzureAIClient(credential=credential, project_endpoint=AZURE_AI_PROJECT_ENDPOINT).create_agent(\n",
    "            name=\"MathAgent\",\n",
    "            instructions=\"You are a helpful math assistant that can solve calculations.\",\n",
    "            tools=mcp_calculator,\n",
    "        ) as agent,\n",
    "    ):\n",
    "        # Simple calculation using MCP tool\n",
    "        user_query = \"What is 15 * 23 + 45?\"\n",
    "        print(f\"\\nüìù User Query: {user_query}\")\n",
    "        \n",
    "        result = await agent.run(user_query)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Agent Response: {result}\")\n",
    "        return result\n",
    "\n",
    "# Run the example\n",
    "await basic_mcp_calculator_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f2777",
   "metadata": {},
   "source": [
    "### 1.2 MCPStreamableHTTPTool - Remote HTTP MCP Server\n",
    "\n",
    "`MCPStreamableHTTPTool` connects to MCP servers over **HTTP with Server-Sent Events (SSE)**. This is useful for connecting to remote services like Microsoft Learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e02f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìö HTTP MCP Tool: Microsoft Learn Documentation\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù User Query: How to create an Azure storage account using az cli?\n",
      "\n",
      "‚úÖ Agent Response: Use these Azure CLI commands:\n",
      "\n",
      "1) Sign in (if needed):\n",
      "```bash\n",
      "az login\n",
      "```\n",
      "\n",
      "2) Create a resource group:\n",
      "```bash\n",
      "az group create \\\n",
      "  --name storage-resource-group \\\n",
      "  --location eastus\n",
      "```\n",
      "\n",
      "3) Create the storage account (name must be globally unique, 3‚Äì24 lowercase letters/numbers):\n",
      "```bash\n",
      "az storage account create \\\n",
      "  --name <account-name> \\\n",
      "  --resource-group storage-resource-group \\\n",
      "  --location eastus \\\n",
      "  --sku Standard_RAGRS \\\n",
      "  --kind StorageV2 \\\n",
      "  --min-tls-version TLS1_2 \\\n",
      "  --allow-blob-public-access false\n",
      "```\n",
      "\n",
      "Reference: https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create#create-a-storage-account\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agent_framework._types.AgentRunResponse at 0x78391f05f390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1.2: HTTP-based MCP Server with MCPStreamableHTTPTool\n",
    "from agent_framework import MCPStreamableHTTPTool\n",
    "from agent_framework.azure import AzureAIClient\n",
    "\n",
    "async def http_mcp_docs_example():\n",
    "    \"\"\"\n",
    "    Connect to an HTTP-based MCP server (e.g., Microsoft Learn API).\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Remote MCP server connection via HTTP\n",
    "    - SSE (Server-Sent Events) for streaming responses\n",
    "    - Authentication header configuration\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìö HTTP MCP Tool: Microsoft Learn Documentation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    async with (\n",
    "        MCPStreamableHTTPTool(\n",
    "            name=\"Microsoft Learn MCP\",\n",
    "            url=\"https://learn.microsoft.com/api/mcp\",\n",
    "            # headers={\"Authorization\": \"Bearer <your-token>\"},  # Uncomment if auth required\n",
    "        ) as mcp_docs,\n",
    "        # Use AzureAIClient pattern (credential from setup cell)\n",
    "        AzureAIClient(credential=credential, project_endpoint=AZURE_AI_PROJECT_ENDPOINT).create_agent(\n",
    "            name=\"DocsAgent\",\n",
    "            instructions=\"You help with Microsoft documentation questions.\",\n",
    "            tools=mcp_docs,\n",
    "        ) as agent,\n",
    "    ):\n",
    "        user_query = \"How to create an Azure storage account using az cli?\"\n",
    "        print(f\"\\nüìù User Query: {user_query}\")\n",
    "        \n",
    "        result = await agent.run(user_query)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Agent Response: {result}\")\n",
    "        return result\n",
    "\n",
    "# Run the example\n",
    "await http_mcp_docs_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bbaec",
   "metadata": {},
   "source": [
    "### 1.3 Understanding Context Window Limits\n",
    "\n",
    "Before optimizing context usage, it's essential to understand the **context window limits** of your model. Different models have different capacities, and knowing your limits helps you plan accordingly.\n",
    "\n",
    "| Model | Context Window | Approximate Characters |\n",
    "|-------|---------------|------------------------|\n",
    "| GPT-4o | 128K tokens | ~512,000 chars |\n",
    "| GPT-4o-mini | 128K tokens | ~512,000 chars |\n",
    "| GPT-4.1 | 1M tokens | ~4,000,000 chars |\n",
    "| Claude 3.5 | 200K tokens | ~800,000 chars |\n",
    "\n",
    "**Why Context Window Matters:**\n",
    "- Tool schemas consume tokens on **every API call**\n",
    "- Conversation history grows with each turn\n",
    "- Large data payloads can quickly exhaust the window\n",
    "- Leaving room for model output is essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4863e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìè 1.3: Understanding Context Window Limits\n",
      "======================================================================\n",
      "\n",
      "üìå Deployment Name: gpt-4.1\n",
      "üìê Context Window: 1,000,000 tokens\n",
      "üì° Source: API model match (gpt-4.1-2025-04-14)\n",
      "\n",
      "üìä Token Estimates for Common Content Types:\n",
      "--------------------------------------------------\n",
      "   Simple prompt:\n",
      "      Characters: 30 | Tokens: 7 | Ratio: 4.3 chars/token\n",
      "   System instruction:\n",
      "      Characters: 74 | Tokens: 13 | Ratio: 5.7 chars/token\n",
      "   1KB of text:\n",
      "      Characters: 1,200 | Tokens: 201 | Ratio: 6.0 chars/token\n",
      "   Sample JSON (100 records):\n",
      "      Characters: 4,469 | Tokens: 2,001 | Ratio: 2.2 chars/token\n",
      "\n",
      "üìã Context Window Budget (1,000,000 tokens):\n",
      "--------------------------------------------------\n",
      "   Total capacity: 1,000,000 tokens\n",
      "   - System prompt: 500 tokens (0.1%)\n",
      "   - Tool schemas (10 tools): 3,000 tokens (0.3%)\n",
      "   - Conversation history (5 turns): 2,500 tokens (0.2%)\n",
      "   - User query: 200 tokens (0.0%)\n",
      "   - Reserved for output: 4,000 tokens (0.4%)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   Available for data/work: 989,800 tokens (99.0%)\n",
      "\n",
      "üí° Tip: The examples in Part 2 will show you how to maximize this available space!\n"
     ]
    }
   ],
   "source": [
    "# 1.3: Check your model's context window and current usage\n",
    "# This helps you understand how much capacity you have for optimization\n",
    "\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def estimate_tokens(text: str, model: str = \"gpt-4.1\") -> int:\n",
    "    \"\"\"Estimate token count for a given text using tiktoken.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Known context window sizes for common models (as of 2025)\n",
    "MODEL_CONTEXT_WINDOWS = {\n",
    "    # GPT-4 series\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-4-32k\": 32768,\n",
    "    \"gpt-4-turbo\": 128000,\n",
    "    \"gpt-4-turbo-preview\": 128000,\n",
    "    \"gpt-4o\": 128000,\n",
    "    \"gpt-4o-mini\": 128000,\n",
    "    \"gpt-4.1\": 1000000,  # 1M tokens (STU)\n",
    "    \"gpt-4.1-mini\": 1000000,\n",
    "    \"gpt-4.1-nano\": 1000000,\n",
    "    # GPT-5 series\n",
    "    \"gpt-5\": 1000000,\n",
    "    \"gpt-5.1\": 1000000,\n",
    "    # GPT-3.5 series\n",
    "    \"gpt-35-turbo\": 4096,\n",
    "    \"gpt-35-turbo-16k\": 16384,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-16k\": 16384,\n",
    "    # o1 series\n",
    "    \"o1\": 200000,\n",
    "    \"o1-mini\": 128000,\n",
    "    \"o1-preview\": 128000,\n",
    "    \"o3\": 200000,\n",
    "    \"o3-mini\": 200000,\n",
    "}\n",
    "\n",
    "def get_model_context_window(deployment_name: str, client: AzureOpenAI = None) -> tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Get the context window size for a model deployment.\n",
    "    \n",
    "    Tries multiple methods:\n",
    "    1. Query Azure OpenAI API for model info\n",
    "    2. Fall back to known model context windows lookup\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (context_window_size, source_method)\n",
    "    \"\"\"\n",
    "    # Method 1: Try to get model info from Azure OpenAI API\n",
    "    if client:\n",
    "        try:\n",
    "            # List models and find the one matching our deployment\n",
    "            models = client.models.list()\n",
    "            for model in models.data:\n",
    "                # Check if this model matches our deployment\n",
    "                if deployment_name.lower() in model.id.lower() or model.id.lower() in deployment_name.lower():\n",
    "                    # Some APIs return context_length or max_tokens\n",
    "                    if hasattr(model, 'context_length') and model.context_length:\n",
    "                        return model.context_length, \"API (context_length)\"\n",
    "                    if hasattr(model, 'max_tokens') and model.max_tokens:\n",
    "                        return model.max_tokens, \"API (max_tokens)\"\n",
    "                    # Try to extract from model ID - sort by length to match specific names first\n",
    "                    model_id = model.id.lower()\n",
    "                    sorted_models = sorted(MODEL_CONTEXT_WINDOWS.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "                    for known_model, ctx_size in sorted_models:\n",
    "                        if known_model in model_id:\n",
    "                            return ctx_size, f\"API model match ({model.id})\"\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è API query failed: {e}\")\n",
    "    \n",
    "    # Method 2: Look up from known models table\n",
    "    deployment_lower = deployment_name.lower()\n",
    "    \n",
    "    # Try exact match first\n",
    "    if deployment_lower in MODEL_CONTEXT_WINDOWS:\n",
    "        return MODEL_CONTEXT_WINDOWS[deployment_lower], \"Known models table (exact)\"\n",
    "    \n",
    "    # Try partial match - sort by model name length descending to match more specific names first\n",
    "    # e.g., \"gpt-4.1\" should match before \"gpt-4\"\n",
    "    sorted_models = sorted(MODEL_CONTEXT_WINDOWS.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for model_name, ctx_size in sorted_models:\n",
    "        if model_name in deployment_lower or deployment_lower in model_name:\n",
    "            return ctx_size, f\"Known models table (partial: {model_name})\"\n",
    "    \n",
    "    # Default fallback\n",
    "    return 128000, \"Default fallback (128K)\"\n",
    "\n",
    "def check_context_window():\n",
    "    \"\"\"\n",
    "    Check the model's context window and demonstrate token counting.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìè 1.3: Understanding Context Window Limits\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create Azure OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Get model info\n",
    "    print(f\"\\nüìå Deployment Name: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")\n",
    "    \n",
    "    # Query context window size\n",
    "    context_window, source = get_model_context_window(AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, client)\n",
    "    print(f\"üìê Context Window: {context_window:,} tokens\")\n",
    "    print(f\"üì° Source: {source}\")\n",
    "    \n",
    "    # Sample content to demonstrate token counting\n",
    "    sample_texts = {\n",
    "        \"Simple prompt\": \"What is the capital of France?\",\n",
    "        \"System instruction\": \"You are a helpful AI assistant that provides accurate and concise answers.\",\n",
    "        \"1KB of text\": \"Lorem ipsum \" * 100,\n",
    "        \"Sample JSON (100 records)\": json.dumps([{\"id\": i, \"name\": f\"item_{i}\", \"value\": i * 10} for i in range(100)]),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Token Estimates for Common Content Types:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, text in sample_texts.items():\n",
    "        tokens = estimate_tokens(text)\n",
    "        chars = len(text)\n",
    "        ratio = chars / tokens if tokens > 0 else 0\n",
    "        print(f\"   {name}:\")\n",
    "        print(f\"      Characters: {chars:,} | Tokens: {tokens:,} | Ratio: {ratio:.1f} chars/token\")\n",
    "    \n",
    "    # Context window budget example (using queried value)\n",
    "    print(f\"\\nüìã Context Window Budget ({context_window:,} tokens):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    budget = {\n",
    "        \"Total capacity\": context_window,\n",
    "        \"System prompt\": 500,\n",
    "        \"Tool schemas (10 tools)\": 3000,\n",
    "        \"Conversation history (5 turns)\": 2500,\n",
    "        \"User query\": 200,\n",
    "        \"Reserved for output\": 4000,\n",
    "    }\n",
    "    \n",
    "    used = sum(v for k, v in budget.items() if k not in [\"Total capacity\", \"Reserved for output\"])\n",
    "    available = budget[\"Total capacity\"] - used - budget[\"Reserved for output\"]\n",
    "    \n",
    "    for name, tokens in budget.items():\n",
    "        if name == \"Total capacity\":\n",
    "            print(f\"   {name}: {tokens:,} tokens\")\n",
    "        else:\n",
    "            pct = (tokens / budget[\"Total capacity\"]) * 100\n",
    "            print(f\"   - {name}: {tokens:,} tokens ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"   Available for data/work: {available:,} tokens ({(available/budget['Total capacity'])*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüí° Tip: The examples in Part 2 will show you how to maximize this available space!\")\n",
    "    \n",
    "    return budget, context_window\n",
    "\n",
    "# Run the context window check\n",
    "context_budget, model_context_window = check_context_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1e1df",
   "metadata": {},
   "source": [
    "## Part 2: Context Optimization\n",
    "\n",
    "Now that you understand MCP basics and context window limits, let's explore **optimization strategies** through **Anti-Pattern vs Best Practice** comparisons.\n",
    "\n",
    "**Problem:** Every byte of data consumes tokens, even if we only need a summary!\n",
    "\n",
    "---\n",
    "\n",
    "In this example, we send the **full dataset** to Azure OpenAI for summarization. This demonstrates the token consumption problem when all data must pass through the model's context window.\n",
    "\n",
    "### Section 1: Single Code Execution\n",
    "\n",
    "This section compares sending **full data through the context window** (anti-pattern) vs **processing data in a sandbox** (best practice).\n",
    "\n",
    "### 1.1 [Anti-Pattern] Full Data in Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä 1.1 [Anti-Pattern] Full Data in Context\n",
      "======================================================================\n",
      "\n",
      "üìà Generated 1000 sales records\n",
      "üìù Full data JSON size: 142,651 characters\n",
      "\n",
      "üîÑ Sending full data to Azure OpenAI for summarization...\n",
      "   Prompt tokens (estimated): 56,427\n",
      "\n",
      "‚úÖ Response received!\n",
      "\n",
      "üìã Summary:\n",
      "--------------------------------------------------\n",
      "**Sales Data Summary:**\n",
      "\n",
      "**1. Total Number of Records**\n",
      "- **1,000 records**\n",
      "\n",
      "---\n",
      "\n",
      "**2. Top 3 Products by Sales Amount**\n",
      "\n",
      "Aggregate sales amounts by product:\n",
      "- Widget A: **$13,169,556.14**\n",
      "- Gadget Y: **$12,474,453.15**\n",
      "- Widget B: **$12,308,032.79**\n",
      "\n",
      "**Top 3:**\n",
      "1. **Widget A**\n",
      "2. **Gadget Y**\n",
      "3. **Widget B**\n",
      "\n",
      "---\n",
      "\n",
      "**3. Best Performing Quarter**\n",
      "\n",
      "Total sales by quarter:\n",
      "- Q1: **$13,253,704.27**\n",
      "- Q2: **$12,183,256.89**\n",
      "- Q3: **$13,951,029.40**\n",
      "- Q4: **$12,083,484.69**\n",
      "\n",
      "**Highest:**  \n",
      "**Q3 ($13,951,029.40)**\n",
      "\n",
      "---\n",
      "\n",
      "**4. Regional Distribution**\n",
      "\n",
      "Total sales by region:\n",
      "- **North:** $8,091,539.10\n",
      "- **South:** $9,718,494.44\n",
      "- **East:** $8,572,454.34\n",
      "- **West:** $8,089,987.38\n",
      "\n",
      "**Largest Sales:**  \n",
      "**South Region ($9,718,494.44)**\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**\n",
      "- 1,000 total sales records.\n",
      "- Widget A, Gadget Y, and Widget B are the top products by total sales.\n",
      "- Q3 is the best performing quarter.\n",
      "- South region has the highest sales distribution.\n",
      "--------------------------------------------------\n",
      "\n",
      "üéüÔ∏è  Token Usage (Full Context Approach):\n",
      "   Prompt tokens:     56,447\n",
      "   Completion tokens: 325\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL tokens:      56,772\n"
     ]
    }
   ],
   "source": [
    "# Example 1.1: [Anti-Pattern] Full Data Summarization with Azure OpenAI\n",
    "# This demonstrates token consumption when full data is sent to the model\n",
    "\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def estimate_tokens(text: str, model: str = \"gpt-4.1\") -> int:\n",
    "    \"\"\"Estimate token count for a given text using tiktoken.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Generate sample sales data\n",
    "def generate_sample_sales_data(num_rows: int = 1000) -> list[dict]:\n",
    "    \"\"\"Generate sample sales data for demonstration.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    products = [\"Widget A\", \"Widget B\", \"Gadget X\", \"Gadget Y\", \"Tool Z\"]\n",
    "    quarters = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "    regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_rows):\n",
    "        data.append({\n",
    "            \"id\": i + 1,\n",
    "            \"product\": random.choice(products),\n",
    "            \"amount\": round(random.uniform(100, 50000), 2),\n",
    "            \"quarter\": random.choice(quarters),\n",
    "            \"region\": random.choice(regions),\n",
    "            \"date\": f\"2025-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Generate sample data\n",
    "sample_data = generate_sample_sales_data(1000)\n",
    "full_data_json = json.dumps(sample_data, indent=2)\n",
    "\n",
    "print(\"=\"  * 70)\n",
    "print(\"üìä 1.1 [Anti-Pattern] Full Data in Context\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìà Generated {len(sample_data)} sales records\")\n",
    "print(f\"üìù Full data JSON size: {len(full_data_json):,} characters\")\n",
    "\n",
    "# Create Azure OpenAI client using Foundry endpoint\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "# Traditional approach: Send FULL data to the model for summarization\n",
    "summarization_prompt = f\"\"\"Analyze the following sales data and provide a brief summary including:\n",
    "1. Total number of records\n",
    "2. Top 3 products by sales amount\n",
    "3. Best performing quarter\n",
    "4. Regional distribution\n",
    "\n",
    "Sales Data:\n",
    "{full_data_json}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüîÑ Sending full data to Azure OpenAI for summarization...\")\n",
    "print(f\"   Prompt tokens (estimated): {estimate_tokens(summarization_prompt):,}\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data analyst. Provide concise summaries.\"},\n",
    "        {\"role\": \"user\", \"content\": summarization_prompt}\n",
    "    ],\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# Extract token usage from response\n",
    "usage = response.usage\n",
    "total_tokens_full_context = usage.total_tokens\n",
    "\n",
    "print(f\"\\n‚úÖ Response received!\")\n",
    "print(f\"\\nüìã Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\nüéüÔ∏è  Token Usage (Full Context Approach):\")\n",
    "print(f\"   Prompt tokens:     {usage.prompt_tokens:,}\")\n",
    "print(f\"   Completion tokens: {usage.completion_tokens:,}\")\n",
    "print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(f\"   TOTAL tokens:      {usage.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc352b3",
   "metadata": {},
   "source": [
    "### 1.2 [Best Practice] Data Generation in Sandbox\n",
    "\n",
    "Instead of sending full data through the context, we use **Code Execution** to process data in a sandboxed environment. Only the **summary** is returned to the model.\n",
    "\n",
    "**Key Benefits:**\n",
    "| Benefit | Description |\n",
    "|---------|-----------|\n",
    "| **Token Efficiency** | Raw data never enters context |\n",
    "| **Scalability** | Works with any data size |\n",
    "| **Privacy** | Sensitive data stays in sandbox |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716cb318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ 1.2 [Best Practice] Data Generation in Sandbox\n",
      "======================================================================\n",
      "\n",
      "üìù Sending code execution request...\n",
      "   Prompt tokens (estimated): 122\n",
      "   Note: Raw data stays in sandbox - never enters model context!\n",
      "\n",
      "‚úÖ Code executed in sandbox!\n",
      "\n",
      "üìã Summary (from sandbox execution):\n",
      "--------------------------------------------------\n",
      "import pandas as pd, numpy as np\n",
      "np.random.seed(42)\n",
      "n=1000\n",
      "products=[\"Widget A\",\"Widget B\",\"Gadget X\",\"Gadget Y\",\"Tool Z\"]\n",
      "quarters=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"]\n",
      "regions=[\"North\",\"South\",\"East\",\"West\"]\n",
      "ids=np.arange(1,n+1)\n",
      "prod=np.random.choice(products,n)\n",
      "amount=np.random.randint(100,50001,n)\n",
      "q=np.random.choice(quarters,n)\n",
      "reg=np.random.choice(regions,n)\n",
      "# generate dates within 2025, aligned with quarter\n",
      "# map quarter to month ranges\n",
      "q_months={\"Q1\":(1,3),\"Q2\":(4,6),\"Q3\":(7,9),\"Q4\":(10,12)}\n",
      "dates=[]\n",
      "for qq in q:\n",
      "    m1,m2=q_months[qq]\n",
      "    month=np.random.randint(m1,m2+1)\n",
      "    day=np.random.randint(1,29)\n",
      "    dates.append(pd.Timestamp(year=2025,month=month,day=day))\n",
      "df=pd.DataFrame({\"id\":ids,\"product\":prod,\"amount\":amount,\"quarter\":q,\"region\":reg,\"date\":dates})\n",
      "total_records=len(df)\n",
      "total_sales=df[\"amount\"].sum()\n",
      "top3=df.groupby(\"product\")[\"amount\"].sum().sort_values(ascending=False).head(3)\n",
      "best_quarter=df.groupby(\"quarter\")[\"amount\"].sum().sort_values(ascending=False)\n",
      "regional=df.groupby(\"region\")[\"amount\"].sum().sort_values(ascending=False)\n",
      "total_records,total_sales,top3,best_quarter,regional\n",
      " ## Sales Data Summary Report (Simulated, n=1000)\n",
      "\n",
      "### 1) Overall\n",
      "- **Total records:** 1,000  \n",
      "- **Total sales amount:** **25,496,302**\n",
      "\n",
      "### 2) Top 3 Products by Total Sales\n",
      "1. **Gadget Y:** 5,379,810  \n",
      "2. **Tool Z:** 5,159,463  \n",
      "3. **Gadget X:** 5,098,621  \n",
      "\n",
      "### 3) Best Quarter by Revenue\n",
      "- **Q2:** **6,696,318**  \n",
      "(Quarter totals: Q2 6,696,318; Q4 6,663,805; Q1 6,489,321; Q3 5,646,858)\n",
      "\n",
      "### 4) ...\n",
      "--------------------------------------------------\n",
      "\n",
      "üéüÔ∏è  Token Usage (Code Execution Approach):\n",
      "   Prompt tokens:     122\n",
      "   Response tokens:   543\n",
      "   Data in sandbox:   ~56,383 (NOT counted!)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL tokens:      665\n",
      "\n",
      "======================================================================\n",
      "üìä Comparison: Full Context vs Code Execution\n",
      "======================================================================\n",
      "   Full Context (1.1):    56,772 tokens\n",
      "   Code Execution (1.2): 665 tokens\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   üí∞ Token Savings:     98.8%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agent_framework._types.AgentRunResponse at 0x78391f05fc50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1.2: Code Execution Approach - Data Generation with Remote Code Execution\n",
    "# This demonstrates token efficiency when data is generated and processed in a sandbox\n",
    "\n",
    "from agent_framework.azure import AzureAIClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "\n",
    "async def code_execution_example():\n",
    "    \"\"\"\n",
    "    Code Execution approach: Generate and analyze data in a sandbox.\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Remote code execution via CodeInterpreterTool\n",
    "    - Data generation inside sandbox (never enters context)\n",
    "    - Only summary returned to model\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ 1.2 [Best Practice] Data Generation in Sandbox\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # User query that triggers code execution\n",
    "    code_execution_query = \"\"\"\n",
    "    Generate 1000 sales records with the following structure:\n",
    "    - id, product (Widget A/B, Gadget X/Y, Tool Z), amount (100-50000), quarter (Q1-Q4), region (North/South/East/West), date\n",
    "\n",
    "    Then analyze the data and return ONLY a summary with:\n",
    "    1. Total records and total sales amount\n",
    "    2. Top 3 products by sales\n",
    "    3. Best quarter by revenue\n",
    "    4. Regional breakdown\n",
    "\n",
    "    Use Python with pandas. Return the summary as a formatted report.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìù Sending code execution request...\")\n",
    "    print(f\"   Prompt tokens (estimated): {estimate_tokens(code_execution_query):,}\")\n",
    "    print(f\"   Note: Raw data stays in sandbox - never enters model context!\")\n",
    "    \n",
    "    async with (\n",
    "        # Use AzureAIClient pattern with CodeInterpreterTool\n",
    "        AzureAIClient(credential=credential, project_endpoint=AZURE_AI_PROJECT_ENDPOINT).create_agent(\n",
    "            name=\"DataAnalystAgent\",\n",
    "            instructions=\"You are a data analyst. Generate data using Python code and return only summarized results.\",\n",
    "            tools=CodeInterpreterTool(),\n",
    "        ) as agent,\n",
    "    ):\n",
    "        result = await agent.run(code_execution_query)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Code executed in sandbox!\")\n",
    "        print(f\"\\nüìã Summary (from sandbox execution):\")\n",
    "        print(\"-\" * 50)\n",
    "        # Get the response text\n",
    "        response_text = str(result)\n",
    "        print(response_text[:1500] + \"...\" if len(response_text) > 1500 else response_text)\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate token usage for code execution approach\n",
    "        # Only the prompt and summary pass through - not the 1000 records!\n",
    "        summary_tokens = estimate_tokens(response_text)\n",
    "        prompt_tokens_code_exec = estimate_tokens(code_execution_query)\n",
    "        total_tokens_code_exec = prompt_tokens_code_exec + summary_tokens\n",
    "        \n",
    "        print(f\"\\nüéüÔ∏è  Token Usage (Code Execution Approach):\")\n",
    "        print(f\"   Prompt tokens:     {prompt_tokens_code_exec:,}\")\n",
    "        print(f\"   Response tokens:   {summary_tokens:,}\")\n",
    "        print(f\"   Data in sandbox:   ~{estimate_tokens(full_data_json):,} (NOT counted!)\")\n",
    "        print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        print(f\"   TOTAL tokens:      {total_tokens_code_exec:,}\")\n",
    "        \n",
    "        # Compare with traditional approach\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"üìä Comparison: Full Context vs Code Execution\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   Full Context (1.1):    {total_tokens_full_context:,} tokens\")\n",
    "        print(f\"   Code Execution (1.2): {total_tokens_code_exec:,} tokens\")\n",
    "        print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        savings = ((total_tokens_full_context - total_tokens_code_exec) / total_tokens_full_context) * 100\n",
    "        print(f\"   üí∞ Token Savings:     {savings:.1f}%\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Run the example\n",
    "await code_execution_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b491e79",
   "metadata": {},
   "source": [
    "### Section 2: Multi-Step Code Execution\n",
    "\n",
    "This section compares sending **full data through complex workflows** (anti-pattern) vs **multi-step pipeline in single execution** (best practice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22648878",
   "metadata": {},
   "source": [
    "### Example 2.1: [Anti-Pattern] Complex Workflows with Full Data\n",
    "\n",
    "In this example, we execute a **4-step data analysis workflow** using the traditional approach. Each step requires sending the **full dataset** to Azure OpenAI, demonstrating the token cost of multi-step workflows.\n",
    "\n",
    "| Step | Description | Data Sent |\n",
    "|------|-------------|-----------|\n",
    "| 1 | Analyze data structure | Full dataset |\n",
    "| 2 | Categorize amounts | Full dataset |\n",
    "| 3 | Aggregate by dimensions | Full dataset |\n",
    "| 4 | Generate final report | Full dataset |\n",
    "\n",
    "**Problem**: The same dataset is sent 4 times, consuming tokens each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06701328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä 2.1 [Anti-Pattern] Complex Workflows (4 Round Trips)\n",
      "======================================================================\n",
      "\n",
      "üîÑ Step 1: Analyzing data structure...\n",
      "   ‚úÖ Step 1 completed: 56,726 tokens\n",
      "\n",
      "üîÑ Step 2: Categorizing amounts...\n",
      "   ‚úÖ Step 2 completed: 56,765 tokens\n",
      "\n",
      "üîÑ Step 3: Aggregating by quarter and region...\n",
      "   ‚úÖ Step 3 completed: 56,854 tokens\n",
      "\n",
      "üîÑ Step 4: Generating final report...\n",
      "   ‚úÖ Step 4 completed: 56,947 tokens\n",
      "\n",
      "======================================================================\n",
      "üìã Final Report (from Step 4):\n",
      "--------------------------------------------------\n",
      "Here is an executive summary based on the analysis of the provided sales data (1,000 transactions across products, regions, and quarters for 2025):\n",
      "\n",
      "---\n",
      "\n",
      "### Executive Summary\n",
      "\n",
      "#### 1. Overall Performance Metrics\n",
      "\n",
      "- **Total Revenue:**  \n",
      "  $31,513,603.08\n",
      "\n",
      "- **Average Sale (per transaction):**  \n",
      "  $31,513.60\n",
      "\n",
      "- **Maximum Single Sale (Transaction):**  \n",
      "  $49,996.19 (Gadget X, North, Q1)\n",
      "\n",
      "- **Minimum Single Sale (Transaction):**  \n",
      "  $102.41 (Tool Z, West, Q2)\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. Best Performing Quarter\n",
      "\n",
      "| Quarter | Total Revenue    | % of Revenue | Number of Transactions |\n",
      "|---------|------------------|--------------|-----------------------|\n",
      "| **Q1**  | $8,639,696.39    | 27.42%       | 262                   |\n",
      "| **Q2**  | $7,722,925.66    | 24.52%       | 240                   |\n",
      "| **Q3**  | $8,773,547.83    | 27.85%       | 261                   |\n",
      "| **Q4**  | $6,377,433.20    | 20.22%       | 237                   |\n",
      "| **Total** | $31,513,603.08 |              | 1,000                 |\n",
      "\n",
      "**Best Performing Quarter:**  \n",
      "**Q3** generated the highest total revenue ($8.77M), with the highest number of high-value sales concentrated in this period. Q1 was a close second in performance.\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. Regional Insights\n",
      "\n",
      "| Region | Total Revenue    | % of Revenue | Number of Transactions |\n",
      "|--------|------------------|--------------|-----------------------|\n",
      "| **North** | $8,234,922.25 | 26.13%       | 256                   |\n",
      "| **South** | $7,867,928.13 | 24.97%       | 253                   |\n",
      "| **East** | $7,667,640.87  | 24.35%       | 241                   |\n",
      "| **West** | $7,743,111.83  | 24.58%       | 250                   |\n",
      "\n",
      "- **Top Region:** The **North** region led in total sales, making up more than a quarter of organization\n",
      "--------------------------------------------------\n",
      "\n",
      "üéüÔ∏è  Token Usage (Traditional 4-Step Workflow):\n",
      "   Step 1 (Structure):    56,726 tokens\n",
      "   Step 2 (Categorize):   56,765 tokens\n",
      "   Step 3 (Aggregate):    56,854 tokens\n",
      "   Step 4 (Report):       56,947 tokens\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL tokens:          227,292 tokens\n",
      "\n",
      "‚ö†Ô∏è  Note: Full dataset (1000 records) was sent 4 times!\n",
      "   Data tokens per trip:  ~56,383\n",
      "   Data tokens (4 trips): ~225,532\n"
     ]
    }
   ],
   "source": [
    "# Example 2.1: Complex Workflows with Full Data (4 Round Trips)\n",
    "# This demonstrates the token cost when executing multi-step workflows traditionally\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def traditional_multi_step_workflow():\n",
    "    \"\"\"\n",
    "    Traditional approach: Execute 4 separate API calls, each with full data.\n",
    "    \n",
    "    This simulates a typical multi-step workflow:\n",
    "    1. Generate/Load data\n",
    "    2. Transform data\n",
    "    3. Aggregate data\n",
    "    4. Generate report\n",
    "    \n",
    "    Each step requires sending full data to the model.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä 2.1 [Anti-Pattern] Complex Workflows (4 Round Trips)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create Azure OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    total_tokens_all_steps = 0\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 1: Analyze raw data structure\n",
    "    # ========================================\n",
    "    print(\"\\nüîÑ Step 1: Analyzing data structure...\")\n",
    "    step1_prompt = f\"\"\"Analyze the following sales data and describe its structure:\n",
    "- Number of records\n",
    "- Fields available\n",
    "- Data types\n",
    "\n",
    "Sales Data:\n",
    "{full_data_json}\n",
    "\"\"\"\n",
    "    \n",
    "    response1 = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": step1_prompt}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    step1_tokens = response1.usage.total_tokens\n",
    "    total_tokens_all_steps += step1_tokens\n",
    "    print(f\"   ‚úÖ Step 1 completed: {step1_tokens:,} tokens\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 2: Transform and categorize data\n",
    "    # ========================================\n",
    "    print(\"\\nüîÑ Step 2: Categorizing amounts...\")\n",
    "    step2_prompt = f\"\"\"Analyze the sales data and categorize the amounts:\n",
    "- Low: < 1,000\n",
    "- Medium: 1,000 - 5,000\n",
    "- High: 5,000 - 20,000\n",
    "- Premium: > 20,000\n",
    "\n",
    "Count how many records fall into each category.\n",
    "\n",
    "Sales Data:\n",
    "{full_data_json}\n",
    "\"\"\"\n",
    "    \n",
    "    response2 = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": step2_prompt}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    step2_tokens = response2.usage.total_tokens\n",
    "    total_tokens_all_steps += step2_tokens\n",
    "    print(f\"   ‚úÖ Step 2 completed: {step2_tokens:,} tokens\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 3: Aggregate by dimensions\n",
    "    # ========================================\n",
    "    print(\"\\nüîÑ Step 3: Aggregating by quarter and region...\")\n",
    "    step3_prompt = f\"\"\"Analyze the sales data and provide:\n",
    "1. Total sales by quarter (Q1, Q2, Q3, Q4)\n",
    "2. Total sales by region (North, South, East, West)\n",
    "3. Top 3 products by total sales\n",
    "\n",
    "Sales Data:\n",
    "{full_data_json}\n",
    "\"\"\"\n",
    "    \n",
    "    response3 = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": step3_prompt}\n",
    "        ],\n",
    "        max_tokens=400,\n",
    "    )\n",
    "    step3_tokens = response3.usage.total_tokens\n",
    "    total_tokens_all_steps += step3_tokens\n",
    "    print(f\"   ‚úÖ Step 3 completed: {step3_tokens:,} tokens\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 4: Generate final report\n",
    "    # ========================================\n",
    "    print(\"\\nüîÑ Step 4: Generating final report...\")\n",
    "    step4_prompt = f\"\"\"Based on the sales data, generate a comprehensive executive summary including:\n",
    "1. Overall performance metrics (total revenue, average sale, max/min)\n",
    "2. Best performing quarter\n",
    "3. Regional insights\n",
    "4. Product recommendations\n",
    "\n",
    "Sales Data:\n",
    "{full_data_json}\n",
    "\"\"\"\n",
    "    \n",
    "    response4 = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a business analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": step4_prompt}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    step4_tokens = response4.usage.total_tokens\n",
    "    total_tokens_all_steps += step4_tokens\n",
    "    print(f\"   ‚úÖ Step 4 completed: {step4_tokens:,} tokens\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Summary\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìã Final Report (from Step 4):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response4.choices[0].message.content)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\nüéüÔ∏è  Token Usage (Traditional 4-Step Workflow):\")\n",
    "    print(f\"   Step 1 (Structure):    {step1_tokens:,} tokens\")\n",
    "    print(f\"   Step 2 (Categorize):   {step2_tokens:,} tokens\")\n",
    "    print(f\"   Step 3 (Aggregate):    {step3_tokens:,} tokens\")\n",
    "    print(f\"   Step 4 (Report):       {step4_tokens:,} tokens\")\n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"   TOTAL tokens:          {total_tokens_all_steps:,} tokens\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Full dataset ({len(sample_data)} records) was sent 4 times!\")\n",
    "    print(f\"   Data tokens per trip:  ~{estimate_tokens(full_data_json):,}\")\n",
    "    print(f\"   Data tokens (4 trips): ~{estimate_tokens(full_data_json) * 4:,}\")\n",
    "    \n",
    "    return total_tokens_all_steps\n",
    "\n",
    "# Run the complex workflow\n",
    "total_tokens_4trips = traditional_multi_step_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71623ef1",
   "metadata": {},
   "source": [
    "### 2.2 [Best Practice] Multi-Step Pipeline in Single Execution\n",
    "\n",
    "With code execution, agents can use familiar programming constructs (loops, conditionals, error handling) to execute **complex workflows in a single step** rather than multiple round trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b37a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ 2.2 [Best Practice] Multi-Step Pipeline in Single Execution\n",
      "======================================================================\n",
      "\n",
      "üìù Sending multi-step pipeline request...\n",
      "   Prompt tokens (estimated): 232\n",
      "   Note: All 4 steps execute in sandbox - only final report returned!\n",
      "\n",
      "‚úÖ Pipeline executed in sandbox!\n",
      "\n",
      "üìã Pipeline Result:\n",
      "--------------------------------------------------\n",
      "import pandas as pd, numpy as np\n",
      "np.random.seed(42)\n",
      "n=1000\n",
      "products=[\"Widget A\",\"Widget B\",\"Gadget X\",\"Gadget Y\",\"Tool Z\"]\n",
      "quarters=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"]\n",
      "regions=[\"North\",\"South\",\"East\",\"West\"]\n",
      "# generate dates within 2025 maybe\n",
      "start=pd.Timestamp(\"2025-01-01\")\n",
      "end=pd.Timestamp(\"2025-12-31\")\n",
      "dates=pd.to_datetime(np.random.randint(start.value//10**9, end.value//10**9, n), unit='s')\n",
      "df=pd.DataFrame({\n",
      "    \"id\": np.arange(1,n+1),\n",
      "    \"product\": np.random.choice(products,n, p=[0.22,0.18,0.2,0.2,0.2]),\n",
      "    \"amount\": np.random.randint(100,50001,n),\n",
      "    \"quarter\": np.random.choice(quarters,n),\n",
      "    \"region\": np.random.choice(regions,n),\n",
      "    \"date\": dates\n",
      "})\n",
      "# transform\n",
      "bins=[-np.inf,999,5000,20000,np.inf]\n",
      "labels=[\"Low\",\"Medium\",\"High\",\"Premium\"]\n",
      "df[\"category\"]=pd.cut(df[\"amount\"], bins=bins, labels=labels)\n",
      "df[\"date\"]=pd.to_datetime(df[\"date\"])\n",
      "df[\"month\"]=df[\"date\"].dt.month\n",
      "# aggregates\n",
      "quarterly=df.groupby(\"quarter\")[\"amount\"].agg(total=\"sum\", avg=\"mean\").reindex(quarters)\n",
      "regional=df.groupby(\"region\")[\"amount\"].agg(total=\"sum\", avg=\"mean\").reindex(regions)\n",
      "category_dist=df[\"category\"].value_counts().reindex(labels)\n",
      "top_products=df.groupby(\"product\")[\"amount\"].sum().sort_values(ascending=False)\n",
      "# overall metrics\n",
      "total_revenue=df[\"amount\"].sum()\n",
      "avg_sale=df[\"amount\"].mean()\n",
      "max_sale=df[\"amount\"].max()\n",
      "min_sale=df[\"amount\"].min()\n",
      "total_records=len(df)\n",
      "quarter_totals=quarterly[\"total\"]\n",
      "# format numbers\n",
      "def money(x): \n",
      "    return f\"${x:,.0f}\"\n",
      "def money2(x):\n",
      "    return f\"${x:,.2f}\"\n",
      "report = {\n",
      "\"Total records processed\": total_records,\n",
      "\"Total revenue\": money(total_revenue),\n",
      "\"Average sale\": money2(avg_sale),\n",
      "\"Max sale\": money(max_sale),\n",
      "\"Min sale\": money(min_sale),\n",
      "\"Quarterly totals\": {q: money(v) for q,v in quarter_totals.items()},\n",
      "\"Regional totals\": {r: money(v) for r,v in regional[\"total\"].items()},\n",
      "\"Category distribution (count, %)\": {cat: (int(category_dist[cat]), f\"{category_dist[cat]/total_records*100:,.1f}%\") for cat in labels},\n",
      "\"Top products by revenue\": {p: money(v) for p...\n",
      "--------------------------------------------------\n",
      "\n",
      "üéüÔ∏è  Token Usage (Multi-Step Pipeline):\n",
      "   Prompt tokens:          232\n",
      "   Response tokens:        916\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL tokens:           1,148\n",
      "\n",
      "üìä Comparison: Traditional 4 Round Trips (2.1) vs Single Pipeline (2.2)\n",
      "   Traditional (1.4):       227,292 tokens\n",
      "   Code Execution (2.2):    1,148 tokens\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   üí∞ Token Savings:        99.5%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agent_framework._types.AgentRunResponse at 0x78391f0aa780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2.2: Multi-Step Pipeline in Single Code Execution\n",
    "# This demonstrates executing complex multi-step workflows in a single agent call\n",
    "\n",
    "from agent_framework.azure import AzureAIClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "\n",
    "async def multi_step_pipeline_example():\n",
    "    \"\"\"\n",
    "    Multi-step data pipeline executed in a single agent call.\n",
    "    \n",
    "    Traditional approach would require 4+ round trips:\n",
    "    1. Get data -> tokens consumed\n",
    "    2. Transform data -> tokens consumed\n",
    "    3. Aggregate data -> tokens consumed\n",
    "    4. Format output -> tokens consumed\n",
    "    \n",
    "    Code execution: All steps run in sandbox, only summary returned.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ 2.2 [Best Practice] Multi-Step Pipeline in Single Execution\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Complex multi-step pipeline request\n",
    "    pipeline_query = \"\"\"\n",
    "    Execute a complete data analysis pipeline with these steps:\n",
    "    \n",
    "    **Step 1: Generate Data**\n",
    "    Create 1000 sales records with: id, product (Widget A/B, Gadget X/Y, Tool Z), \n",
    "    amount (100-50000), quarter (Q1-Q4), region (North/South/East/West), date\n",
    "    \n",
    "    **Step 2: Transform Data**\n",
    "    - Categorize amounts: Low (<1000), Medium (1000-5000), High (5000-20000), Premium (>20000)\n",
    "    - Parse dates and extract month\n",
    "    \n",
    "    **Step 3: Aggregate by Multiple Dimensions**\n",
    "    - Quarterly totals and averages\n",
    "    - Regional breakdown\n",
    "    - Category distribution\n",
    "    - Top products\n",
    "    \n",
    "    **Step 4: Generate Report**\n",
    "    Return a formatted summary report with:\n",
    "    - Total records processed\n",
    "    - Quarterly performance (Q1-Q4 totals)\n",
    "    - Regional performance\n",
    "    - Overall metrics (total revenue, avg sale, max/min)\n",
    "    - Category distribution\n",
    "    \n",
    "    Use Python with pandas. Execute all steps and return ONLY the final summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìù Sending multi-step pipeline request...\")\n",
    "    print(f\"   Prompt tokens (estimated): {estimate_tokens(pipeline_query):,}\")\n",
    "    print(f\"   Note: All 4 steps execute in sandbox - only final report returned!\")\n",
    "    \n",
    "    async with (\n",
    "        AzureAIClient(credential=credential, project_endpoint=AZURE_AI_PROJECT_ENDPOINT).create_agent(\n",
    "            name=\"PipelineAgent\",\n",
    "            instructions=\"You are a data engineer. Execute multi-step data pipelines and return only summarized results.\",\n",
    "            tools=CodeInterpreterTool(),\n",
    "        ) as agent,\n",
    "    ):\n",
    "        result = await agent.run(pipeline_query)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Pipeline executed in sandbox!\")\n",
    "        print(f\"\\nüìã Pipeline Result:\")\n",
    "        print(\"-\" * 50)\n",
    "        response_text = str(result)\n",
    "        print(response_text[:2000] + \"...\" if len(response_text) > 2000 else response_text)\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Token analysis\n",
    "        response_tokens = estimate_tokens(response_text)\n",
    "        prompt_tokens = estimate_tokens(pipeline_query)\n",
    "        total_tokens_pipeline = prompt_tokens + response_tokens\n",
    "        \n",
    "        print(f\"\\nüéüÔ∏è  Token Usage (Multi-Step Pipeline):\")\n",
    "        print(f\"   Prompt tokens:          {prompt_tokens:,}\")\n",
    "        print(f\"   Response tokens:        {response_tokens:,}\")\n",
    "        print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        print(f\"   TOTAL tokens:           {total_tokens_pipeline:,}\")\n",
    "        \n",
    "        # Compare with actual traditional approach from Example 2.1\n",
    "        print(f\"\\nüìä Comparison: Traditional 4 Round Trips (2.1) vs Single Pipeline (2.2)\")\n",
    "        print(f\"   Traditional (1.4):       {total_tokens_4trips:,} tokens\")\n",
    "        print(f\"   Code Execution (2.2):    {total_tokens_pipeline:,} tokens\")\n",
    "        print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        savings = ((total_tokens_4trips - total_tokens_pipeline) / total_tokens_4trips) * 100\n",
    "        print(f\"   üí∞ Token Savings:        {savings:.1f}%\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Run the example\n",
    "await multi_step_pipeline_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93caee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 3: MCP Optimization\n",
    "\n",
    "This section compares **loading all MCP tools** (anti-pattern) vs **filtering tools by intent** (best practice).\n",
    "\n",
    "### 3.1 [Anti-Pattern] Remote MCP Schema Bloat\n",
    "\n",
    "When connecting to **large MCP servers** (like Azure MCP with 50+ tools), the tool schema definitions alone consume significant context tokens. Every conversation turn includes ALL tool definitions, even if only one tool is used.\n",
    "\n",
    "| MCP Server | Estimated Tools | Schema Tokens |\n",
    "|------------|-----------------|---------------|\n",
    "| Calculator | 4-5 tools | ~500 tokens |\n",
    "| Filesystem | 10-15 tools | ~2,000 tokens |\n",
    "| GitHub | 20-30 tools | ~5,000 tokens |\n",
    "| **Azure MCP** | **50+ tools** | **~15,000+ tokens** |\n",
    "\n",
    "**Problem**: Tool schemas are included in EVERY API call, multiplying token cost with conversation length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b727ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä 3.1 [Anti-Pattern] Remote MCP Schema Bloat\n",
      "======================================================================\n",
      "\n",
      "üîß Simulated Azure MCP Server:\n",
      "   Total tools: 53\n",
      "   Schema tokens: 7,309\n",
      "\n",
      "üîÑ Turn 1: List all my storage containers in the 'production-...\n",
      "   ‚úÖ Tokens used: 2,179 (prompt: 2,113)\n",
      "\n",
      "üîÑ Turn 2: Now show me all VMs in the same resource group....\n",
      "   ‚úÖ Tokens used: 2,244 (prompt: 2,197)\n",
      "\n",
      "üîÑ Turn 3: Finally, get the connection string secret from my ...\n",
      "   ‚úÖ Tokens used: 2,324 (prompt: 2,264)\n",
      "\n",
      "======================================================================\n",
      "üéüÔ∏è  Token Usage Summary (Traditional - All Tools Every Turn)\n",
      "======================================================================\n",
      "   Turn 1: 2,179 tokens (prompt: 2,113)\n",
      "   Turn 2: 2,244 tokens (prompt: 2,197)\n",
      "   Turn 3: 2,324 tokens (prompt: 2,264)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL (3 turns): 6,747 tokens\n",
      "\n",
      "‚ö†Ô∏è  Problem Analysis:\n",
      "   Tool schema tokens:     7,309 (per turn)\n",
      "   Schema √ó 3 turns:       ~21,927 tokens\n",
      "   Actual total:           6,747 tokens\n",
      "   Schema overhead:        ~325.0%\n"
     ]
    }
   ],
   "source": [
    "# Example 3.1 [Anti-Pattern] Tool Schema Bloat with Large MCP Servers\n",
    "# This demonstrates how large tool schemas consume context even before any work is done\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def simulate_large_mcp_schema():\n",
    "    \"\"\"\n",
    "    Simulates the tool schema from a large MCP server like Azure MCP.\n",
    "    In reality, Azure MCP has 50+ tools for various Azure services.\n",
    "    \"\"\"\n",
    "    # Simulated Azure MCP tool definitions (simplified)\n",
    "    azure_mcp_tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_storage_list_containers\",\n",
    "                \"description\": \"List all containers in an Azure Storage account. Returns container names, metadata, and properties.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"storage_account\": {\"type\": \"string\", \"description\": \"Name of the Azure Storage account\"},\n",
    "                        \"resource_group\": {\"type\": \"string\", \"description\": \"Resource group containing the storage account\"},\n",
    "                        \"subscription_id\": {\"type\": \"string\", \"description\": \"Azure subscription ID\"},\n",
    "                        \"include_metadata\": {\"type\": \"boolean\", \"description\": \"Whether to include container metadata\"},\n",
    "                        \"include_deleted\": {\"type\": \"boolean\", \"description\": \"Whether to include soft-deleted containers\"},\n",
    "                    },\n",
    "                    \"required\": [\"storage_account\", \"resource_group\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_storage_upload_blob\",\n",
    "                \"description\": \"Upload a blob to an Azure Storage container. Supports block blobs, append blobs, and page blobs.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"storage_account\": {\"type\": \"string\", \"description\": \"Name of the Azure Storage account\"},\n",
    "                        \"container\": {\"type\": \"string\", \"description\": \"Name of the container\"},\n",
    "                        \"blob_name\": {\"type\": \"string\", \"description\": \"Name of the blob to create\"},\n",
    "                        \"content\": {\"type\": \"string\", \"description\": \"Content to upload\"},\n",
    "                        \"blob_type\": {\"type\": \"string\", \"enum\": [\"BlockBlob\", \"AppendBlob\", \"PageBlob\"]},\n",
    "                        \"content_type\": {\"type\": \"string\", \"description\": \"MIME type of the content\"},\n",
    "                        \"metadata\": {\"type\": \"object\", \"description\": \"Custom metadata for the blob\"},\n",
    "                    },\n",
    "                    \"required\": [\"storage_account\", \"container\", \"blob_name\", \"content\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_vm_list\",\n",
    "                \"description\": \"List all virtual machines in a subscription or resource group.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"subscription_id\": {\"type\": \"string\", \"description\": \"Azure subscription ID\"},\n",
    "                        \"resource_group\": {\"type\": \"string\", \"description\": \"Optional resource group filter\"},\n",
    "                        \"status_filter\": {\"type\": \"string\", \"enum\": [\"running\", \"stopped\", \"deallocated\", \"all\"]},\n",
    "                    },\n",
    "                    \"required\": [\"subscription_id\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_vm_start\",\n",
    "                \"description\": \"Start an Azure virtual machine.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"vm_name\": {\"type\": \"string\", \"description\": \"Name of the virtual machine\"},\n",
    "                        \"resource_group\": {\"type\": \"string\", \"description\": \"Resource group containing the VM\"},\n",
    "                        \"subscription_id\": {\"type\": \"string\", \"description\": \"Azure subscription ID\"},\n",
    "                    },\n",
    "                    \"required\": [\"vm_name\", \"resource_group\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_vm_stop\",\n",
    "                \"description\": \"Stop an Azure virtual machine. Can optionally deallocate to save costs.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"vm_name\": {\"type\": \"string\", \"description\": \"Name of the virtual machine\"},\n",
    "                        \"resource_group\": {\"type\": \"string\", \"description\": \"Resource group containing the VM\"},\n",
    "                        \"subscription_id\": {\"type\": \"string\", \"description\": \"Azure subscription ID\"},\n",
    "                        \"deallocate\": {\"type\": \"boolean\", \"description\": \"Whether to deallocate (true) or just stop (false)\"},\n",
    "                    },\n",
    "                    \"required\": [\"vm_name\", \"resource_group\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_keyvault_get_secret\",\n",
    "                \"description\": \"Retrieve a secret from Azure Key Vault.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"vault_name\": {\"type\": \"string\", \"description\": \"Name of the Key Vault\"},\n",
    "                        \"secret_name\": {\"type\": \"string\", \"description\": \"Name of the secret\"},\n",
    "                        \"version\": {\"type\": \"string\", \"description\": \"Optional secret version\"},\n",
    "                    },\n",
    "                    \"required\": [\"vault_name\", \"secret_name\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_keyvault_set_secret\",\n",
    "                \"description\": \"Create or update a secret in Azure Key Vault.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"vault_name\": {\"type\": \"string\", \"description\": \"Name of the Key Vault\"},\n",
    "                        \"secret_name\": {\"type\": \"string\", \"description\": \"Name of the secret\"},\n",
    "                        \"value\": {\"type\": \"string\", \"description\": \"Value of the secret\"},\n",
    "                        \"content_type\": {\"type\": \"string\", \"description\": \"Content type of the secret\"},\n",
    "                        \"expires_on\": {\"type\": \"string\", \"description\": \"Expiration date in ISO format\"},\n",
    "                    },\n",
    "                    \"required\": [\"vault_name\", \"secret_name\", \"value\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_cosmos_query\",\n",
    "                \"description\": \"Execute a SQL query against an Azure Cosmos DB container.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"account\": {\"type\": \"string\", \"description\": \"Cosmos DB account name\"},\n",
    "                        \"database\": {\"type\": \"string\", \"description\": \"Database name\"},\n",
    "                        \"container\": {\"type\": \"string\", \"description\": \"Container name\"},\n",
    "                        \"query\": {\"type\": \"string\", \"description\": \"SQL query to execute\"},\n",
    "                        \"parameters\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}, \"description\": \"Query parameters\"},\n",
    "                        \"max_items\": {\"type\": \"integer\", \"description\": \"Maximum items to return\"},\n",
    "                    },\n",
    "                    \"required\": [\"account\", \"database\", \"container\", \"query\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_sql_execute\",\n",
    "                \"description\": \"Execute a SQL query against Azure SQL Database.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"server\": {\"type\": \"string\", \"description\": \"SQL Server name\"},\n",
    "                        \"database\": {\"type\": \"string\", \"description\": \"Database name\"},\n",
    "                        \"query\": {\"type\": \"string\", \"description\": \"SQL query to execute\"},\n",
    "                        \"parameters\": {\"type\": \"object\", \"description\": \"Query parameters\"},\n",
    "                    },\n",
    "                    \"required\": [\"server\", \"database\", \"query\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"azure_function_invoke\",\n",
    "                \"description\": \"Invoke an Azure Function via HTTP trigger.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"function_app\": {\"type\": \"string\", \"description\": \"Function App name\"},\n",
    "                        \"function_name\": {\"type\": \"string\", \"description\": \"Function name\"},\n",
    "                        \"method\": {\"type\": \"string\", \"enum\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"]},\n",
    "                        \"body\": {\"type\": \"object\", \"description\": \"Request body\"},\n",
    "                        \"headers\": {\"type\": \"object\", \"description\": \"Request headers\"},\n",
    "                    },\n",
    "                    \"required\": [\"function_app\", \"function_name\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Add more tools to simulate a realistic large MCP server (50+ tools)\n",
    "    additional_services = [\n",
    "        \"azure_monitor_query_logs\", \"azure_monitor_get_metrics\", \"azure_monitor_create_alert\",\n",
    "        \"azure_aks_list_clusters\", \"azure_aks_get_credentials\", \"azure_aks_scale_nodepool\",\n",
    "        \"azure_acr_list_repos\", \"azure_acr_push_image\", \"azure_acr_delete_image\",\n",
    "        \"azure_servicebus_send_message\", \"azure_servicebus_receive_messages\", \"azure_servicebus_create_queue\",\n",
    "        \"azure_eventhub_send_event\", \"azure_eventhub_receive_events\", \"azure_eventhub_create_hub\",\n",
    "        \"azure_redis_get\", \"azure_redis_set\", \"azure_redis_delete\", \"azure_redis_list_keys\",\n",
    "        \"azure_apim_list_apis\", \"azure_apim_create_api\", \"azure_apim_get_subscription_key\",\n",
    "        \"azure_logic_app_trigger\", \"azure_logic_app_list_runs\", \"azure_logic_app_cancel_run\",\n",
    "        \"azure_cdn_purge\", \"azure_cdn_list_endpoints\", \"azure_cdn_create_profile\",\n",
    "        \"azure_frontdoor_list_routes\", \"azure_frontdoor_create_origin\", \"azure_frontdoor_update_policy\",\n",
    "        \"azure_dns_list_zones\", \"azure_dns_create_record\", \"azure_dns_delete_record\",\n",
    "        \"azure_network_list_vnets\", \"azure_network_create_subnet\", \"azure_network_get_nsg_rules\",\n",
    "        \"azure_appservice_list_apps\", \"azure_appservice_deploy\", \"azure_appservice_restart\",\n",
    "        \"azure_batch_create_job\", \"azure_batch_list_tasks\", \"azure_batch_get_output\",\n",
    "    ]\n",
    "    \n",
    "    for service in additional_services:\n",
    "        azure_mcp_tools.append({\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": service,\n",
    "                \"description\": f\"Execute {service.replace('_', ' ')} operation in Azure.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"resource_id\": {\"type\": \"string\", \"description\": \"Azure resource ID\"},\n",
    "                        \"options\": {\"type\": \"object\", \"description\": \"Operation-specific options\"},\n",
    "                    },\n",
    "                    \"required\": [\"resource_id\"]\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return azure_mcp_tools\n",
    "\n",
    "\n",
    "def traditional_tool_schema_bloat_example():\n",
    "    \"\"\"\n",
    "    Traditional approach: All tool schemas included in every API call.\n",
    "    \n",
    "    This demonstrates the problem of tool schema bloat where:\n",
    "    - 50+ tool definitions consume ~15,000+ tokens\n",
    "    - Every conversation turn repeats all tool definitions\n",
    "    - Multi-turn conversations multiply the token cost\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä 3.1 [Anti-Pattern] Remote MCP Schema Bloat\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get simulated large MCP tools\n",
    "    azure_mcp_tools = simulate_large_mcp_schema()\n",
    "    \n",
    "    tools_json = json.dumps(azure_mcp_tools, indent=2)\n",
    "    tools_token_count = estimate_tokens(tools_json)\n",
    "    \n",
    "    print(f\"\\nüîß Simulated Azure MCP Server:\")\n",
    "    print(f\"   Total tools: {len(azure_mcp_tools)}\")\n",
    "    print(f\"   Schema tokens: {tools_token_count:,}\")\n",
    "    \n",
    "    # Create Azure OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Simulate a 3-turn conversation with tool schema bloat\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an Azure infrastructure assistant. Help users manage their Azure resources.\"}\n",
    "    ]\n",
    "    \n",
    "    user_queries = [\n",
    "        \"List all my storage containers in the 'production-rg' resource group.\",\n",
    "        \"Now show me all VMs in the same resource group.\",\n",
    "        \"Finally, get the connection string secret from my 'prod-keyvault'.\",\n",
    "    ]\n",
    "    \n",
    "    total_tokens_all_turns = 0\n",
    "    turn_details = []\n",
    "    \n",
    "    for turn_num, query in enumerate(user_queries, 1):\n",
    "        print(f\"\\nüîÑ Turn {turn_num}: {query[:50]}...\")\n",
    "        \n",
    "        # Add user message\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Make API call with ALL tools (traditional approach)\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            messages=conversation_history,\n",
    "            tools=azure_mcp_tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        usage = response.usage\n",
    "        total_tokens_all_turns += usage.total_tokens\n",
    "        \n",
    "        # Get assistant response\n",
    "        assistant_message = response.choices[0].message\n",
    "        \n",
    "        # Add assistant response to history (simulated tool response)\n",
    "        if assistant_message.tool_calls:\n",
    "            tool_name = assistant_message.tool_calls[0].function.name\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": None, \"tool_calls\": [\n",
    "                {\"id\": \"call_1\", \"type\": \"function\", \"function\": {\"name\": tool_name, \"arguments\": \"{}\"}}\n",
    "            ]})\n",
    "            # Simulate tool response\n",
    "            conversation_history.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_1\", \n",
    "                \"content\": f\"[Simulated response from {tool_name}]\"\n",
    "            })\n",
    "        else:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message.content or \"\"})\n",
    "        \n",
    "        turn_details.append({\n",
    "            \"turn\": turn_num,\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Tokens used: {usage.total_tokens:,} (prompt: {usage.prompt_tokens:,})\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéüÔ∏è  Token Usage Summary (Traditional - All Tools Every Turn)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for detail in turn_details:\n",
    "        print(f\"   Turn {detail['turn']}: {detail['total_tokens']:,} tokens (prompt: {detail['prompt_tokens']:,})\")\n",
    "    \n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"   TOTAL (3 turns): {total_tokens_all_turns:,} tokens\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Problem Analysis:\")\n",
    "    print(f\"   Tool schema tokens:     {tools_token_count:,} (per turn)\")\n",
    "    print(f\"   Schema √ó 3 turns:       ~{tools_token_count * 3:,} tokens\")\n",
    "    print(f\"   Actual total:           {total_tokens_all_turns:,} tokens\")\n",
    "    print(f\"   Schema overhead:        ~{(tools_token_count * 3 / total_tokens_all_turns * 100):.1f}%\")\n",
    "    \n",
    "    return total_tokens_all_turns, len(azure_mcp_tools), tools_token_count\n",
    "\n",
    "# Run the example\n",
    "total_tokens_schema_bloat, num_tools, schema_tokens = traditional_tool_schema_bloat_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea2dc9",
   "metadata": {},
   "source": [
    "### 3.2 [Best Practice] Tool Filtering + History Limit\n",
    "\n",
    "Instead of loading ALL tools from a large MCP server, we can:\n",
    "1. **Filter tools on-demand** - Only load tools relevant to the current task\n",
    "2. **Limit conversation history** - Keep only the last N turns to reduce context size\n",
    "\n",
    "This approach dramatically reduces token consumption while maintaining functionality.\n",
    "\n",
    "| Optimization | Anti-Pattern (3.1) | Best Practice (3.2) |\n",
    "|--------------|-------------|-------------|\n",
    "| Tools loaded | ALL (50+) | Filtered (5-10) |\n",
    "| History kept | ALL turns | Last 3 turns |\n",
    "| Schema tokens | ~15,000/turn | ~2,000/turn |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96aba7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ 3.2 [Best Practice] Tool Filtering + History Limit\n",
      "======================================================================\n",
      "\n",
      "üìä Full MCP Server: 53 tools (7,309 tokens)\n",
      "\n",
      "üîÑ Turn 1: List all my storage containers in the 'production-...\n",
      "   üîç Filtered: 2 tools (513 tokens)\n",
      "   ‚úÖ Tokens used: 295 (prompt: 243)\n",
      "\n",
      "üîÑ Turn 2: Now show me all VMs in the same resource group....\n",
      "   üîç Filtered: 0 tools (1 tokens)\n",
      "   ‚úÖ Tokens used: 244 (prompt: 115)\n",
      "\n",
      "üîÑ Turn 3: Finally, get the connection string secret from my ...\n",
      "   üîç Filtered: 0 tools (1 tokens)\n",
      "   ‚úÖ Tokens used: 339 (prompt: 262)\n",
      "\n",
      "======================================================================\n",
      "üéüÔ∏è  Token Usage Summary (Optimized - Filtered Tools + Limited History)\n",
      "======================================================================\n",
      "   Turn 1: 295 tokens (2 tools)\n",
      "   Turn 2: 244 tokens (0 tools)\n",
      "   Turn 3: 339 tokens (0 tools)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL (3 turns): 878 tokens\n",
      "\n",
      "üìä Comparison: Traditional (3.1) vs Optimized (3.2)\n",
      "   Traditional (all tools):    6,747 tokens\n",
      "   Optimized (filtered):       878 tokens\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   üí∞ Token Savings:           87.0%\n"
     ]
    }
   ],
   "source": [
    "# Example 2.4: Optimized Approach - Tool Filtering + History Limit\n",
    "# This demonstrates how to reduce context by filtering tools and limiting history\n",
    "\n",
    "from agent_framework.azure import AzureAIClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "\n",
    "def filter_tools_by_intent(all_tools: list[dict], user_query: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Filter tools based on user intent detected from the query.\n",
    "    \n",
    "    In production, this could use:\n",
    "    - Keyword matching\n",
    "    - Embedding similarity\n",
    "    - LLM-based classification\n",
    "    \"\"\"\n",
    "    query_lower = user_query.lower()\n",
    "    \n",
    "    # Define tool categories and their keywords\n",
    "    tool_categories = {\n",
    "        \"storage\": [\"storage\", \"blob\", \"container\", \"upload\", \"download\", \"file\"],\n",
    "        \"compute\": [\"vm\", \"virtual machine\", \"start\", \"stop\", \"scale\", \"compute\"],\n",
    "        \"security\": [\"keyvault\", \"secret\", \"key\", \"certificate\", \"vault\", \"password\"],\n",
    "        \"database\": [\"cosmos\", \"sql\", \"database\", \"query\", \"table\"],\n",
    "        \"monitoring\": [\"monitor\", \"log\", \"metric\", \"alert\", \"diagnostic\"],\n",
    "    }\n",
    "    \n",
    "    # Detect relevant categories\n",
    "    relevant_categories = set()\n",
    "    for category, keywords in tool_categories.items():\n",
    "        if any(kw in query_lower for kw in keywords):\n",
    "            relevant_categories.add(category)\n",
    "    \n",
    "    # If no category detected, return a minimal default set\n",
    "    if not relevant_categories:\n",
    "        relevant_categories = {\"storage\"}  # Default fallback\n",
    "    \n",
    "    # Filter tools by category\n",
    "    filtered_tools = []\n",
    "    for tool in all_tools:\n",
    "        tool_name = tool.get(\"function\", {}).get(\"name\", \"\")\n",
    "        for category in relevant_categories:\n",
    "            if category in tool_name:\n",
    "                filtered_tools.append(tool)\n",
    "                break\n",
    "    \n",
    "    return filtered_tools\n",
    "\n",
    "\n",
    "def limit_conversation_history(history: list[dict], max_turns: int = 3) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Keep only the last N turns of conversation history.\n",
    "    \n",
    "    A \"turn\" consists of a user message and its corresponding assistant response.\n",
    "    System message is always preserved.\n",
    "    \"\"\"\n",
    "    # Always keep the system message\n",
    "    system_messages = [m for m in history if m.get(\"role\") == \"system\"]\n",
    "    other_messages = [m for m in history if m.get(\"role\") != \"system\"]\n",
    "    \n",
    "    # Calculate messages per turn (user + assistant + potential tool responses)\n",
    "    # Keep roughly 2-3 messages per turn\n",
    "    max_messages = max_turns * 3\n",
    "    \n",
    "    # Keep only the last max_messages\n",
    "    trimmed_messages = other_messages[-max_messages:] if len(other_messages) > max_messages else other_messages\n",
    "    \n",
    "    return system_messages + trimmed_messages\n",
    "\n",
    "\n",
    "def optimized_tool_filtering_example():\n",
    "    \"\"\"\n",
    "    Optimized approach: Filter tools by intent and limit history.\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Loading only relevant tools (5-10 instead of 50+)\n",
    "    - Keeping only last 3 turns of history\n",
    "    - Significant token savings\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ 3.2 [Best Practice] Tool Filtering + History Limit\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get the full tool set from Example 3.1\n",
    "    azure_mcp_tools = simulate_large_mcp_schema()\n",
    "    full_tools_tokens = estimate_tokens(json.dumps(azure_mcp_tools, indent=2))\n",
    "    \n",
    "    print(f\"\\nüìä Full MCP Server: {len(azure_mcp_tools)} tools ({full_tools_tokens:,} tokens)\")\n",
    "    \n",
    "    # Create Azure OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Same 3-turn conversation as Example 3.1\n",
    "    base_system_message = {\"role\": \"system\", \"content\": \"You are an Azure infrastructure assistant. Help users manage their Azure resources.\"}\n",
    "    conversation_history = [base_system_message.copy()]\n",
    "    \n",
    "    user_queries = [\n",
    "        \"List all my storage containers in the 'production-rg' resource group.\",\n",
    "        \"Now show me all VMs in the same resource group.\",\n",
    "        \"Finally, get the connection string secret from my 'prod-keyvault'.\",\n",
    "    ]\n",
    "    \n",
    "    total_tokens_optimized = 0\n",
    "    turn_details = []\n",
    "    \n",
    "    for turn_num, query in enumerate(user_queries, 1):\n",
    "        print(f\"\\nüîÑ Turn {turn_num}: {query[:50]}...\")\n",
    "        \n",
    "        # 1. Filter tools based on user intent\n",
    "        filtered_tools = filter_tools_by_intent(azure_mcp_tools, query)\n",
    "        filtered_tokens = estimate_tokens(json.dumps(filtered_tools, indent=2))\n",
    "        print(f\"   üîç Filtered: {len(filtered_tools)} tools ({filtered_tokens:,} tokens)\")\n",
    "        \n",
    "        # 2. Limit conversation history\n",
    "        limited_history = limit_conversation_history(conversation_history, max_turns=3)\n",
    "        \n",
    "        # Add current user message\n",
    "        limited_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Make API call with FILTERED tools and LIMITED history\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            messages=limited_history,\n",
    "            tools=filtered_tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        usage = response.usage\n",
    "        total_tokens_optimized += usage.total_tokens\n",
    "        \n",
    "        # Update full history for next turn\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        assistant_message = response.choices[0].message\n",
    "        \n",
    "        if assistant_message.tool_calls:\n",
    "            tool_name = assistant_message.tool_calls[0].function.name\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": None, \"tool_calls\": [\n",
    "                {\"id\": f\"call_{turn_num}\", \"type\": \"function\", \"function\": {\"name\": tool_name, \"arguments\": \"{}\"}}\n",
    "            ]})\n",
    "            conversation_history.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": f\"call_{turn_num}\",\n",
    "                \"content\": f\"[Simulated response from {tool_name}]\"\n",
    "            })\n",
    "        else:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message.content or \"\"})\n",
    "        \n",
    "        turn_details.append({\n",
    "            \"turn\": turn_num,\n",
    "            \"tools_used\": len(filtered_tools),\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Tokens used: {usage.total_tokens:,} (prompt: {usage.prompt_tokens:,})\")\n",
    "    \n",
    "    # Summary and comparison\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéüÔ∏è  Token Usage Summary (Optimized - Filtered Tools + Limited History)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for detail in turn_details:\n",
    "        print(f\"   Turn {detail['turn']}: {detail['total_tokens']:,} tokens ({detail['tools_used']} tools)\")\n",
    "    \n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"   TOTAL (3 turns): {total_tokens_optimized:,} tokens\")\n",
    "    \n",
    "    # Compare with traditional approach from Example 3.1\n",
    "    print(f\"\\nüìä Comparison: Traditional (3.1) vs Optimized (3.2)\")\n",
    "    print(f\"   Traditional (all tools):    {total_tokens_schema_bloat:,} tokens\")\n",
    "    print(f\"   Optimized (filtered):       {total_tokens_optimized:,} tokens\")\n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    savings = ((total_tokens_schema_bloat - total_tokens_optimized) / total_tokens_schema_bloat) * 100\n",
    "    print(f\"   üí∞ Token Savings:           {savings:.1f}%\")\n",
    "    \n",
    "    return total_tokens_optimized\n",
    "\n",
    "# Run the example\n",
    "total_tokens_filtered = optimized_tool_filtering_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88222f96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 4: Context Compression\n",
    "\n",
    "This section compares **uncontrolled context growth** (anti-pattern) vs **intelligent context compression** (best practice).\n",
    "\n",
    "### 4.1 [Anti-Pattern] Inappropriate Large Context\n",
    "\n",
    "As conversations grow longer, the context window fills up with **full conversation history**. This leads to:\n",
    "\n",
    "1. **Exponential token growth** - Each turn adds more tokens\n",
    "2. **Irrelevant old context** - Early messages may no longer be relevant\n",
    "3. **Wasted capacity** - Less room for actual work\n",
    "4. **Higher costs** - More tokens = higher API costs\n",
    "\n",
    "**Example of Uncontrolled Growth:**\n",
    "```\n",
    "Turn 1: System (500) + User (100) + Assistant (200) = 800 tokens\n",
    "Turn 2: Previous (800) + User (100) + Assistant (200) = 1,100 tokens\n",
    "Turn 3: Previous (1,100) + User (100) + Assistant (200) = 1,400 tokens\n",
    "...\n",
    "Turn 10: Previous context keeps growing = 5,000+ tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41425394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚ö†Ô∏è  4.1 [Anti-Pattern] Inappropriate Large Context\n",
      "======================================================================\n",
      "\n",
      "üìù Initial System Message: 639 characters\n",
      "   Tokens: ~140\n",
      "\n",
      "üîÑ Turn 1: I need to set up a new web application. What Azure...\n",
      "   üìä Context size: 183 tokens\n",
      "   ‚úÖ API tokens: 470 (prompt: 170)\n",
      "\n",
      "üîÑ Turn 2: Tell me more about the VM sizing options and which...\n",
      "   üìä Context size: 544 tokens\n",
      "   ‚úÖ API tokens: 793 (prompt: 493)\n",
      "\n",
      "üîÑ Turn 3: How do I configure the networking for high availab...\n",
      "   üìä Context size: 915 tokens\n",
      "   ‚úÖ API tokens: 1,111 (prompt: 811)\n",
      "\n",
      "üîÑ Turn 4: What about security best practices for the storage...\n",
      "   üìä Context size: 1,280 tokens\n",
      "   ‚úÖ API tokens: 1,429 (prompt: 1,129)\n",
      "\n",
      "üîÑ Turn 5: Can you explain the monitoring setup in more detai...\n",
      "   üìä Context size: 1,660 tokens\n",
      "   ‚úÖ API tokens: 1,747 (prompt: 1,447)\n",
      "\n",
      "======================================================================\n",
      "üéüÔ∏è  Token Usage Summary (Anti-Pattern - Full History Every Turn)\n",
      "======================================================================\n",
      "\n",
      "   Context Growth Over Time:\n",
      "   Turn 1: 183 tokens \n",
      "   Turn 2: 544 tokens ‚ñà‚ñà\n",
      "   Turn 3: 915 tokens ‚ñà‚ñà‚ñà‚ñà\n",
      "   Turn 4: 1,280 tokens ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Turn 5: 1,660 tokens ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL API tokens (5 turns): 5,550\n",
      "\n",
      "‚ö†Ô∏è  Problem Analysis:\n",
      "   Context growth: 183 ‚Üí 1,660 tokens (807% increase)\n",
      "   Messages in history: 11\n",
      "   Average tokens/turn: 1,110\n"
     ]
    }
   ],
   "source": [
    "# 4.1 [Anti-Pattern]: Inappropriate Large Context - Uncontrolled Growth\n",
    "# This demonstrates how context grows without proper management\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def demonstrate_context_growth():\n",
    "    \"\"\"\n",
    "    Demonstrate the problem of uncontrolled context growth.\n",
    "    \n",
    "    This simulates a multi-turn conversation where:\n",
    "    - Full history is kept for every turn\n",
    "    - Context grows linearly with each turn\n",
    "    - No optimization is applied\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  4.1 [Anti-Pattern] Inappropriate Large Context\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create Azure OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Start with a detailed system message\n",
    "    system_message = \"\"\"You are a comprehensive Azure infrastructure assistant. \n",
    "You help users with all aspects of Azure resource management including:\n",
    "- Virtual Machines (VMs): creation, sizing, networking, disk management\n",
    "- Storage Accounts: blob storage, file shares, queues, tables\n",
    "- Networking: VNets, subnets, NSGs, load balancers, VPN gateways\n",
    "- Databases: Azure SQL, Cosmos DB, PostgreSQL, MySQL\n",
    "- Security: Key Vault, Managed Identities, RBAC, Azure AD\n",
    "- Monitoring: Azure Monitor, Log Analytics, Application Insights\n",
    "- DevOps: Azure DevOps, GitHub Actions, Container Registry, AKS\n",
    "\n",
    "Always provide detailed explanations with examples and best practices.\"\"\"\n",
    "    \n",
    "    conversation_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # Simulate a multi-turn conversation\n",
    "    user_queries = [\n",
    "        \"I need to set up a new web application. What Azure resources should I use?\",\n",
    "        \"Tell me more about the VM sizing options and which one fits my needs.\",\n",
    "        \"How do I configure the networking for high availability?\",\n",
    "        \"What about security best practices for the storage account?\",\n",
    "        \"Can you explain the monitoring setup in more detail?\",\n",
    "    ]\n",
    "    \n",
    "    # Detailed responses to simulate realistic conversation\n",
    "    simulated_responses = [\n",
    "        \"For a new web application, I recommend using Azure App Service for the frontend, Azure SQL Database for persistent storage, Azure Blob Storage for static files, and Azure CDN for content delivery. You should also consider Azure Key Vault for secrets management and Application Insights for monitoring...\",\n",
    "        \"For VM sizing, Azure offers various series: B-series for burstable workloads, D-series for general purpose, E-series for memory-optimized, and F-series for compute-optimized. Based on your web application needs, I'd recommend starting with D2s_v5 (2 vCPU, 8GB RAM) and scaling as needed...\",\n",
    "        \"For high availability networking, you should deploy across multiple Availability Zones, use Azure Load Balancer or Application Gateway for traffic distribution, implement Network Security Groups (NSGs) for access control, and consider Azure Front Door for global load balancing...\",\n",
    "        \"Storage security best practices include: enabling encryption at rest with customer-managed keys, using Private Endpoints instead of public access, implementing Azure AD authentication, configuring CORS policies, enabling soft delete for blob protection, and setting up access policies with SAS tokens...\",\n",
    "        \"For monitoring, set up Application Insights for application-level telemetry, Azure Monitor for infrastructure metrics, Log Analytics workspace for centralized logging, create custom dashboards in Azure Portal, set up alerts for critical metrics, and use Azure Monitor Workbooks for reporting...\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìù Initial System Message: {len(system_message)} characters\")\n",
    "    print(f\"   Tokens: ~{estimate_tokens(system_message):,}\")\n",
    "    \n",
    "    total_tokens_anti_pattern = 0\n",
    "    turn_details = []\n",
    "    \n",
    "    for turn_num, (query, sim_response) in enumerate(zip(user_queries, simulated_responses), 1):\n",
    "        print(f\"\\nüîÑ Turn {turn_num}: {query[:50]}...\")\n",
    "        \n",
    "        # Add user message\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Calculate context size BEFORE API call\n",
    "        context_json = json.dumps(conversation_history)\n",
    "        context_tokens = estimate_tokens(context_json)\n",
    "        \n",
    "        # Make API call with FULL history (anti-pattern)\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            messages=conversation_history,\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        usage = response.usage\n",
    "        total_tokens_anti_pattern += usage.total_tokens\n",
    "        \n",
    "        # Add response to history (simulating realistic response)\n",
    "        assistant_content = response.choices[0].message.content or sim_response\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "        \n",
    "        turn_details.append({\n",
    "            \"turn\": turn_num,\n",
    "            \"context_tokens\": context_tokens,\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "            \"history_length\": len(conversation_history),\n",
    "        })\n",
    "        \n",
    "        print(f\"   üìä Context size: {context_tokens:,} tokens\")\n",
    "        print(f\"   ‚úÖ API tokens: {usage.total_tokens:,} (prompt: {usage.prompt_tokens:,})\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéüÔ∏è  Token Usage Summary (Anti-Pattern - Full History Every Turn)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n   Context Growth Over Time:\")\n",
    "    for detail in turn_details:\n",
    "        bar = \"‚ñà\" * (detail['context_tokens'] // 200)\n",
    "        print(f\"   Turn {detail['turn']}: {detail['context_tokens']:,} tokens {bar}\")\n",
    "    \n",
    "    print(f\"\\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"   TOTAL API tokens (5 turns): {total_tokens_anti_pattern:,}\")\n",
    "    \n",
    "    # Calculate growth rate\n",
    "    first_turn = turn_details[0]['context_tokens']\n",
    "    last_turn = turn_details[-1]['context_tokens']\n",
    "    growth_pct = ((last_turn - first_turn) / first_turn) * 100\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Problem Analysis:\")\n",
    "    print(f\"   Context growth: {first_turn:,} ‚Üí {last_turn:,} tokens ({growth_pct:.0f}% increase)\")\n",
    "    print(f\"   Messages in history: {len(conversation_history)}\")\n",
    "    print(f\"   Average tokens/turn: {total_tokens_anti_pattern // 5:,}\")\n",
    "    \n",
    "    return total_tokens_anti_pattern, turn_details\n",
    "\n",
    "# Run the anti-pattern example\n",
    "total_tokens_anti_pattern, context_growth_details = demonstrate_context_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00da133",
   "metadata": {},
   "source": [
    "### 4.2 [Best Practice] Context Compression\n",
    "\n",
    "Context compression takes optimization further by **summarizing conversation history** before sending to the model. This approach:\n",
    "\n",
    "1. **Compresses past turns** into concise summaries\n",
    "2. **Preserves key information** while reducing tokens\n",
    "3. **Uses LLM** to generate intelligent summaries\n",
    "\n",
    "| Turn | Anti-Pattern (4.1) | Best Practice (4.2) |\n",
    "|------|---------------------|----------------------|\n",
    "| 1 | Full message | Full message |\n",
    "| 2 | Full message | Full message |\n",
    "| 3+ | Full messages (growing) | Compressed summary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d24613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üóúÔ∏è  4.2 [Best Practice] Context Compression\n",
      "======================================================================\n",
      "\n",
      "üîÑ Turn 1: List all my storage containers in 'production-rg'....\n",
      "   ‚úÖ Tokens used: 283 (2 tools)\n",
      "\n",
      "üîÑ Turn 2: Upload a test file to the 'data' container....\n",
      "   ‚úÖ Tokens used: 390 (2 tools)\n",
      "\n",
      "üîÑ Turn 3: Now show me all VMs in the resource group....\n",
      "   üóúÔ∏è  History compressed: 230 ‚Üí 230 tokens (saved 0)\n",
      "   ‚úÖ Tokens used: 290 (0 tools)\n",
      "\n",
      "üîÑ Turn 4: Start the VM named 'web-server-01'....\n",
      "   üóúÔ∏è  History compressed: 370 ‚Üí 383 tokens (saved -13)\n",
      "   ‚úÖ Tokens used: 394 (0 tools)\n",
      "\n",
      "üîÑ Turn 5: Finally, get the database connection string from '...\n",
      "   üóúÔ∏è  History compressed: 464 ‚Üí 423 tokens (saved 41)\n",
      "   ‚úÖ Tokens used: 467 (0 tools)\n",
      "\n",
      "======================================================================\n",
      "üéüÔ∏è  Token Usage Summary (Compressed + Filtered)\n",
      "======================================================================\n",
      "   Turn 1: 283 tokens (2 tools)\n",
      "   Turn 2: 390 tokens (2 tools)\n",
      "   Turn 3: 290 tokens (0 tools)\n",
      "   Turn 4: 394 tokens (0 tools)\n",
      "   Turn 5: 467 tokens (0 tools)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "   TOTAL (5 turns): 1,824 tokens\n",
      "\n",
      "======================================================================\n",
      "üìä Final Comparison: All Optimization Approaches\n",
      "======================================================================\n",
      "   Traditional (3.1, 3 turns):      6,747 tokens\n",
      "   Filtered (3.2, 3 turns):         878 tokens\n",
      "   Compressed (4.2, 5 turns):       1,824 tokens\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "   Per-turn average:\n",
      "   Traditional:    2,249 tokens/turn\n",
      "   Compressed:     365 tokens/turn\n",
      "   üí∞ Savings:     83.8% per turn\n"
     ]
    }
   ],
   "source": [
    "# Example 2.5: Advanced Optimization - Context Compression\n",
    "# This demonstrates compressing conversation history to reduce context size\n",
    "\n",
    "from agent_framework.azure import AzureAIClient\n",
    "from azure.ai.projects.models import CodeInterpreterTool\n",
    "\n",
    "def compress_conversation_history(\n",
    "    client: AzureOpenAI,\n",
    "    history: list[dict],\n",
    "    keep_recent: int = 2\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Compress older conversation history into a summary.\n",
    "    \n",
    "    Args:\n",
    "        client: Azure OpenAI client for generating summaries\n",
    "        history: Full conversation history\n",
    "        keep_recent: Number of recent turns to keep uncompressed\n",
    "    \n",
    "    Returns:\n",
    "        Compressed history with summary of older turns\n",
    "    \"\"\"\n",
    "    # Separate system message\n",
    "    system_messages = [m for m in history if m.get(\"role\") == \"system\"]\n",
    "    other_messages = [m for m in history if m.get(\"role\") != \"system\"]\n",
    "    \n",
    "    # If history is short enough, no compression needed\n",
    "    messages_per_turn = 3  # user + assistant + tool response\n",
    "    if len(other_messages) <= keep_recent * messages_per_turn:\n",
    "        return history\n",
    "    \n",
    "    # Split into old (to compress) and recent (to keep)\n",
    "    split_point = len(other_messages) - (keep_recent * messages_per_turn)\n",
    "    old_messages = other_messages[:split_point]\n",
    "    recent_messages = other_messages[split_point:]\n",
    "    \n",
    "    # Generate summary of old messages\n",
    "    old_text = \"\"\n",
    "    for msg in old_messages:\n",
    "        role = msg.get(\"role\", \"unknown\")\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        if content:\n",
    "            old_text += f\"{role}: {content}\\n\"\n",
    "    \n",
    "    # Use LLM to compress\n",
    "    compression_response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following conversation in 2-3 concise sentences, preserving key facts and context.\"},\n",
    "            {\"role\": \"user\", \"content\": old_text}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    \n",
    "    summary = compression_response.choices[0].message.content\n",
    "    \n",
    "    # Create compressed history\n",
    "    compressed_history = system_messages.copy()\n",
    "    compressed_history.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"[Previous conversation summary: {summary}]\"\n",
    "    })\n",
    "    compressed_history.extend(recent_messages)\n",
    "    \n",
    "    return compressed_history\n",
    "\n",
    "\n",
    "def context_compression_example():\n",
    "    \"\"\"\n",
    "    Advanced optimization: Compress conversation history.\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Summarizing older conversation turns\n",
    "    - Keeping recent context intact\n",
    "    - Combining with tool filtering for maximum savings\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üóúÔ∏è  4.2 [Best Practice] Context Compression\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get filtered tools (from 3.2 approach)\n",
    "    azure_mcp_tools = simulate_large_mcp_schema()\n",
    "    \n",
    "    # Create Azure OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Simulate a longer 5-turn conversation\n",
    "    base_system_message = {\"role\": \"system\", \"content\": \"You are an Azure infrastructure assistant.\"}\n",
    "    conversation_history = [base_system_message.copy()]\n",
    "    \n",
    "    user_queries = [\n",
    "        \"List all my storage containers in 'production-rg'.\",\n",
    "        \"Upload a test file to the 'data' container.\",\n",
    "        \"Now show me all VMs in the resource group.\",\n",
    "        \"Start the VM named 'web-server-01'.\",\n",
    "        \"Finally, get the database connection string from 'prod-keyvault'.\",\n",
    "    ]\n",
    "    \n",
    "    total_tokens_compressed = 0\n",
    "    total_compression_tokens = 0\n",
    "    turn_details = []\n",
    "    \n",
    "    for turn_num, query in enumerate(user_queries, 1):\n",
    "        print(f\"\\nüîÑ Turn {turn_num}: {query[:50]}...\")\n",
    "        \n",
    "        # 1. Filter tools\n",
    "        filtered_tools = filter_tools_by_intent(azure_mcp_tools, query)\n",
    "        \n",
    "        # 2. Compress history (keep last 2 turns uncompressed)\n",
    "        if turn_num > 2:\n",
    "            original_history_tokens = estimate_tokens(json.dumps(conversation_history))\n",
    "            compressed_history = compress_conversation_history(client, conversation_history, keep_recent=2)\n",
    "            compressed_history_tokens = estimate_tokens(json.dumps(compressed_history))\n",
    "            compression_savings = original_history_tokens - compressed_history_tokens\n",
    "            print(f\"   üóúÔ∏è  History compressed: {original_history_tokens:,} ‚Üí {compressed_history_tokens:,} tokens (saved {compression_savings:,})\")\n",
    "        else:\n",
    "            compressed_history = conversation_history.copy()\n",
    "        \n",
    "        # Add current query\n",
    "        compressed_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Make API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            messages=compressed_history,\n",
    "            tools=filtered_tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        usage = response.usage\n",
    "        total_tokens_compressed += usage.total_tokens\n",
    "        \n",
    "        # Update full history (uncompressed, for reference)\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        assistant_message = response.choices[0].message\n",
    "        \n",
    "        if assistant_message.tool_calls:\n",
    "            tool_name = assistant_message.tool_calls[0].function.name\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": None, \"tool_calls\": [\n",
    "                {\"id\": f\"call_{turn_num}\", \"type\": \"function\", \"function\": {\"name\": tool_name, \"arguments\": \"{}\"}}\n",
    "            ]})\n",
    "            conversation_history.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": f\"call_{turn_num}\",\n",
    "                \"content\": f\"[Simulated response from {tool_name}]\"\n",
    "            })\n",
    "        else:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message.content or \"\"})\n",
    "        \n",
    "        turn_details.append({\n",
    "            \"turn\": turn_num,\n",
    "            \"tools_used\": len(filtered_tools),\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Tokens used: {usage.total_tokens:,} ({len(filtered_tools)} tools)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéüÔ∏è  Token Usage Summary (Compressed + Filtered)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for detail in turn_details:\n",
    "        print(f\"   Turn {detail['turn']}: {detail['total_tokens']:,} tokens ({detail['tools_used']} tools)\")\n",
    "    \n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"   TOTAL (5 turns): {total_tokens_compressed:,} tokens\")\n",
    "    \n",
    "    # Final comparison\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä Final Comparison: All Optimization Approaches\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Estimate what traditional would cost for 5 turns\n",
    "    estimated_traditional_5turns = int(total_tokens_schema_bloat * 5 / 3)  # Extrapolate from 3-turn data\n",
    "    \n",
    "    print(f\"   Traditional (3.1, 3 turns):      {total_tokens_schema_bloat:,} tokens\")\n",
    "    print(f\"   Filtered (3.2, 3 turns):         {total_tokens_filtered:,} tokens\")\n",
    "    print(f\"   Compressed (4.2, 5 turns):       {total_tokens_compressed:,} tokens\")\n",
    "    print(f\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    \n",
    "    # Calculate savings vs traditional (normalized per turn)\n",
    "    trad_per_turn = total_tokens_schema_bloat / 3\n",
    "    compressed_per_turn = total_tokens_compressed / 5\n",
    "    savings_vs_trad = ((trad_per_turn - compressed_per_turn) / trad_per_turn) * 100\n",
    "    \n",
    "    print(f\"\\n   Per-turn average:\")\n",
    "    print(f\"   Traditional:    {trad_per_turn:,.0f} tokens/turn\")\n",
    "    print(f\"   Compressed:     {compressed_per_turn:,.0f} tokens/turn\")\n",
    "    print(f\"   üí∞ Savings:     {savings_vs_trad:.1f}% per turn\")\n",
    "    \n",
    "    return total_tokens_compressed\n",
    "\n",
    "# Run the example\n",
    "total_tokens_compressed = context_compression_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667810e",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### When to Use Each Optimization\n",
    "\n",
    "| Optimization | Use When | Avoid When |\n",
    "|--------------|----------|------------|\n",
    "| **Code Execution** | Large datasets (>1000 rows), multi-step workflows | Simple queries, small data |\n",
    "| **Tool Filtering** | Many tools available (10+), varied user intents | Few tools, consistent tool usage |\n",
    "| **History Limiting** | Long conversations (5+ turns), repetitive queries | Context-dependent tasks |\n",
    "| **Context Compression** | Very long sessions, returning users | Short interactions |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57195d4",
   "metadata": {},
   "source": [
    "### Security Considerations for Code Execution\n",
    "\n",
    "When implementing code execution with MCP, pay careful attention to security:\n",
    "\n",
    "| Security Layer | Implementation |\n",
    "|---------------|----------------|\n",
    "| **Sandboxing** | Run code in isolated containers (e.g., Docker with limited permissions) |\n",
    "| **Resource Limits** | Set memory, CPU, time, and disk limits |\n",
    "| **Code Validation** | Check for dangerous patterns before execution |\n",
    "| **Network Isolation** | Disable or restrict network access in sandbox |\n",
    "| **Monitoring** | Log all executions and alert on suspicious patterns |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa84600",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "This notebook demonstrated **Anti-Pattern vs Best Practice** pairs for context optimization:\n",
    "\n",
    "| Section | Anti-Pattern | Best Practice | Token Savings |\n",
    "|---------|--------------|---------------|---------------|\n",
    "| **1. Single Code Execution** | Full data in context | Data in sandbox | ~90% |\n",
    "| **2. Multi-Step Workflows** | 4 round trips | Single pipeline | ~95% |\n",
    "| **3. MCP Optimization** | All tools loaded | Filtered by intent | ~60-70% |\n",
    "| **4. Context Compression** | Full history | Compressed summary | ~70-80% |\n",
    "\n",
    "### Examples Reference\n",
    "\n",
    "| Section | Example | Approach | Key Technique |\n",
    "|---------|---------|----------|---------------|\n",
    "| Part 1 | 1.1-1.2 | Warming Up | MCP connection basics |\n",
    "| Part 1 | 1.3 | Warming Up | Context window understanding |\n",
    "| Part 2.1 | 1.1 [Anti] | Traditional | Full data in context |\n",
    "| Part 2.1 | 1.2 [Best] | Code Exec | Data generation in sandbox |\n",
    "| Part 2.2 | 2.1 [Anti] | Traditional | 4 round trips |\n",
    "| Part 2.2 | 2.2 [Best] | Code Exec | Single pipeline execution |\n",
    "| Part 2.3 | 3.1 [Anti] | Traditional | MCP schema bloat |\n",
    "| Part 2.3 | 3.2 [Best] | Optimized | Tool filtering + history limit |\n",
    "| Part 2.4 | 4.1 [Anti] | Traditional | Uncontrolled context growth |\n",
    "| Part 2.4 | 4.2 [Best] | Optimized | Context compression |\n",
    "\n",
    "### Summary: Context Optimization Strategies\n",
    "\n",
    "1. **Code Execution**: Process data in sandbox, return only summaries\n",
    "2. **Tool Filtering**: Load only relevant tools based on user intent\n",
    "3. **History Limiting**: Keep only recent N turns of conversation\n",
    "4. **Context Compression**: Summarize older conversations with LLM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5a73d",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Model Context Protocol Documentation](https://modelcontextprotocol.io/)\n",
    "- [Microsoft Agent Framework - Using MCP Tools](https://learn.microsoft.com/en-us/agent-framework/user-guide/model-context-protocol/using-mcp-tools)\n",
    "- [Code Execution with MCP - Anthropic Engineering](https://www.anthropic.com/engineering/code-execution-with-mcp)\n",
    "- [MCP GitHub Repository](https://github.com/modelcontextprotocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8a421",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
