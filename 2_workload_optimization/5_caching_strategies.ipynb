{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091207ea",
   "metadata": {},
   "source": [
    "# Optimization: Caching Strategies\n",
    "\n",
    "----\n",
    "\n",
    "This notebook focuses on **caching strategies** to reduce latency, cost, and redundant API calls for AI workloads.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "- **Prompt Caching**: Azure OpenAI native caching for repeated prompt prefixes\n",
    "- **Response Caching**: Cache exact responses with Redis\n",
    "- **Semantic Caching**: Cache similar queries using vector similarity\n",
    "- **Cost Analysis**: Compare cached vs non-cached costs\n",
    "\n",
    "**Reference**: [Azure/agent-innovator-lab - Caching](https://github.com/Azure/agent-innovator-lab/tree/main/3_optimization-design-ptn/02_caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19f0c5",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Why Caching Matters](#why-caching-matters)\n",
    "- [Pre-requisites](#pre-requisites)\n",
    "- [Setup](#setup)\n",
    "- [Part 1: Azure OpenAI Prompt Caching](#part-1-azure-openai-prompt-caching)\n",
    "- [Part 2: Redis Response Caching](#part-2-redis-response-caching)\n",
    "- [Part 3: Semantic Caching](#part-3-semantic-caching)\n",
    "- [Part 4: Caching Strategy Comparison](#part-4-caching-strategy-comparison)\n",
    "- [Best Practices Summary](#best-practices-summary)\n",
    "\n",
    "- [Cleanup Resources](#cleanup-resources)- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423404e",
   "metadata": {},
   "source": [
    "## Why Caching Matters\n",
    "\n",
    "### The Problem: Repetitive Costs\n",
    "\n",
    "| Scenario | Without Caching | With Caching |\n",
    "|----------|----------------|---------------|\n",
    "| Same system prompt (10K tokens) sent 1000√ó | 10M input tokens | 10K tokens + cache reads |\n",
    "| FAQ bot answering \"What are your hours?\" 500√ó | 500 API calls | 1 API call + 499 cache hits |\n",
    "| RAG with similar queries | Full retrieval each time | Cached retrievals |\n",
    "\n",
    "### Caching Types Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        Caching Strategies                           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   Prompt Caching    ‚îÇ  Response Caching   ‚îÇ   Semantic Caching      ‚îÇ\n",
    "‚îÇ   (Azure OpenAI)    ‚îÇ  (Redis Exact)      ‚îÇ   (Vector Similarity)   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Cache prompt prefix ‚îÇ Cache exact Q&A     ‚îÇ Cache similar queries   ‚îÇ\n",
    "‚îÇ 50% cost reduction  ‚îÇ ~0 latency on hit   ‚îÇ Fuzzy matching          ‚îÇ\n",
    "‚îÇ Automatic           ‚îÇ Manual setup        ‚îÇ Embedding required      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Cost Impact\n",
    "\n",
    "| Model | Standard Input | Cached Input | Savings |\n",
    "|-------|---------------|--------------|----------|\n",
    "| GPT-4o | $2.50/1M tokens | $1.25/1M tokens | **50%** |\n",
    "| GPT-4o-mini | $0.15/1M tokens | $0.075/1M tokens | **50%** |\n",
    "| GPT-4.1 | $2.00/1M tokens | $0.50/1M tokens | **75%** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea987e",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **Azure account** with an active subscription\n",
    "2. **Azure OpenAI resource** with GPT-4x and embedding model deployments\n",
    "3. **Azure Cache for Redis Enterprise** with **RediSearch** module enabled (for Parts 2-3)\n",
    "\n",
    "### Creating Azure Redis Enterprise with RediSearch\n",
    "\n",
    "The following cell creates an **Azure Managed Redis (Enterprise)** instance with the **RediSearch** module enabled, which is required for semantic caching.\n",
    "\n",
    "> **Note**: Azure Managed Redis Enterprise provisioning can take 15-30 minutes. You can skip this if you already have a Redis instance with RediSearch enabled.\n",
    "\n",
    "**Pricing**: Enterprise tier is required for RediSearch module. See [Azure Redis pricing](https://azure.microsoft.com/pricing/details/cache/) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6c8217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Azure Redis Enterprise Configuration\n",
      "============================================================\n",
      "Resource Group: foundry-rg\n",
      "Redis Name: redis-cache-swed\n",
      "Location: swedencentral\n",
      "SKU: Enterprise_E1\n",
      "CREATE_REDIS: False\n",
      "============================================================\n",
      "\n",
      "‚ÑπÔ∏è Set CREATE_REDIS=True to create Azure Redis Enterprise.\n",
      "   Or set REDIS_ENDPOINT and REDIS_PASSWORD in .env for existing instance.\n"
     ]
    }
   ],
   "source": [
    "# Create Azure Managed Redis Enterprise with RediSearch\n",
    "# ======================================================\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Configuration\n",
    "CREATE_REDIS = False  # Set to True to create Redis instance\n",
    "\n",
    "# Load config from Foundry setup\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'. Run 0_setup/1_setup.ipynb first.\")\n",
    "    config = {}\n",
    "\n",
    "# Redis configuration\n",
    "REDIS_RESOURCE_GROUP = config.get('RESOURCE_GROUP', os.environ.get('RESOURCE_GROUP', 'rg-caching-lab'))\n",
    "REDIS_LOCATION = config.get('LOCATION', os.environ.get('LOCATION', 'eastus'))\n",
    "REDIS_NAME = os.environ.get('REDIS_NAME', f\"redis-cache-{REDIS_LOCATION[:4]}\")\n",
    "AZURE_SUBSCRIPTION_ID = config.get('AZURE_SUBSCRIPTION_ID', os.environ.get('AZURE_SUBSCRIPTION_ID', ''))\n",
    "\n",
    "# Redis Enterprise SKU options:\n",
    "# - Enterprise_E1 (1GB, lowest cost for dev/test)\n",
    "# - Enterprise_E10 (10GB)\n",
    "# - Enterprise_E20 (20GB)\n",
    "# - Enterprise_E50 (50GB)\n",
    "# - Enterprise_E100 (100GB)\n",
    "REDIS_SKU = os.environ.get('REDIS_SKU', 'Enterprise_E1')\n",
    "REDIS_CAPACITY = 2  # Number of nodes (minimum 2 for Enterprise)\n",
    "\n",
    "def run_az(args: list) -> str:\n",
    "    \"\"\"Run Azure CLI command and return stdout.\"\"\"\n",
    "    cmd = [\"az\"] + args\n",
    "    print(f\"  $ az {' '.join(args[:8])}{'...' if len(args) > 8 else ''}\")\n",
    "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError((p.stderr or p.stdout).strip())\n",
    "    return p.stdout.strip()\n",
    "\n",
    "def maybe_set_subscription():\n",
    "    if AZURE_SUBSCRIPTION_ID:\n",
    "        run_az([\"account\", \"set\", \"--subscription\", AZURE_SUBSCRIPTION_ID])\n",
    "\n",
    "def ensure_resource_group():\n",
    "    exists = run_az([\"group\", \"exists\", \"-n\", REDIS_RESOURCE_GROUP]).strip().lower() == \"true\"\n",
    "    if not exists:\n",
    "        print(f\"üÜï Creating resource group: {REDIS_RESOURCE_GROUP}\")\n",
    "        run_az([\"group\", \"create\", \"-n\", REDIS_RESOURCE_GROUP, \"-l\", REDIS_LOCATION])\n",
    "    else:\n",
    "        print(f\"‚úÖ Resource group exists: {REDIS_RESOURCE_GROUP}\")\n",
    "\n",
    "def redis_exists() -> bool:\n",
    "    try:\n",
    "        run_az([\"redisenterprise\", \"show\", \"-g\", REDIS_RESOURCE_GROUP, \"-n\", REDIS_NAME, \"-o\", \"none\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def database_exists() -> bool:\n",
    "    \"\"\"Check if the default database exists on the Redis cluster.\"\"\"\n",
    "    try:\n",
    "        run_az([\n",
    "            \"redisenterprise\", \"database\", \"show\",\n",
    "            \"-g\", REDIS_RESOURCE_GROUP,\n",
    "            \"--cluster-name\", REDIS_NAME,\n",
    "            \"-o\", \"none\"\n",
    "        ])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ensure_database():\n",
    "    \"\"\"Create the database with RediSearch if it doesn't exist.\"\"\"\n",
    "    import time\n",
    "    if database_exists():\n",
    "        print(\"   ‚úÖ Database already exists.\")\n",
    "        return\n",
    "\n",
    "    # Delete any leftover database with wrong config (just in case)\n",
    "    print(\"\\nüì¶ Creating Redis database with RediSearch module...\")\n",
    "    run_az([\n",
    "        \"redisenterprise\", \"database\", \"create\",\n",
    "        \"-g\", REDIS_RESOURCE_GROUP,\n",
    "        \"--cluster-name\", REDIS_NAME,\n",
    "        \"--client-protocol\", \"Encrypted\",\n",
    "        \"--clustering-policy\", \"EnterpriseCluster\",\n",
    "        \"--eviction-policy\", \"NoEviction\",\n",
    "        \"--modules\", \"name=RediSearch\",\n",
    "    ])\n",
    "    print(\"   ‚úÖ Database created successfully!\")\n",
    "\n",
    "def create_redis_enterprise():\n",
    "    \"\"\"Create Azure Managed Redis Enterprise with RediSearch module.\"\"\"\n",
    "    print(f\"\\nüöÄ Creating Azure Managed Redis Enterprise: {REDIS_NAME}\")\n",
    "    print(f\"   Location: {REDIS_LOCATION}\")\n",
    "    print(f\"   SKU: {REDIS_SKU}\")\n",
    "    print(f\"   Modules: RediSearch (required for semantic caching)\")\n",
    "    print(\"\\n‚è≥ This may take 15-30 minutes...\")\n",
    "    \n",
    "    # Create Redis Enterprise cluster\n",
    "    run_az([\n",
    "        \"redisenterprise\", \"create\",\n",
    "        \"-g\", REDIS_RESOURCE_GROUP,\n",
    "        \"-n\", REDIS_NAME,\n",
    "        \"-l\", REDIS_LOCATION,\n",
    "        \"--sku\", REDIS_SKU,\n",
    "        \"--capacity\", str(REDIS_CAPACITY),\n",
    "        \"--no-wait\",\n",
    "        \"--public-network-access\", \"Enabled\",\n",
    "        \"--access-keys-auth\", \"Enabled\"\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüìù Redis cluster creation started. Creating database with RediSearch...\")\n",
    "    \n",
    "    # Wait for cluster to be ready before creating database\n",
    "    import time\n",
    "    max_wait = 1800  # 30 minutes\n",
    "    poll_interval = 30\n",
    "    waited = 0\n",
    "    \n",
    "    while waited < max_wait:\n",
    "        try:\n",
    "            state = run_az([\n",
    "                \"redisenterprise\", \"show\",\n",
    "                \"-g\", REDIS_RESOURCE_GROUP,\n",
    "                \"-n\", REDIS_NAME,\n",
    "                \"--query\", \"provisioningState\",\n",
    "                \"-o\", \"tsv\"\n",
    "            ]).strip()\n",
    "            print(f\"   Provisioning state: {state}\")\n",
    "            if state == \"Succeeded\":\n",
    "                break\n",
    "            elif state in [\"Failed\", \"Canceled\"]:\n",
    "                raise RuntimeError(f\"Redis provisioning failed: {state}\")\n",
    "        except RuntimeError as e:\n",
    "            if \"ResourceNotFound\" not in str(e):\n",
    "                print(f\"   Waiting for cluster... ({waited}s)\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "        waited += poll_interval\n",
    "    \n",
    "    if waited >= max_wait:\n",
    "        print(\"‚ö†Ô∏è Timeout waiting for Redis cluster. Check Azure portal for status.\")\n",
    "        return\n",
    "    \n",
    "    # Delete existing default database if present (may have wrong clustering policy)\n",
    "    print(\"\\nüóëÔ∏è Removing existing default database (if any)...\")\n",
    "    try:\n",
    "        run_az([\n",
    "            \"redisenterprise\", \"database\", \"delete\",\n",
    "            \"-g\", REDIS_RESOURCE_GROUP,\n",
    "            \"--cluster-name\", REDIS_NAME,\n",
    "            \"--yes\",\n",
    "        ])\n",
    "        print(\"   Deleted existing database.\")\n",
    "        time.sleep(10)  # Wait for deletion to propagate\n",
    "    except RuntimeError:\n",
    "        print(\"   No existing database to delete.\")\n",
    "\n",
    "    # Create database with RediSearch module\n",
    "    print(\"\\nüì¶ Creating Redis database with RediSearch module...\")\n",
    "    run_az([\n",
    "        \"redisenterprise\", \"database\", \"create\",\n",
    "        \"-g\", REDIS_RESOURCE_GROUP,\n",
    "        \"--cluster-name\", REDIS_NAME,\n",
    "        \"--client-protocol\", \"Encrypted\",\n",
    "        \"--clustering-policy\", \"EnterpriseCluster\",\n",
    "        \"--eviction-policy\", \"NoEviction\",\n",
    "        \"--modules\", \"name=RediSearch\",\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n‚úÖ Redis Enterprise with RediSearch created successfully!\")\n",
    "\n",
    "def get_redis_connection_info() -> dict:\n",
    "    \"\"\"Get Redis endpoint and access key.\"\"\"\n",
    "    try:\n",
    "        # Get endpoint\n",
    "        endpoint = run_az([\n",
    "            \"redisenterprise\", \"database\", \"show\",\n",
    "            \"-g\", REDIS_RESOURCE_GROUP,\n",
    "            \"--cluster-name\", REDIS_NAME,\n",
    "            \"--query\", \"[resourceState, port]\",\n",
    "            \"-o\", \"tsv\"\n",
    "        ])\n",
    "        \n",
    "        # Get hostname from cluster\n",
    "        hostname = run_az([\n",
    "            \"redisenterprise\", \"show\",\n",
    "            \"-g\", REDIS_RESOURCE_GROUP,\n",
    "            \"-n\", REDIS_NAME,\n",
    "            \"--query\", \"hostName\",\n",
    "            \"-o\", \"tsv\"\n",
    "        ]).strip()\n",
    "        \n",
    "        # Get access key\n",
    "        keys = run_az([\n",
    "            \"redisenterprise\", \"database\", \"list-keys\",\n",
    "            \"-g\", REDIS_RESOURCE_GROUP,\n",
    "            \"--cluster-name\", REDIS_NAME,\n",
    "            \"-o\", \"tsv\"\n",
    "        ]).strip()\n",
    "        \n",
    "        return {\n",
    "            \"endpoint\": f\"{hostname}:10000\",\n",
    "            \"password\": keys,\n",
    "            \"url\": f\"rediss://:{keys}@{hostname}:10000\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not retrieve Redis connection info: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Main execution\n",
    "print(\"üîß Azure Redis Enterprise Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Resource Group: {REDIS_RESOURCE_GROUP}\")\n",
    "print(f\"Redis Name: {REDIS_NAME}\")\n",
    "print(f\"Location: {REDIS_LOCATION}\")\n",
    "print(f\"SKU: {REDIS_SKU}\")\n",
    "print(f\"CREATE_REDIS: {CREATE_REDIS}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if CREATE_REDIS:\n",
    "    maybe_set_subscription()\n",
    "    ensure_resource_group()\n",
    "    \n",
    "    if redis_exists():\n",
    "        print(f\"\\n‚úÖ Redis Enterprise cluster exists: {REDIS_NAME}\")\n",
    "        ensure_database()\n",
    "    else:\n",
    "        create_redis_enterprise()\n",
    "    \n",
    "    # Get connection info\n",
    "    print(\"\\nüîë Retrieving connection information...\")\n",
    "    conn_info = get_redis_connection_info()\n",
    "    if conn_info:\n",
    "        print(f\"\\nüìå Redis Endpoint: {conn_info.get('endpoint', 'N/A')}\")\n",
    "        print(f\"üìå Redis URL: rediss://:<key>@{conn_info.get('endpoint', 'N/A')}\")\n",
    "        print(\"\\nüí° Add to your .env file:\")\n",
    "        print(f'   REDIS_ENDPOINT=\"{conn_info.get(\"endpoint\", \"\")}\"')\n",
    "        print('   REDIS_PASSWORD=\"Check REDIS_PASSWORD environment variable...\"')\n",
    "        \n",
    "        # Optionally set environment variables\n",
    "        os.environ['REDIS_ENDPOINT'] = conn_info.get('endpoint', '')\n",
    "        os.environ['REDIS_PASSWORD'] = conn_info.get('password', '')\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è Set CREATE_REDIS=True to create Azure Redis Enterprise.\")\n",
    "    print(\"   Or set REDIS_ENDPOINT and REDIS_PASSWORD in .env for existing instance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493c14a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89a098a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Azure CLI found: /anaconda/envs/azureml_py38/bin//az\n",
      "‚úÖ PATH looks good already\n",
      "\n",
      "PATH (first 150 chars): /anaconda/envs/azureml_py38/bin/:/afh/code/agent-operator-lab/.venv/bin:/home/azureuser/.vscode-server/cli/servers/Stable-c9d77990917f3102ada88be140d2...\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import asyncio\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'üîç Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"‚úÖ Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('‚úÖ PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95faacf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded settings from '../0_setup/.foundry_config.json'.\n",
      "\n",
      "üìå Foundry name: foundry-rq90gs\n",
      "üìå Resource group: foundry-rg\n",
      "üìå Location: swedencentral\n",
      "üìå Azure OpenAI endpoint: https://foundry-rq90gs.openai.azure.com\n",
      "üìå Chat deployment: gpt-4.1\n",
      "üìå Embedding deployment: text-embedding-3-large\n",
      "üìå Redis configured: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'.\")\n",
    "    print('üí° Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "# Project variables from config\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "\n",
    "# Azure OpenAI variables from env\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-3-large\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "# Redis configuration (optional)\n",
    "REDIS_ENDPOINT = os.environ.get(\"REDIS_ENDPOINT\", \"\")\n",
    "REDIS_PASSWORD = os.environ.get(\"REDIS_PASSWORD\", \"\")\n",
    "REDIS_URL = f\"rediss://:{REDIS_PASSWORD}@{REDIS_ENDPOINT}\" if REDIS_ENDPOINT else \"\"\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"‚úÖ Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nüìå Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"üìå Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"üìå Location: {LOCATION}\")\n",
    "print(f\"üìå Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"üìå Chat deployment: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")\n",
    "print(f\"üìå Embedding deployment: {AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME}\")\n",
    "print(f\"üìå Redis configured: {'‚úÖ' if REDIS_URL else '‚ùå (Parts 2-3 will use in-memory cache)'}\")\n",
    "\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7779e93",
   "metadata": {},
   "source": [
    "## Part 1: Azure OpenAI Prompt Caching\n",
    "\n",
    "Azure OpenAI automatically caches prompt prefixes to reduce costs on repeated requests with identical beginnings.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Request 1:  [System Prompt: 1000 tokens] + [User: 50 tokens] ‚Üí Full processing\n",
    "Request 2:  [System Prompt: 1000 tokens] + [User: 60 tokens] ‚Üí Cached prefix + new tokens\n",
    "Request 3:  [System Prompt: 1000 tokens] + [User: 45 tokens] ‚Üí Cached prefix + new tokens\n",
    "```\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Minimum **1,024 tokens** in the cacheable prefix\n",
    "- Prefix must be **identical** across requests\n",
    "- Supported models: GPT-4o, GPT-4o-mini, GPT-4.1 series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9515dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Azure OpenAI Prompt Caching Demo\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Request 1: \"How do I set up a Python virtual environ...\"\n",
      "   ‚è±Ô∏è  Latency: 1257ms\n",
      "   üì• Input tokens: 1,292\n",
      "   üì§ Output tokens: 100\n",
      "   üíæ Cached tokens: 0\n",
      "   ‚ùå CACHE MISS\n",
      "\n",
      "üìù Request 2: \"What's the best way to handle exceptions...\"\n",
      "   ‚è±Ô∏è  Latency: 1228ms\n",
      "   üì• Input tokens: 1,292\n",
      "   üì§ Output tokens: 100\n",
      "   üíæ Cached tokens: 0\n",
      "   ‚ùå CACHE MISS\n",
      "\n",
      "üìù Request 3: \"Explain Python decorators briefly....\"\n",
      "   ‚è±Ô∏è  Latency: 1195ms\n",
      "   üì• Input tokens: 1,287\n",
      "   üì§ Output tokens: 100\n",
      "   üíæ Cached tokens: 1,152\n",
      "   ‚úÖ CACHE HIT\n",
      "\n",
      "üìà Summary:\n",
      "   Total input tokens: 3,871\n",
      "   Total cached tokens: 1,152\n",
      "   Cache rate: 29.8%\n",
      "   üí∞ Estimated savings: 14.9% cost reduction\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI Prompt Caching demonstration\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create a long system prompt (must be >1024 tokens for caching)\n",
    "LARGE_SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly specialized AI assistant for enterprise software development.\n",
    "\n",
    "## Your Expertise Areas\n",
    "1. Cloud Architecture: AWS, Azure, GCP best practices\n",
    "2. Programming Languages: Python, Java, TypeScript, Go, Rust\n",
    "3. DevOps: CI/CD, Kubernetes, Docker, Terraform\n",
    "4. Security: OWASP, Zero Trust, IAM best practices\n",
    "5. Databases: SQL, NoSQL, Graph databases, Time-series DBs\n",
    "\n",
    "## Response Guidelines\n",
    "- Always provide code examples when relevant\n",
    "- Include security considerations in all recommendations\n",
    "- Suggest monitoring and observability approaches\n",
    "- Consider scalability and cost implications\n",
    "- Reference official documentation when possible\n",
    "\n",
    "## Code Quality Standards\n",
    "- Follow SOLID principles\n",
    "- Include comprehensive error handling\n",
    "- Add meaningful comments and documentation\n",
    "- Consider edge cases and failure modes\n",
    "- Suggest appropriate testing strategies\n",
    "\n",
    "## Architecture Principles\n",
    "- Design for resilience and fault tolerance\n",
    "- Implement proper caching strategies\n",
    "- Use asynchronous processing where appropriate\n",
    "- Consider data consistency requirements\n",
    "- Plan for horizontal scalability\n",
    "\n",
    "## Security Requirements\n",
    "- Never expose sensitive credentials in code\n",
    "- Implement proper authentication and authorization\n",
    "- Use encryption for data at rest and in transit\n",
    "- Follow least privilege access principles\n",
    "- Include input validation and sanitization\n",
    "\n",
    "[Additional context padding to ensure >1024 tokens for cache eligibility]\n",
    "\"\"\" + (\"This is additional context. \" * 200)  # Pad to ensure >1024 tokens\n",
    "\n",
    "@dataclass\n",
    "class CacheResult:\n",
    "    \"\"\"Result of a cached API call.\"\"\"\n",
    "    response: str\n",
    "    latency_ms: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    cached_tokens: int\n",
    "    cache_hit: bool\n",
    "\n",
    "def call_with_cache_tracking(\n",
    "    client: AzureOpenAI,\n",
    "    deployment: str,\n",
    "    system_prompt: str,\n",
    "    user_message: str\n",
    ") -> CacheResult:\n",
    "    \"\"\"\n",
    "    Make an API call and track caching metrics.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_completion_tokens=100,\n",
    "    )\n",
    "    \n",
    "    elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    usage = response.usage\n",
    "    \n",
    "    # Extract cached_tokens from prompt_tokens_details (Pydantic object)\n",
    "    cached_tokens = 0\n",
    "    if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details is not None:\n",
    "        cached_tokens = getattr(usage.prompt_tokens_details, 'cached_tokens', 0) or 0\n",
    "    \n",
    "    return CacheResult(\n",
    "        response=response.choices[0].message.content,\n",
    "        latency_ms=elapsed_ms,\n",
    "        input_tokens=usage.prompt_tokens,\n",
    "        output_tokens=usage.completion_tokens,\n",
    "        cached_tokens=cached_tokens,\n",
    "        cache_hit=cached_tokens > 0\n",
    "    )\n",
    "\n",
    "# Demonstrate prompt caching\n",
    "print(\"üìä Azure OpenAI Prompt Caching Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Series of requests with same system prompt\n",
    "    user_queries = [\n",
    "        \"How do I set up a Python virtual environment?\",\n",
    "        \"What's the best way to handle exceptions in Python?\",\n",
    "        \"Explain Python decorators briefly.\",\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, query in enumerate(user_queries, 1):\n",
    "        result = call_with_cache_tracking(\n",
    "            client,\n",
    "            AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "            LARGE_SYSTEM_PROMPT,\n",
    "            query\n",
    "        )\n",
    "        results.append(result)\n",
    "        \n",
    "        cache_status = \"‚úÖ CACHE HIT\" if result.cache_hit else \"‚ùå CACHE MISS\"\n",
    "        print(f\"\\nüìù Request {i}: \\\"{query[:40]}...\\\"\")\n",
    "        print(f\"   ‚è±Ô∏è  Latency: {result.latency_ms:.0f}ms\")\n",
    "        print(f\"   üì• Input tokens: {result.input_tokens:,}\")\n",
    "        print(f\"   üì§ Output tokens: {result.output_tokens}\")\n",
    "        print(f\"   üíæ Cached tokens: {result.cached_tokens:,}\")\n",
    "        print(f\"   {cache_status}\")\n",
    "    \n",
    "    # Summary\n",
    "    total_input = sum(r.input_tokens for r in results)\n",
    "    total_cached = sum(r.cached_tokens for r in results)\n",
    "    cache_rate = (total_cached / total_input * 100) if total_input > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"   Total input tokens: {total_input:,}\")\n",
    "    print(f\"   Total cached tokens: {total_cached:,}\")\n",
    "    print(f\"   Cache rate: {cache_rate:.1f}%\")\n",
    "    print(f\"   üí∞ Estimated savings: {cache_rate/2:.1f}% cost reduction\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f1d7f",
   "metadata": {},
   "source": [
    "## Part 2: Redis Response Caching\n",
    "\n",
    "Cache exact responses to avoid redundant API calls for identical queries.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- FAQ bots with common questions\n",
    "- Data processing with repeated prompts\n",
    "- Development/testing environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d24b3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Response Caching Demo\n",
      "============================================================\n",
      "‚ö†Ô∏è Redis connection failed: invalid username-password pair\n",
      "   Using in-memory cache instead\n",
      "\n",
      "üìù Query 1: \"What is Python?\"\n",
      "   ‚è±Ô∏è  Latency: 1953.3ms\n",
      "   ‚ùå CACHE MISS (API call)\n",
      "\n",
      "üìù Query 2: \"What is Python?\"\n",
      "   ‚è±Ô∏è  Latency: 0.0ms\n",
      "   ‚úÖ CACHE HIT\n",
      "\n",
      "üìù Query 3: \"What is JavaScript?\"\n",
      "   ‚è±Ô∏è  Latency: 2713.8ms\n",
      "   ‚ùå CACHE MISS (API call)\n",
      "\n",
      "üìù Query 4: \"What is Python?\"\n",
      "   ‚è±Ô∏è  Latency: 0.0ms\n",
      "   ‚úÖ CACHE HIT\n",
      "\n",
      "üìù Query 5: \"What is JavaScript?\"\n",
      "   ‚è±Ô∏è  Latency: 0.0ms\n",
      "   ‚úÖ CACHE HIT\n",
      "\n",
      "üìà Summary:\n",
      "   Total queries: 5\n",
      "   API calls: 2\n",
      "   Cache hits: 3\n",
      "   Cache hit rate: 60%\n",
      "   üí∞ API cost reduction: 60%\n"
     ]
    }
   ],
   "source": [
    "# Response Caching implementation (in-memory fallback if no Redis)\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"\n",
    "    Simple response cache with TTL support.\n",
    "    Uses Redis if available, otherwise falls back to in-memory dict.\n",
    "    \"\"\"\n",
    "    def __init__(self, redis_url: str = None, ttl_seconds: int = 3600):\n",
    "        self.ttl = ttl_seconds\n",
    "        self.use_redis = False\n",
    "        self._memory_cache: Dict[str, Tuple[str, float]] = {}  # {key: (value, expiry_time)}\n",
    "        \n",
    "        if redis_url:\n",
    "            try:\n",
    "                import redis\n",
    "                self.redis_client = redis.from_url(redis_url)\n",
    "                self.redis_client.ping()\n",
    "                self.use_redis = True\n",
    "                print(\"‚úÖ Connected to Redis\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Redis connection failed: {e}\")\n",
    "                print(\"   Using in-memory cache instead\")\n",
    "        else:\n",
    "            print(\"üì¶ Using in-memory cache (no Redis URL provided)\")\n",
    "    \n",
    "    def _hash_key(self, prompt: str) -> str:\n",
    "        \"\"\"Create a hash key from the prompt.\"\"\"\n",
    "        return hashlib.sha256(prompt.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def get(self, prompt: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response for a prompt.\"\"\"\n",
    "        key = self._hash_key(prompt)\n",
    "        \n",
    "        if self.use_redis:\n",
    "            value = self.redis_client.get(key)\n",
    "            return value.decode() if value else None\n",
    "        else:\n",
    "            if key in self._memory_cache:\n",
    "                value, expiry = self._memory_cache[key]\n",
    "                if time.time() < expiry:\n",
    "                    return value\n",
    "                else:\n",
    "                    del self._memory_cache[key]\n",
    "            return None\n",
    "    \n",
    "    def set(self, prompt: str, response: str):\n",
    "        \"\"\"Cache a response for a prompt.\"\"\"\n",
    "        key = self._hash_key(prompt)\n",
    "        \n",
    "        if self.use_redis:\n",
    "            self.redis_client.setex(key, self.ttl, response)\n",
    "        else:\n",
    "            self._memory_cache[key] = (response, time.time() + self.ttl)\n",
    "    \n",
    "    def stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        if self.use_redis:\n",
    "            info = self.redis_client.info()\n",
    "            return {\n",
    "                \"hits\": info.get(\"keyspace_hits\", 0),\n",
    "                \"misses\": info.get(\"keyspace_misses\", 0),\n",
    "            }\n",
    "        else:\n",
    "            return {\"entries\": len(self._memory_cache)}\n",
    "\n",
    "def cached_completion(\n",
    "    client: AzureOpenAI,\n",
    "    cache: ResponseCache,\n",
    "    deployment: str,\n",
    "    prompt: str,\n",
    "    force_refresh: bool = False\n",
    ") -> Tuple[str, bool, float]:\n",
    "    \"\"\"\n",
    "    Get completion with caching.\n",
    "    Returns: (response, cache_hit, latency_ms)\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Check cache first\n",
    "    if not force_refresh:\n",
    "        cached = cache.get(prompt)\n",
    "        if cached:\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            return cached, True, latency\n",
    "    \n",
    "    # Call API\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    latency = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    # Store in cache\n",
    "    cache.set(prompt, result)\n",
    "    \n",
    "    return result, False, latency\n",
    "\n",
    "# Demonstrate response caching\n",
    "print(\"üìä Response Caching Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cache = ResponseCache(REDIS_URL if REDIS_URL else None, ttl_seconds=300)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Simulate repeated queries (common in FAQ scenarios)\n",
    "    queries = [\n",
    "        \"What is Python?\",\n",
    "        \"What is Python?\",  # Repeated - should hit cache\n",
    "        \"What is JavaScript?\",\n",
    "        \"What is Python?\",  # Repeated - should hit cache\n",
    "        \"What is JavaScript?\",  # Repeated - should hit cache\n",
    "    ]\n",
    "    \n",
    "    total_api_calls = 0\n",
    "    total_cache_hits = 0\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        response, cache_hit, latency = cached_completion(\n",
    "            client, cache, AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, query\n",
    "        )\n",
    "        \n",
    "        if cache_hit:\n",
    "            total_cache_hits += 1\n",
    "            status = \"‚úÖ CACHE HIT\"\n",
    "        else:\n",
    "            total_api_calls += 1\n",
    "            status = \"‚ùå CACHE MISS (API call)\"\n",
    "        \n",
    "        print(f\"\\nüìù Query {i}: \\\"{query}\\\"\")\n",
    "        print(f\"   ‚è±Ô∏è  Latency: {latency:.1f}ms\")\n",
    "        print(f\"   {status}\")\n",
    "    \n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"   Total queries: {len(queries)}\")\n",
    "    print(f\"   API calls: {total_api_calls}\")\n",
    "    print(f\"   Cache hits: {total_cache_hits}\")\n",
    "    print(f\"   Cache hit rate: {total_cache_hits/len(queries)*100:.0f}%\")\n",
    "    print(f\"   üí∞ API cost reduction: {total_cache_hits/len(queries)*100:.0f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c5d94",
   "metadata": {},
   "source": [
    "## Part 3: Semantic Caching\n",
    "\n",
    "Semantic caching uses vector similarity to match queries that are **similar but not identical**.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Query: \"What is the capital of France?\"  ‚Üí Cache MISS ‚Üí API call ‚Üí Store embedding + response\n",
    "Query: \"What's France's capital city?\"   ‚Üí Similarity > 0.9 ‚Üí Cache HIT ‚Üí Return cached response\n",
    "Query: \"Tell me about Paris\"             ‚Üí Similarity < 0.7 ‚Üí Cache MISS ‚Üí API call\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- Handles paraphrased queries\n",
    "- Reduces redundant API calls for semantically similar questions\n",
    "- Configurable similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccb69e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Semantic Caching Demo\n",
      "============================================================\n",
      "üéØ Similarity threshold: 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Query 1: \"What is the capital of France?\"\n",
      "   ‚è±Ô∏è  Latency: 903ms\n",
      "   ‚ùå CACHE MISS (best match: 0.00)\n",
      "\n",
      "üìù Query 2: \"What's France's capital city?\"\n",
      "   ‚è±Ô∏è  Latency: 53ms\n",
      "   ‚úÖ SEMANTIC HIT (similarity: 0.78)\n",
      "\n",
      "üìù Query 3: \"Tell me the capital of France\"\n",
      "   ‚è±Ô∏è  Latency: 76ms\n",
      "   ‚úÖ SEMANTIC HIT (similarity: 0.73)\n",
      "\n",
      "üìù Query 4: \"What is the capital of Germany?\"\n",
      "   ‚è±Ô∏è  Latency: 919ms\n",
      "   ‚ùå CACHE MISS (best match: 0.57)\n",
      "\n",
      "üìù Query 5: \"Germany's capital is?\"\n",
      "   ‚è±Ô∏è  Latency: 52ms\n",
      "   ‚úÖ SEMANTIC HIT (similarity: 0.84)\n",
      "\n",
      "üìà Summary:\n",
      "   Total queries: 5\n",
      "   Semantic hits: 3\n",
      "   Cache misses: 1\n",
      "   Hit rate: 60%\n"
     ]
    }
   ],
   "source": [
    "# Semantic Caching implementation\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class SemanticCacheEntry:\n",
    "    \"\"\"Entry in the semantic cache.\"\"\"\n",
    "    prompt: str\n",
    "    response: str\n",
    "    embedding: List[float]\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "\n",
    "class SemanticCache:\n",
    "    \"\"\"\n",
    "    Semantic cache using vector similarity.\n",
    "    \n",
    "    Uses embeddings to find similar queries and return cached responses.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_client: AzureOpenAI,\n",
    "        embedding_model: str,\n",
    "        similarity_threshold: float = 0.9\n",
    "    ):\n",
    "        self.embedding_client = embedding_client\n",
    "        self.embedding_model = embedding_model\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.entries: List[SemanticCacheEntry] = []\n",
    "        self.stats = {\"hits\": 0, \"misses\": 0}\n",
    "    \n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding for text.\"\"\"\n",
    "        response = self.embedding_client.embeddings.create(\n",
    "            model=self.embedding_model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        a_np = np.array(a)\n",
    "        b_np = np.array(b)\n",
    "        return float(np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np)))\n",
    "    \n",
    "    def get(self, prompt: str) -> Tuple[Optional[str], float]:\n",
    "        \"\"\"\n",
    "        Find cached response for semantically similar prompt.\n",
    "        Returns: (response or None, best_similarity_score)\n",
    "        \"\"\"\n",
    "        if not self.entries:\n",
    "            return None, 0.0\n",
    "        \n",
    "        query_embedding = self._get_embedding(prompt)\n",
    "        \n",
    "        best_match = None\n",
    "        best_similarity = 0.0\n",
    "        \n",
    "        for entry in self.entries:\n",
    "            similarity = self._cosine_similarity(query_embedding, entry.embedding)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match = entry\n",
    "        \n",
    "        if best_similarity >= self.similarity_threshold:\n",
    "            self.stats[\"hits\"] += 1\n",
    "            return best_match.response, best_similarity\n",
    "        \n",
    "        self.stats[\"misses\"] += 1\n",
    "        return None, best_similarity\n",
    "    \n",
    "    def set(self, prompt: str, response: str):\n",
    "        \"\"\"Store prompt-response pair in cache.\"\"\"\n",
    "        embedding = self._get_embedding(prompt)\n",
    "        self.entries.append(SemanticCacheEntry(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            embedding=embedding\n",
    "        ))\n",
    "\n",
    "def semantic_cached_completion(\n",
    "    client: AzureOpenAI,\n",
    "    cache: SemanticCache,\n",
    "    deployment: str,\n",
    "    prompt: str\n",
    ") -> Tuple[str, bool, float, float]:\n",
    "    \"\"\"\n",
    "    Get completion with semantic caching.\n",
    "    Returns: (response, cache_hit, latency_ms, similarity)\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Check semantic cache\n",
    "    cached_response, similarity = cache.get(prompt)\n",
    "    \n",
    "    if cached_response:\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "        return cached_response, True, latency, similarity\n",
    "    \n",
    "    # Call API\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    latency = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    # Store in cache\n",
    "    cache.set(prompt, result)\n",
    "    \n",
    "    return result, False, latency, similarity\n",
    "\n",
    "# Demonstrate semantic caching\n",
    "print(\"üìä Semantic Caching Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    semantic_cache = SemanticCache(\n",
    "        embedding_client=client,\n",
    "        embedding_model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "        similarity_threshold=0.70\n",
    "    )\n",
    "    \n",
    "    # Queries with semantic variations\n",
    "    queries = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"What's France's capital city?\",  # Semantically similar\n",
    "        \"Tell me the capital of France\",  # Semantically similar\n",
    "        \"What is the capital of Germany?\",  # Different topic\n",
    "        \"Germany's capital is?\",  # Semantically similar to #4\n",
    "    ]\n",
    "    \n",
    "    print(f\"üéØ Similarity threshold: {semantic_cache.similarity_threshold}\")\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        response, cache_hit, latency, similarity = semantic_cached_completion(\n",
    "            client, semantic_cache, AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, query\n",
    "        )\n",
    "        \n",
    "        if cache_hit:\n",
    "            status = f\"‚úÖ SEMANTIC HIT (similarity: {similarity:.2f})\"\n",
    "        else:\n",
    "            status = f\"‚ùå CACHE MISS (best match: {similarity:.2f})\"\n",
    "        \n",
    "        print(f\"\\nüìù Query {i}: \\\"{query}\\\"\")\n",
    "        print(f\"   ‚è±Ô∏è  Latency: {latency:.0f}ms\")\n",
    "        print(f\"   {status}\")\n",
    "    \n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"   Total queries: {len(queries)}\")\n",
    "    print(f\"   Semantic hits: {semantic_cache.stats['hits']}\")\n",
    "    print(f\"   Cache misses: {semantic_cache.stats['misses']}\")\n",
    "    print(f\"   Hit rate: {semantic_cache.stats['hits']/len(queries)*100:.0f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0904415",
   "metadata": {},
   "source": [
    "## Cleanup Resources\n",
    "\n",
    "When you're done with this lab, you can delete the Azure Redis Enterprise instance to avoid ongoing charges.\n",
    "\n",
    "> **Warning**: This will permanently delete the Redis instance and all cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Delete Azure Redis Enterprise instance\n",
    "# ================================================\n",
    "\n",
    "DELETE_REDIS = False  # Set to True to delete the Redis instance\n",
    "\n",
    "if DELETE_REDIS:\n",
    "    print(\"üóëÔ∏è Deleting Azure Redis Enterprise...\")\n",
    "    print(f\"   Resource Group: {REDIS_RESOURCE_GROUP}\")\n",
    "    print(f\"   Redis Name: {REDIS_NAME}\")\n",
    "    \n",
    "    confirm = input(\"\\n‚ö†Ô∏è Are you sure? Type 'yes' to confirm: \")\n",
    "    \n",
    "    if confirm.lower() == 'yes':\n",
    "        try:\n",
    "            # Delete the Redis Enterprise cluster (this also deletes the database)\n",
    "            run_az([\n",
    "                \"redisenterprise\", \"delete\",\n",
    "                \"-g\", REDIS_RESOURCE_GROUP,\n",
    "                \"-n\", REDIS_NAME,\n",
    "                \"--yes\",\n",
    "                \"--no-wait\"\n",
    "            ])\n",
    "            print(\"\\n‚úÖ Redis Enterprise deletion initiated.\")\n",
    "            print(\"   This may take a few minutes to complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Failed to delete Redis: {e}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Deletion cancelled.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Set DELETE_REDIS=True to delete the Azure Redis Enterprise instance.\")\n",
    "    print(f\"   Current Redis: {REDIS_NAME} in {REDIS_RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching Strategy Comparison\n",
    "print(\"üìä Caching Strategy Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison = [\n",
    "    {\n",
    "        \"Strategy\": \"Prompt Caching (Azure OpenAI)\",\n",
    "        \"Match Type\": \"Exact prefix\",\n",
    "        \"Setup\": \"Automatic\",\n",
    "        \"Latency Reduction\": \"Moderate\",\n",
    "        \"Cost Reduction\": \"50-75%\",\n",
    "        \"Best For\": \"Long system prompts, few-shot examples\",\n",
    "    },\n",
    "    {\n",
    "        \"Strategy\": \"Response Caching (Redis)\",\n",
    "        \"Match Type\": \"Exact query\",\n",
    "        \"Setup\": \"Redis required\",\n",
    "        \"Latency Reduction\": \"~99%\",\n",
    "        \"Cost Reduction\": \"100% on hits\",\n",
    "        \"Best For\": \"FAQ bots, repeated queries\",\n",
    "    },\n",
    "    {\n",
    "        \"Strategy\": \"Semantic Caching\",\n",
    "        \"Match Type\": \"Similar queries\",\n",
    "        \"Setup\": \"Embeddings + Vector DB\",\n",
    "        \"Latency Reduction\": \"~90%\",\n",
    "        \"Cost Reduction\": \"100% on hits\",\n",
    "        \"Best For\": \"Natural language variations, chatbots\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\n{'Strategy':<30} {'Match':<15} {'Latency':<12} {'Cost':<10} {'Best For'}\")\n",
    "print(\"-\" * 100)\n",
    "for row in comparison:\n",
    "    print(f\"{row['Strategy']:<30} {row['Match Type']:<15} {row['Latency Reduction']:<12} {row['Cost Reduction']:<10} {row['Best For']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã Decision Guide:\")\n",
    "print(\"\\n   Use PROMPT CACHING when:\")\n",
    "print(\"   ‚Ä¢ You have long system prompts (>1024 tokens)\")\n",
    "print(\"   ‚Ä¢ Your prompts share common prefixes (few-shot examples)\")\n",
    "print(\"   ‚Ä¢ You want automatic cost reduction with no setup\")\n",
    "\n",
    "print(\"\\n   Use RESPONSE CACHING when:\")\n",
    "print(\"   ‚Ä¢ You have frequently repeated exact queries\")\n",
    "print(\"   ‚Ä¢ Latency is critical (need <10ms responses)\")\n",
    "print(\"   ‚Ä¢ You're building FAQ or support bots\")\n",
    "\n",
    "print(\"\\n   Use SEMANTIC CACHING when:\")\n",
    "print(\"   ‚Ä¢ Users ask the same question in different ways\")\n",
    "print(\"   ‚Ä¢ Exact match caching has low hit rates\")\n",
    "print(\"   ‚Ä¢ You're building conversational AI with varied inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9362f3c",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### Caching Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     Multi-Layer Caching                             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                     ‚îÇ\n",
    "‚îÇ  User Query ‚îÄ‚îÄ‚ñ∫ [Semantic Cache] ‚îÄ‚îÄ‚ñ∫ [Response Cache] ‚îÄ‚îÄ‚ñ∫ API      ‚îÇ\n",
    "‚îÇ                      ‚îÇ                     ‚îÇ               ‚îÇ        ‚îÇ\n",
    "‚îÇ                      ‚îÇ                     ‚îÇ               ‚îÇ        ‚îÇ\n",
    "‚îÇ              Similar query?          Exact query?    Prompt Cache   ‚îÇ\n",
    "‚îÇ                  ‚Üì Yes                  ‚Üì Yes          (automatic)  ‚îÇ\n",
    "‚îÇ              Return cached          Return cached                   ‚îÇ\n",
    "‚îÇ                                                                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Recommendations\n",
    "\n",
    "| Practice | Recommendation |\n",
    "|----------|----------------|\n",
    "| **System prompts** | Keep >1024 tokens for prompt caching eligibility |\n",
    "| **Cache TTL** | 5-60 minutes for dynamic content, longer for static |\n",
    "| **Similarity threshold** | Start at 0.9, lower to 0.85 if hit rate too low |\n",
    "| **Cache invalidation** | Implement clear mechanism for content updates |\n",
    "| **Monitoring** | Track hit rates, latency distribution, cost savings |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c03f6",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Prompt Caching**: Azure OpenAI automatic caching for repeated prefixes (50-75% cost reduction)\n",
    "2. **Response Caching**: Exact-match caching with Redis for FAQ-style applications\n",
    "3. **Semantic Caching**: Vector similarity caching for natural language variations\n",
    "4. **Multi-layer strategy**: Combine approaches for optimal performance\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure OpenAI Prompt Caching](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/prompt-caching)\n",
    "- [Azure Cache for Redis - Semantic Caching](https://learn.microsoft.com/en-us/azure/redis/tutorial-semantic-cache)\n",
    "- [Agent Innovator Lab - Caching](https://github.com/Azure/agent-innovator-lab/tree/main/3_optimization-design-ptn/02_caching)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **6_cost_analytics.ipynb**: Monitor token usage and analyze costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
