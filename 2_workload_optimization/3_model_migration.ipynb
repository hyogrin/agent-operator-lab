{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd09c4",
   "metadata": {},
   "source": [
    "# Optimization: Model Migration\n",
    "----\n",
    "\n",
    "This notebook focuses on **Workload Optimization** for model migration (e.g., GPT-4.x ‚Üí GPT-5.x) using the **Azure OpenAI Responses API**.\n",
    "\n",
    "You will learn:\n",
    "- What changes when migrating to GPT-5.x (quality/latency/cost/reasoning controls)\n",
    "- Responses API parameter differences (e.g., `max_tokens` is not used)\n",
    "- Migration checklist (regions, model availability, deployment naming, caching, evaluation)\n",
    "- Weighted routing (canary rollout) across two models and how to analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3aa82",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Why Model Migration Needs a Plan](#why-model-migration-needs-a-plan)\n",
    "- [Migration Checklist (GPT-4.x ‚Üí GPT-5.x)](#migration-checklist-gpt-4x--gpt-5x)\n",
    "- [Responses API Migration Notes](#responses-api-migration-notes)\n",
    "- [Region / Model Availability Checks](#region--model-availability-checks)\n",
    "- [Example: APIM Weighted Routing + Analysis](#example-apim-weighted-routing--analysis)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee86bf",
   "metadata": {},
   "source": [
    "## Why Model Migration Needs a Plan\n",
    "\n",
    "Model migration is not just a ‚Äúdrop-in replacement‚Äù. Even when an answer looks correct, the system-level behavior can change:\n",
    "\n",
    "- **Output shape**: verbosity, formatting, instruction-following fidelity\n",
    "- **Latency profile**: time-to-first-token (streaming) and tail latency\n",
    "- **Cost profile**: tokenization differences + caching behavior + pricing deltas\n",
    "- **Reasoning controls**: GPT-5.x introduces explicit reasoning controls (`reasoning.effort`)\n",
    "- **Availability**: model versions and SKUs vary by **region** and **deployment type**\n",
    "\n",
    "A practical approach is to run a controlled migration: **evaluate ‚Üí canary ‚Üí ramp ‚Üí finalize**, with routing weights and measurable acceptance criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34aa123",
   "metadata": {},
   "source": [
    "## Migration Checklist (GPT-4.x ‚Üí GPT-5.x)\n",
    "\n",
    "Use this checklist before you switch production traffic.\n",
    "\n",
    "### 1) Inventory & Compatibility\n",
    "\n",
    "- Confirm your current **deployment names**, **API version**, and any gateway/proxy policies (APIM, custom router).\n",
    "- Identify prompts that rely on ‚Äúquirks‚Äù (formatting, strict JSON, special tokens) and add contract tests.\n",
    "- Decide whether you need **streaming** and measure **TTFT** separately from total latency.\n",
    "\n",
    "### 2) Parameter & API Differences (High impact)\n",
    "\n",
    "- If you migrate to **Responses API**, you typically use:\n",
    "  - `instructions` + `input` (instead of Chat Completions `messages`)\n",
    "  - `max_output_tokens` (instead of `max_tokens`)\n",
    "  - `reasoning: { effort: ... }` for GPT-5.x reasoning control\n",
    "- If you keep Chat Completions, expect some parameter names and behaviors to differ by model family and SDK version.\n",
    "\n",
    "### 3) Region / SKU / Model Availability\n",
    "\n",
    "- **Do not assume** the same model/version exists in every region.\n",
    "- Verify the target model is available in your region and SKU (GlobalStandard / Standard / PTU, etc.).\n",
    "- If you use multi-region routing, validate cross-region latency and compliance requirements.\n",
    "\n",
    "### 4) Evaluation & Rollout\n",
    "\n",
    "- Run an evaluation set representative of your workload: accuracy, refusal rate, formatting compliance, safety constraints.\n",
    "- Start with a **canary weight** (e.g., 95/5), then ramp (80/20 ‚Üí 50/50 ‚Üí 0/100) based on metrics.\n",
    "- Monitor: error rate (429/5xx), latency percentiles, cache hit %, and cost/request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1c3b1",
   "metadata": {},
   "source": [
    "## Responses API Migration Notes\n",
    "\n",
    "If you are moving from Chat Completions to Responses API, the most common ‚Äúgotchas‚Äù are parameter names and payload shape changes.\n",
    "\n",
    "### Common Mapping\n",
    "\n",
    "| Legacy (Chat Completions) | Responses API | Notes |\n",
    "|---|---|---|\n",
    "| `messages=[{role, content}]` | `instructions=...` + `input=...` | Split system vs user content |\n",
    "| `max_tokens` | `max_output_tokens` | Responses API uses output token cap |\n",
    "| `stream=True` | `stream=True` | Streaming event types differ |\n",
    "| `temperature`, `top_p` | `temperature`, `top_p` | Still exist, but defaults/behavior can vary by model |\n",
    "| (N/A) | `reasoning={effort: ...}` | GPT-5.x reasoning control (optional) |\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- If your code still passes `max_tokens`, it will be ignored or error depending on SDK/version. Use `max_output_tokens`.\n",
    "- GPT-5.x may behave differently with the same prompt; treat it as a **behavioral migration** and re-run evaluations.\n",
    "- Prompt caching in Azure requires a sufficiently large stable prefix; align your `instructions` / prefix for cache eligibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7827a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Verify required packages\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    print(\"‚úÖ Azure OpenAI package imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå ERROR: openai package not installed.\")\n",
    "    print(\"   Run: pip install openai>=1.60.0\")\n",
    "    raise\n",
    "\n",
    "# Load Foundry project settings (created by 0_setup/1_setup.ipynb)\n",
    "config_file = \"../0_setup/.foundry_config.json\"\n",
    "try:\n",
    "    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"‚úÖ Loaded settings from '{config_file}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'.\")\n",
    "    print(\"üí° Run 0_setup/1_setup.ipynb first to create it.\")\n",
    "    config = {}\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_API_VERSION = \"2025-04-01-preview\"\n",
    "\n",
    "print(f\"\\nüìå Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT[:50]}...\" if AZURE_OPENAI_ENDPOINT else \"‚ö†Ô∏è AZURE_OPENAI_ENDPOINT not set\")\n",
    "print(f\"üìå API Key: {'‚úÖ Set' if AZURE_OPENAI_API_KEY else '‚ö†Ô∏è Not set'}\")\n",
    "\n",
    "client: Optional[AzureOpenAI] = None\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    print(\"‚úÖ Azure OpenAI client initialized\")\n",
    "    print(f\"   Base URL: {AZURE_OPENAI_ENDPOINT.rstrip('/')}/openai/v1/\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Live calls disabled (missing endpoint/key). You can still read the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdca8eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Models Configured\n",
      "============================================================\n",
      "  ‚Ä¢ gpt-4o (baseline): api_model=gpt-4o price_key=gpt-4o reasoning_effort=None\n",
      "  ‚Ä¢ gpt-5.1 (candidate): api_model=gpt-5.1 price_key=gpt-5.1 reasoning_effort=low\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Pricing (per 1M tokens) - update to match your contract\n",
    "PRICING: Dict[str, Dict[str, float]] = {\n",
    "    \"gpt-4o\": {\"input\": 2.50, \"cached_input\": 1.25, \"output\": 10.00},\n",
    "    \"gpt-5.1\": {\"input\": 1.25, \"cached_input\": 0.13, \"output\": 10.00},\n",
    "}\n",
    "\n",
    "# Select TWO models for a migration experiment\n",
    "MODEL_A = {\n",
    "    \"label\": \"gpt-4o (baseline)\",\n",
    "    \"api_model\": \"gpt-4o\",\n",
    "    \"price_key\": \"gpt-4o\",\n",
    "    \"reasoning_effort\": None,\n",
    "}\n",
    "MODEL_B = {\n",
    "    \"label\": \"gpt-5.1 (candidate)\",\n",
    "    \"api_model\": \"gpt-5.1\",\n",
    "    \"price_key\": \"gpt-5.1\",\n",
    "    \"reasoning_effort\": \"low\",  # none/low/medium/high (only for GPT-5.x)\n",
    "}\n",
    "\n",
    "MODELS = [MODEL_A, MODEL_B]\n",
    "MODEL_LABELS = [m[\"label\"] for m in MODELS]\n",
    "\n",
    "# Validate price keys exist\n",
    "for m in MODELS:\n",
    "    if m[\"price_key\"] not in PRICING:\n",
    "        raise KeyError(f\"Missing PRICING entry for price_key='{m['price_key']}'\")\n",
    "\n",
    "print(\"üß™ Models Configured\")\n",
    "print(\"=\" * 60)\n",
    "for m in MODELS:\n",
    "    print(f\"  ‚Ä¢ {m['label']}: api_model={m['api_model']} price_key={m['price_key']} reasoning_effort={m.get('reasoning_effort')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0b0ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helpers ready: calculate_cost / check_answer / call_with_responses_api\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(price_key: str, input_tokens: int, output_tokens: int, cached_tokens: int = 0) -> float:\n",
    "    \"\"\"Calculate request cost in USD using PRICING (per 1M tokens).\"\"\"\n",
    "    pricing = PRICING[price_key]\n",
    "    uncached_tokens = max(0, input_tokens - cached_tokens)\n",
    "    return (\n",
    "        uncached_tokens * pricing[\"input\"]\n",
    "        + cached_tokens * pricing[\"cached_input\"]\n",
    "        + output_tokens * pricing[\"output\"]\n",
    "    ) / 1_000_000\n",
    "\n",
    "\n",
    "def check_answer(response: str, correct_variants: List[str]) -> bool:\n",
    "    \"\"\"Simple keyword-based check (use a stronger eval for production).\"\"\"\n",
    "    text = (response or \"\").lower().strip()\n",
    "    return any(v.lower() in text for v in correct_variants)\n",
    "\n",
    "\n",
    "def call_with_responses_api(\n",
    "    client: AzureOpenAI,\n",
    "    api_model: str,\n",
    "    instructions: str,\n",
    "    user_input: str,\n",
    "    *,\n",
    "    max_output_tokens: int = 200,\n",
    "    reasoning_effort: Optional[str] = None,\n",
    "    prompt_cache_key: Optional[str] = None,\n",
    "    stream: bool = False,\n",
    "    temperature: Optional[float] = None,\n",
    "    top_p: Optional[float] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Wrapper for Azure OpenAI Responses API calls.\n",
    "\n",
    "    Notes:\n",
    "    - Responses API uses `max_output_tokens` (NOT `max_tokens`).\n",
    "    - GPT-5.x can accept `reasoning={effort: ...}`.\n",
    "    - Azure prompt caching can be routed via `extra_body={\"prompt_cache_key\": ...}`.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"model\": api_model,\n",
    "        \"instructions\": instructions,\n",
    "        \"input\": user_input,\n",
    "        \"max_output_tokens\": max_output_tokens,\n",
    "    }\n",
    "    if prompt_cache_key:\n",
    "        params[\"extra_body\"] = {\"prompt_cache_key\": prompt_cache_key}\n",
    "    if reasoning_effort:\n",
    "        params[\"reasoning\"] = {\"effort\": reasoning_effort}\n",
    "    if temperature is not None:\n",
    "        params[\"temperature\"] = temperature\n",
    "    if top_p is not None:\n",
    "        params[\"top_p\"] = top_p\n",
    "    if stream:\n",
    "        params[\"stream\"] = True\n",
    "\n",
    "    start = time.time()\n",
    "    if not stream:\n",
    "        response = client.responses.create(**params)\n",
    "        latency = time.time() - start\n",
    "\n",
    "        usage = getattr(response, \"usage\", None)\n",
    "        input_tokens = getattr(usage, \"input_tokens\", 0) if usage else 0\n",
    "        output_tokens = getattr(usage, \"output_tokens\", 0) if usage else 0\n",
    "        cached_tokens = 0\n",
    "        details = getattr(usage, \"input_tokens_details\", None) if usage else None\n",
    "        if details:\n",
    "            cached_tokens = getattr(details, \"cached_tokens\", 0) or 0\n",
    "\n",
    "        content = \"\"\n",
    "        if getattr(response, \"output\", None):\n",
    "            for item in response.output:\n",
    "                if hasattr(item, \"content\"):\n",
    "                    for part in item.content:\n",
    "                        if hasattr(part, \"text\") and part.text:\n",
    "                            content += part.text\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"latency\": latency,\n",
    "            \"first_token_time\": latency,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cached_tokens\": cached_tokens,\n",
    "            \"content\": content,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "\n",
    "    # Streaming mode (TTFT included)\n",
    "    stream_iter = client.responses.create(**params)\n",
    "    first_token_time: Optional[float] = None\n",
    "    content = \"\"\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    cached_tokens = 0\n",
    "\n",
    "    for event in stream_iter:\n",
    "        if first_token_time is None and hasattr(event, \"type\"):\n",
    "            if event.type in [\n",
    "                \"response.output_item.added\",\n",
    "                \"response.content_part.added\",\n",
    "                \"response.output_text.delta\",\n",
    "            ]:\n",
    "                first_token_time = time.time() - start\n",
    "\n",
    "        if hasattr(event, \"type\") and event.type == \"response.output_text.delta\":\n",
    "            delta = getattr(event, \"delta\", None)\n",
    "            if delta:\n",
    "                content += delta\n",
    "\n",
    "        if hasattr(event, \"type\") and event.type == \"response.completed\":\n",
    "            resp = getattr(event, \"response\", None)\n",
    "            usage = getattr(resp, \"usage\", None) if resp else None\n",
    "            if usage:\n",
    "                input_tokens = getattr(usage, \"input_tokens\", 0)\n",
    "                output_tokens = getattr(usage, \"output_tokens\", 0)\n",
    "                details = getattr(usage, \"input_tokens_details\", None)\n",
    "                if details:\n",
    "                    cached_tokens = getattr(details, \"cached_tokens\", 0) or 0\n",
    "\n",
    "    latency = time.time() - start\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"latency\": latency,\n",
    "        \"first_token_time\": first_token_time or latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cached_tokens\": cached_tokens,\n",
    "        \"content\": content,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helpers ready: calculate_cost / check_answer / call_with_responses_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd1393",
   "metadata": {},
   "source": [
    "### Code Call Differences: GPT-4.x vs GPT-5.x\n",
    "\n",
    "In practice, the biggest code-level changes are:\n",
    "\n",
    "- **Responses API output cap**: use `max_output_tokens` (not `max_tokens`)\n",
    "- **GPT-5.x reasoning**: optionally set `reasoning={\"effort\": \"low|medium|high\"}`\n",
    "- **Deployment naming**: on Azure, `model=...` is often the deployment name (not the base model family name)\n",
    "\n",
    "The cell below shows a minimal side-by-side call pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "291362df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Set ENABLE_LIVE_CALLS=True (defined later in this notebook) to run live calls.\n"
     ]
    }
   ],
   "source": [
    "# Minimal side-by-side call example (safe by default)\n",
    "enable_live_calls = bool(globals().get(\"ENABLE_LIVE_CALLS\", False))\n",
    "instructions = globals().get(\n",
    "    \"INSTRUCTIONS\",\n",
    "    \"You are a helpful assistant. Answer concisely and directly. \"\n",
    "    \"For questions requiring a specific format, follow the format exactly.\",\n",
    " )\n",
    "max_output_tokens = int(globals().get(\"MAX_OUTPUT_TOKENS\", 120))\n",
    "prompt_cache_key = globals().get(\"PROMPT_CACHE_KEY\", \"migration_canary_v1\")\n",
    "\n",
    "if not client:\n",
    "    print(\"‚ö†Ô∏è Client not initialized; skipping live example.\")\n",
    "elif not enable_live_calls:\n",
    "    print(\"‚ÑπÔ∏è Set ENABLE_LIVE_CALLS=True (defined later in this notebook) to run live calls.\")\n",
    "else:\n",
    "    question = \"Explain this code: def f(n): return n if n<=1 else f(n-1)+f(n-2). Answer in 2 sentences.\"\n",
    "    print(\"\\nüß™ GPT-4.x-style (baseline) call\")\n",
    "    res_a = call_with_responses_api(\n",
    "        client=client,\n",
    "        api_model=MODEL_A[\"api_model\"],\n",
    "        instructions=instructions,\n",
    "        user_input=question,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        prompt_cache_key=prompt_cache_key,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(f\"  latency={res_a['latency']:.3f}s input={res_a['input_tokens']} output={res_a['output_tokens']}\")\n",
    "    print(f\"  content={res_a['content'][:120]}...\")\n",
    "\n",
    "    print(\"\\nüß™ GPT-5.x-style (candidate) call with reasoning control\")\n",
    "    res_b = call_with_responses_api(\n",
    "        client=client,\n",
    "        api_model=MODEL_B[\"api_model\"],\n",
    "        instructions=instructions,\n",
    "        user_input=question,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        reasoning_effort=MODEL_B.get(\"reasoning_effort\"),\n",
    "        prompt_cache_key=prompt_cache_key,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(f\"  latency={res_b['latency']:.3f}s input={res_b['input_tokens']} output={res_b['output_tokens']}\")\n",
    "    print(f\"  content={res_b['content'][:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8cdc7",
   "metadata": {},
   "source": [
    "## Region / Model Availability Checks\n",
    "\n",
    "A frequent migration blocker is **availability**: the target GPT-5.x model (and the specific version/SKU) may not be available in the same region as your existing GPT-4.x deployment.\n",
    "\n",
    "### What to verify\n",
    "\n",
    "- Your Azure OpenAI / Foundry resource **location (region)**\n",
    "- The target model **is supported in that region** and in your intended **SKU**\n",
    "- You have a deployment created for the target model, and your code uses the correct **deployment name** (often passed as `model=...`)\n",
    "\n",
    "Below is a helper cell that prints your endpoint and (optionally) queries Azure if you provide resource info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "print(\"üîé Endpoint\")\n",
    "print(\"=\" * 60)\n",
    "print(AZURE_OPENAI_ENDPOINT or \"(not set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Optional: if you know the Azure resource name and resource group, we can query its region via Azure CLI.\n",
    "# Set these (env vars or inline) to enable the checks below.\n",
    "AZURE_OPENAI_RESOURCE_GROUP = os.environ.get(\"AZURE_OPENAI_RESOURCE_GROUP\", \"\")\n",
    "AZURE_OPENAI_RESOURCE_NAME = os.environ.get(\"AZURE_OPENAI_RESOURCE_NAME\", \"\")\n",
    "\n",
    "if not shutil.which(\"az\"):\n",
    "    print(\"‚ö†Ô∏è Azure CLI not found in PATH. Install/login if you want CLI-based checks.\")\n",
    "elif not AZURE_OPENAI_RESOURCE_GROUP or not AZURE_OPENAI_RESOURCE_NAME:\n",
    "    print(\"‚ÑπÔ∏è To query region via Azure CLI, set:\")\n",
    "    print(\"  export AZURE_OPENAI_RESOURCE_GROUP='...'\")\n",
    "    print(\"  export AZURE_OPENAI_RESOURCE_NAME='...'\")\n",
    "else:\n",
    "    print(\"‚úÖ Azure CLI detected, querying resource...\")\n",
    "    cmd = [\n",
    "        \"az\",\n",
    "        \"cognitiveservices\",\n",
    "        \"account\",\n",
    "        \"show\",\n",
    "        \"-g\",\n",
    "        AZURE_OPENAI_RESOURCE_GROUP,\n",
    "        \"-n\",\n",
    "        AZURE_OPENAI_RESOURCE_NAME,\n",
    "        \"-o\",\n",
    "        \"json\",\n",
    "    ]\n",
    "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if p.returncode != 0:\n",
    "        print(\"‚ùå Failed to query resource via Azure CLI:\")\n",
    "        print(p.stderr.strip() or p.stdout.strip())\n",
    "    else:\n",
    "        data = json.loads(p.stdout)\n",
    "        print(f\"üìç Resource location: {data.get('location')}\")\n",
    "        print(f\"üè∑Ô∏è  Kind: {data.get('kind')}\")\n",
    "        print(f\"‚úÖ Provisioning state: {data.get('properties', {}).get('provisioningState')}\")\n",
    "\n",
    "print(\"\\nüí° Region availability tip:\")\n",
    "print(\"- Validate model availability by region before rollout (models/SKU differ by region).\")\n",
    "print(\"- If you use multi-region routing, consider using response headers (e.g., x-ms-region) in gateway tests to confirm backend selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41823224",
   "metadata": {},
   "source": [
    "## Example: APIM Weighted Routing + Analysis\n",
    "\n",
    "The client-side router above is great for quick experiments, but many production migrations prefer **centralized routing** (policy/gateway) so that rollouts don‚Äôt require redeploying every client.\n",
    "\n",
    "This section provides a minimal **Azure API Management (APIM)** example that:\n",
    "- Creates (or reuses) an APIM instance\n",
    "- Configures an API endpoint that **routes traffic by weight** to two Azure OpenAI backends (baseline vs candidate)\n",
    "- Adds a response header (`x-routed-backend`) so you can **analyze which backend served each request**\n",
    "- Runs a small load test and summarizes distribution + latency\n",
    "\n",
    "This is inspired by the AI-Gateway backend pool load-balancing lab, but simplified for a two-model migration scenario.\n",
    "\n",
    "> Note: APIM provisioning can take a long time depending on SKU/region. The cells below are written so you can **skip creation** and only apply policy to an existing APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a6aeb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß APIM config\n",
      "====================================================================================================\n",
      "CREATE_APIM=True\n",
      "Subscription override: set\n",
      "APIM: rg=rg-model-migration name=apim-model-migration location=eastus\n",
      "API: https://apim-model-migration.azure-api.net/inference (POST /responses)\n",
      "Weights: A=90 B=10\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# APIM routing configuration (fill these in)\n",
    "import shutil\n",
    "import subprocess\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle infra actions\n",
    "CREATE_APIM = True  # If True, create RG + APIM (can take time)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = os.environ.get(\"AZURE_SUBSCRIPTION_ID\", \"\")\n",
    "APIM_LOCATION = os.environ.get(\"APIM_LOCATION\", \"eastus\")\n",
    "APIM_RESOURCE_GROUP = os.environ.get(\"APIM_RESOURCE_GROUP\", \"rg-model-migration\")\n",
    "APIM_SERVICE_NAME = os.environ.get(\"APIM_SERVICE_NAME\", \"apim-model-migration\")  # must be globally unique in Azure\n",
    "APIM_PUBLISHER_EMAIL = os.environ.get(\"APIM_PUBLISHER_EMAIL\", \"you@example.com\")\n",
    "APIM_PUBLISHER_NAME = os.environ.get(\"APIM_PUBLISHER_NAME\", \"Model Migration Lab\")\n",
    "\n",
    "# API surface exposed by APIM\n",
    "APIM_API_ID = \"migration-router\"\n",
    "APIM_API_PATH = \"inference\"  # https://{apim}.azure-api.net/inference/...\n",
    "APIM_OPERATION_ID = \"responses\"  # POST /responses\n",
    "\n",
    "# Responses API version (same family as used elsewhere in this repo)\n",
    "RESPONSES_API_VERSION = \"2025-04-01-preview\"\n",
    "\n",
    "# Backend A (baseline) and Backend B (candidate)\n",
    "# Provide TWO backend endpoints and keys so APIM can route between them.\n",
    "BACKEND_A = {\n",
    "    \"label\": MODEL_A[\"label\"],\n",
    "    \"endpoint\": os.environ.get(\"BACKEND_A_AZURE_OPENAI_ENDPOINT\", \"\"),  # e.g., https://xxx.openai.azure.com\n",
    "    \"api_key\": os.environ.get(\"BACKEND_A_AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"deployment\": os.environ.get(\"BACKEND_A_DEPLOYMENT\", MODEL_A[\"api_model\"]),\n",
    "}\n",
    "BACKEND_B = {\n",
    "    \"label\": MODEL_B[\"label\"],\n",
    "    \"endpoint\": os.environ.get(\"BACKEND_B_AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"api_key\": os.environ.get(\"BACKEND_B_AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"deployment\": os.environ.get(\"BACKEND_B_DEPLOYMENT\", MODEL_B[\"api_model\"]),\n",
    "}\n",
    "\n",
    "# Weight split (baseline vs candidate)\n",
    "APIM_WEIGHTS = {\n",
    "    \"A\": int(os.environ.get(\"APIM_WEIGHT_A\", \"90\")),\n",
    "    \"B\": int(os.environ.get(\"APIM_WEIGHT_B\", \"10\")),\n",
    "}\n",
    "\n",
    "# Optional: APIM subscription key (if your APIM requires it)\n",
    "APIM_SUBSCRIPTION_KEY = os.environ.get(\"APIM_SUBSCRIPTION_KEY\", \"\")\n",
    "\n",
    "def require_nonempty(name: str, value: str) -> None:\n",
    "    if not value:\n",
    "        raise ValueError(f\"Missing required value: {name}\")\n",
    "\n",
    "def run_az(args: List[str]) -> str:\n",
    "    cmd = [\"az\"] + args\n",
    "    print(\"\\n$ \" + \" \".join(cmd))\n",
    "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError((p.stderr or p.stdout).strip())\n",
    "    return p.stdout.strip()\n",
    "\n",
    "def maybe_set_subscription() -> None:\n",
    "    sub = (AZURE_SUBSCRIPTION_ID or \"\").strip()\n",
    "    if sub:\n",
    "        run_az([\"account\", \"set\", \"--subscription\", sub])\n",
    "\n",
    "def ensure_resource_group() -> None:\n",
    "    exists = run_az([\"group\", \"exists\", \"-n\", APIM_RESOURCE_GROUP]).strip().lower() == \"true\"\n",
    "    if exists:\n",
    "        print(f\"‚ÑπÔ∏è Resource group exists: {APIM_RESOURCE_GROUP} (skip create)\")\n",
    "        return\n",
    "    print(f\"üÜï Creating resource group: {APIM_RESOURCE_GROUP} ({APIM_LOCATION})\")\n",
    "    run_az([\"group\", \"create\", \"-n\", APIM_RESOURCE_GROUP, \"-l\", APIM_LOCATION])\n",
    "\n",
    "def apim_service_exists() -> bool:\n",
    "    try:\n",
    "        # `-o none` keeps output small; command returns non-zero if not found\n",
    "        run_az([\"apim\", \"show\", \"-g\", APIM_RESOURCE_GROUP, \"-n\", APIM_SERVICE_NAME, \"-o\", \"none\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"üîß APIM config\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"CREATE_APIM={CREATE_APIM}\")\n",
    "print(f\"Subscription override: {'set' if AZURE_SUBSCRIPTION_ID else '(use current az context)'}\")\n",
    "print(f\"APIM: rg={APIM_RESOURCE_GROUP} name={APIM_SERVICE_NAME} location={APIM_LOCATION}\")\n",
    "print(f\"API: https://{APIM_SERVICE_NAME}.azure-api.net/{APIM_API_PATH} (POST /responses)\")\n",
    "print(f\"Weights: A={APIM_WEIGHTS['A']} B={APIM_WEIGHTS['B']}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90ce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ az account set --subscription 3d4d3dd0-79d4-40cf-a94e-b4154812c6ca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ az group exists -n rg-model-migration\n",
      "‚ÑπÔ∏è Resource group exists: rg-model-migration (skip create)\n",
      "\n",
      "$ az apim show -g rg-model-migration -n apim-model-migration -o none\n",
      "‚ÑπÔ∏è APIM service exists: apim-model-migration (skip create)\n",
      "\n",
      "üîë APIM_SUBSCRIPTION_KEY already set (first 8 chars): 8bce805a...\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Create resource group + APIM (can take a while)\n",
    "import time as _time\n",
    "\n",
    "def wait_for_apim_ready(timeout_minutes: int = 60, poll_interval_s: int = 30) -> bool:\n",
    "    \"\"\"Poll APIM provisioningState until Succeeded or timeout.\"\"\"\n",
    "    deadline = _time.time() + timeout_minutes * 60\n",
    "    print(f\"\\n‚è≥ Waiting for APIM provisioning (timeout={timeout_minutes}m, poll every {poll_interval_s}s)...\")\n",
    "    while _time.time() < deadline:\n",
    "        try:\n",
    "            out = run_az([\n",
    "                \"apim\", \"show\",\n",
    "                \"-g\", APIM_RESOURCE_GROUP,\n",
    "                \"-n\", APIM_SERVICE_NAME,\n",
    "                \"--query\", \"provisioningState\",\n",
    "                \"-o\", \"tsv\",\n",
    "            ])\n",
    "            state = out.strip()\n",
    "            print(f\"   provisioningState: {state}\")\n",
    "            if state == \"Succeeded\":\n",
    "                return True\n",
    "            if state in (\"Failed\", \"Canceled\"):\n",
    "                print(f\"‚ùå APIM provisioning ended with state: {state}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"   (poll error: {e})\")\n",
    "        _time.sleep(poll_interval_s)\n",
    "    print(\"‚ùå Timeout waiting for APIM provisioning\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_apim_subscription_key() -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the primary key from the built-in 'all-access' subscription.\n",
    "    If not found, try 'master' subscription. Returns empty string on failure.\n",
    "    \"\"\"\n",
    "    for sub_name in [\"Built-in all-access subscription\", \"master\"]:\n",
    "        try:\n",
    "            key = run_az([\n",
    "                \"apim\", \"subscription\", \"show\",\n",
    "                \"-g\", APIM_RESOURCE_GROUP,\n",
    "                \"-n\", APIM_SERVICE_NAME,\n",
    "                \"--subscription-id\", sub_name,\n",
    "                \"--query\", \"primaryKey\",\n",
    "                \"-o\", \"tsv\",\n",
    "            ]).strip()\n",
    "            if key:\n",
    "                return key\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback: list subscriptions and pick first one with a key\n",
    "    try:\n",
    "        keys_json = run_az([\n",
    "            \"apim\", \"subscription\", \"list\",\n",
    "            \"-g\", APIM_RESOURCE_GROUP,\n",
    "            \"-n\", APIM_SERVICE_NAME,\n",
    "            \"--query\", \"[0].primaryKey\",\n",
    "            \"-o\", \"tsv\",\n",
    "        ]).strip()\n",
    "        if keys_json:\n",
    "            return keys_json\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "if not shutil.which(\"az\"):\n",
    "    print(\"‚ö†Ô∏è Azure CLI not found in PATH. Install/login to run APIM steps.\")\n",
    "else:\n",
    "    maybe_set_subscription()\n",
    "    ensure_resource_group()\n",
    "\n",
    "    exists = apim_service_exists()\n",
    "    if exists:\n",
    "        print(f\"‚ÑπÔ∏è APIM service exists: {APIM_SERVICE_NAME} (skip create)\")\n",
    "    elif not CREATE_APIM:\n",
    "        print(\"‚ö†Ô∏è APIM service not found, and CREATE_APIM=False.\")\n",
    "        print(\"   Either set CREATE_APIM=True or point to an existing APIM via APIM_SERVICE_NAME/APIM_RESOURCE_GROUP.\")\n",
    "    else:\n",
    "        # SKU note: Developer is common for labs but can still take time to provision.\n",
    "        # Adjust SKU to your needs (and cost constraints).\n",
    "        run_az([\n",
    "            \"apim\", \"create\",\n",
    "            \"-g\", APIM_RESOURCE_GROUP,\n",
    "            \"-n\", APIM_SERVICE_NAME,\n",
    "            \"--location\", APIM_LOCATION,\n",
    "            \"--publisher-email\", APIM_PUBLISHER_EMAIL,\n",
    "            \"--publisher-name\", APIM_PUBLISHER_NAME,\n",
    "            \"--sku-name\", \"Developer\",\n",
    "            \"--no-wait\",\n",
    "        ])\n",
    "\n",
    "        print(\"\\n‚è≥ APIM provisioning started (Developer SKU can take 30-60 minutes).\")\n",
    "        print(\"   You can also poll manually with:\")\n",
    "        print(f\"   az apim show -g {APIM_RESOURCE_GROUP} -n {APIM_SERVICE_NAME} -o table\")\n",
    "\n",
    "        # Wait for provisioning to complete\n",
    "        ready = wait_for_apim_ready(timeout_minutes=70, poll_interval_s=60)\n",
    "        if ready:\n",
    "            print(f\"\\n‚úÖ APIM ready: {APIM_SERVICE_NAME}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"APIM provisioning did not succeed in time\")\n",
    "\n",
    "    # Retrieve and set APIM subscription key if not already set\n",
    "    if not APIM_SUBSCRIPTION_KEY:\n",
    "        print(\"\\nüîë APIM_SUBSCRIPTION_KEY not set; attempting to retrieve from APIM...\")\n",
    "        retrieved_key = get_apim_subscription_key()\n",
    "        if retrieved_key:\n",
    "            # Update the global variable and environment\n",
    "            globals()[\"APIM_SUBSCRIPTION_KEY\"] = retrieved_key\n",
    "            os.environ[\"APIM_SUBSCRIPTION_KEY\"] = retrieved_key\n",
    "            print(\"‚úÖ Retrieved APIM subscription key and set in environment\")\n",
    "            print(f\"   Key (first 2 chars): {retrieved_key[:2]}...\")\n",
    "            print(\"\\nüí° To persist, add to your .env file:\")\n",
    "            print(f'   APIM_SUBSCRIPTION_KEY=\"{retrieved_key}\"')\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not retrieve APIM subscription key automatically.\")\n",
    "            print(\"   You may need to create a subscription manually or set APIM_SUBSCRIPTION_KEY in .env\")\n",
    "    else:\n",
    "        print(\"\\nüîë APIM_SUBSCRIPTION_KEY already set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an APIM API + weighted routing policy for Responses API\n",
    "from xml.sax.saxutils import escape as xml_escape\n",
    "\n",
    "def get_subscription_id() -> str:\n",
    "    # Prefer explicit override, else use current az context.\n",
    "    if (AZURE_SUBSCRIPTION_ID or \"\").strip():\n",
    "        return AZURE_SUBSCRIPTION_ID.strip()\n",
    "    return run_az([\"account\", \"show\", \"--query\", \"id\", \"-o\", \"tsv\"]).strip()\n",
    "\n",
    "def apply_apim_api_policy(policy_xml: str) -> None:\n",
    "    \"\"\"Apply API-level policy. Uses CLI if available, else falls back to az rest.\"\"\"\n",
    "    policy_path = Path(\"apim_migration_policy.xml\")\n",
    "    policy_path.write_text(policy_xml, encoding=\"utf-8\")\n",
    "\n",
    "    # 1) Try CLI subcommand (may be missing depending on az version/extensions)\n",
    "    try:\n",
    "        run_az([\n",
    "            \"apim\", \"api\", \"policy\", \"create\",\n",
    "            \"-g\", APIM_RESOURCE_GROUP,\n",
    "            \"--service-name\", APIM_SERVICE_NAME,\n",
    "            \"--api-id\", APIM_API_ID,\n",
    "            \"--xml-content\", f\"@{policy_path}\",\n",
    "        ])\n",
    "        print(\"‚úÖ Policy applied via: az apim api policy create\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(\"‚ÑπÔ∏è az apim api policy create not available or failed; trying az rest fallback\")\n",
    "        print(f\"   ({type(e).__name__}) {str(e).splitlines()[-1] if str(e) else str(e)}\")\n",
    "\n",
    "    sub_id = get_subscription_id()\n",
    "    uri = (\n",
    "        \"https://management.azure.com\"\n",
    "        f\"/subscriptions/{sub_id}\"\n",
    "        f\"/resourceGroups/{APIM_RESOURCE_GROUP}\"\n",
    "        f\"/providers/Microsoft.ApiManagement/service/{APIM_SERVICE_NAME}\"\n",
    "        f\"/apis/{APIM_API_ID}\"\n",
    "        \"/policies/policy\"\n",
    "        \"?api-version=2022-08-01\"\n",
    "    )\n",
    "    payload = {\"properties\": {\"format\": \"xml\", \"value\": policy_xml}}\n",
    "    payload_path = Path(\"apim_policy_payload.json\")\n",
    "    payload_path.write_text(json.dumps(payload), encoding=\"utf-8\")\n",
    "    run_az([\n",
    "        \"rest\",\n",
    "        \"--method\", \"put\",\n",
    "        \"--uri\", uri,\n",
    "        \"--body\", f\"@{payload_path}\",\n",
    "        \"--headers\", \"Content-Type=application/json\",\n",
    "    ])\n",
    "    print(\"‚úÖ Policy applied via: az rest (Management API)\")\n",
    "    print(f\"   Policy file: {policy_path}\")\n",
    "\n",
    "\n",
    "# Azure CLI is used to create/configure APIM; this cell is idempotent-ish.\n",
    "if not shutil.which(\"az\"):\n",
    "    print(\"‚ö†Ô∏è Azure CLI not found; cannot configure APIM via CLI.\")\n",
    "else:\n",
    "    maybe_set_subscription()\n",
    "    ensure_resource_group()\n",
    "\n",
    "    if not apim_service_exists():\n",
    "        print(f\"‚ùå APIM service not found: {APIM_SERVICE_NAME} (rg={APIM_RESOURCE_GROUP})\")\n",
    "        print(\"   Create it first (set CREATE_APIM=True in the previous cell) or point to an existing APIM.\")\n",
    "        raise RuntimeError(\"APIM service missing\")\n",
    "\n",
    "    # Validate required backend inputs\n",
    "    require_nonempty(\"BACKEND_A.endpoint\", BACKEND_A[\"endpoint\"])\n",
    "    require_nonempty(\"BACKEND_A.api_key\", BACKEND_A[\"api_key\"])\n",
    "    require_nonempty(\"BACKEND_A.deployment\", BACKEND_A[\"deployment\"])\n",
    "    require_nonempty(\"BACKEND_B.endpoint\", BACKEND_B[\"endpoint\"])\n",
    "    require_nonempty(\"BACKEND_B.api_key\", BACKEND_B[\"api_key\"])\n",
    "    require_nonempty(\"BACKEND_B.deployment\", BACKEND_B[\"deployment\"])\n",
    "\n",
    "    weight_a = int(APIM_WEIGHTS[\"A\"])\n",
    "    weight_b = int(APIM_WEIGHTS[\"B\"])\n",
    "    if weight_a < 0 or weight_b < 0 or (weight_a + weight_b) <= 0:\n",
    "        raise ValueError(\"APIM_WEIGHTS must be non-negative and sum to > 0\")\n",
    "    threshold = int(round(100.0 * (weight_a / (weight_a + weight_b))))\n",
    "\n",
    "    # Build APIM policy XML (routes by weight, rewrites to /openai/v1/responses, overrides `model`).\n",
    "    # Note: policy expressions use Newtonsoft JSON objects (JObject).\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "  <inbound>\n",
    "    <base />\n",
    "    <set-variable name=\\\"roll\\\" value=\\\"@((new System.Random()).Next(0, 100))\\\" />\n",
    "    <choose>\n",
    "      <when condition=\\\"@(((int)context.Variables[&quot;roll&quot;]) &lt; {threshold})\\\">\n",
    "        <set-variable name=\\\"backendLabel\\\" value=\\\"A\\\" />\n",
    "        <set-variable name=\\\"targetModel\\\" value=\\\"{xml_escape(BACKEND_A['deployment'])}\\\" />\n",
    "        <set-backend-service base-url=\\\"{xml_escape(BACKEND_A['endpoint'].rstrip('/'))}\\\" />\n",
    "        <set-header name=\\\"api-key\\\" exists-action=\\\"override\\\">\n",
    "          <value>{xml_escape(BACKEND_A['api_key'])}</value>\n",
    "        </set-header>\n",
    "      </when>\n",
    "      <otherwise>\n",
    "        <set-variable name=\\\"backendLabel\\\" value=\\\"B\\\" />\n",
    "        <set-variable name=\\\"targetModel\\\" value=\\\"{xml_escape(BACKEND_B['deployment'])}\\\" />\n",
    "        <set-backend-service base-url=\\\"{xml_escape(BACKEND_B['endpoint'].rstrip('/'))}\\\" />\n",
    "        <set-header name=\\\"api-key\\\" exists-action=\\\"override\\\">\n",
    "          <value>{xml_escape(BACKEND_B['api_key'])}</value>\n",
    "        </set-header>\n",
    "      </otherwise>\n",
    "    </choose>\n",
    "\n",
    "    <set-query-parameter name=\\\"api-version\\\" exists-action=\\\"override\\\">\n",
    "      <value>{RESPONSES_API_VERSION}</value>\n",
    "    </set-query-parameter>\n",
    "\n",
    "    <rewrite-uri template=\\\"/openai/responses\\\" />\n",
    "\n",
    "    <set-body><![CDATA[\n",
    "@{{\n",
    "    var body = context.Request.Body.As<Newtonsoft.Json.Linq.JObject>(preserveContent: true);\n",
    "    body[\\\"model\\\"] = (string)context.Variables[\\\"targetModel\\\"];\n",
    "    return body.ToString(Newtonsoft.Json.Formatting.None);\n",
    "}}\n",
    "    ]]></set-body>\n",
    "  </inbound>\n",
    "\n",
    "  <backend>\n",
    "    <base />\n",
    "  </backend>\n",
    "\n",
    "  <outbound>\n",
    "    <base />\n",
    "    <set-header name=\\\"x-routed-backend\\\" exists-action=\\\"override\\\">\n",
    "      <value>@((string)context.Variables.GetValueOrDefault(\\\"backendLabel\\\", \\\"unknown\\\"))</value>\n",
    "    </set-header>\n",
    "  </outbound>\n",
    "\n",
    "  <on-error>\n",
    "    <base />\n",
    "  </on-error>\n",
    "</policies>\n",
    "\"\"\"\n",
    "\n",
    "    # Create API (idempotent-ish) and operation, then apply policy\n",
    "    try:\n",
    "        run_az([\n",
    "            \"apim\", \"api\", \"create\",\n",
    "            \"-g\", APIM_RESOURCE_GROUP,\n",
    "            \"--service-name\", APIM_SERVICE_NAME,\n",
    "            \"--api-id\", APIM_API_ID,\n",
    "            \"--path\", APIM_API_PATH,\n",
    "            \"--display-name\", \"Model Migration Router\",\n",
    "            \"--protocols\", \"https\",\n",
    "            \"--service-url\", \"https://example.com\",\n",
    "            \"--subscription-required\", \"false\",\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è API create may have failed (already exists?): {e}\")\n",
    "\n",
    "    try:\n",
    "        run_az([\n",
    "            \"apim\", \"api\", \"operation\", \"create\",\n",
    "            \"-g\", APIM_RESOURCE_GROUP,\n",
    "            \"--service-name\", APIM_SERVICE_NAME,\n",
    "            \"--api-id\", APIM_API_ID,\n",
    "            \"--operation-id\", APIM_OPERATION_ID,\n",
    "            \"--display-name\", \"Responses\",\n",
    "            \"--method\", \"POST\",\n",
    "            \"--url-template\", \"/responses\",\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è Operation create may have failed (already exists?): {e}\")\n",
    "\n",
    "    apply_apim_api_policy(policy_xml)\n",
    "\n",
    "    # Re-check subscription key from environment (may have been set in APIM creation cell)\n",
    "    _current_key = os.environ.get(\"APIM_SUBSCRIPTION_KEY\", \"\") or APIM_SUBSCRIPTION_KEY\n",
    "    print(\"\\n‚úÖ APIM policy applied\")\n",
    "    print(f\"   Endpoint: https://{APIM_SERVICE_NAME}.azure-api.net/{APIM_API_PATH}/responses\")\n",
    "    if _current_key:\n",
    "        print(f\"   Using APIM_SUBSCRIPTION_KEY (first 2 chars): {_current_key[:2]}...\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No APIM_SUBSCRIPTION_KEY set; assumes subscription not required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "478df7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Using APIM subscription key...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIM URL: https://apim-model-migration.azure-api.net/inference/responses\n",
      "Runs: 10 | Failures: 0\n",
      "Routing counts:\n",
      "  A: 8 (80.0%)\n",
      "  B: 2 (20.0%)\n",
      "Latency avg: 1.347s | p50: 1.177s | p95: 1.998s\n"
     ]
    }
   ],
   "source": [
    "# Invoke APIM endpoint N times and analyze routing distribution + latency\n",
    "import time\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "APIM_BASE_URL = f\"https://{APIM_SERVICE_NAME}.azure-api.net\"\n",
    "APIM_URL = f\"{APIM_BASE_URL}/{APIM_API_PATH}/responses\"\n",
    "\n",
    "# Re-read subscription key from env (may have been set by APIM creation cell)\n",
    "_APIM_SUB_KEY = os.environ.get(\"APIM_SUBSCRIPTION_KEY\", \"\") or APIM_SUBSCRIPTION_KEY\n",
    "if _APIM_SUB_KEY:\n",
    "    print(f\"üîë Using APIM subscription key...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No APIM subscription key set. Assuming subscription not required on API.\")\n",
    "\n",
    "def percentile(data: list, p: float) -> float:\n",
    "    \"\"\"Calculate the p-th percentile of a list of numbers.\"\"\"\n",
    "    if not data:\n",
    "        return 0.0\n",
    "    sorted_data = sorted(data)\n",
    "    k = (len(sorted_data) - 1) * (p / 100.0)\n",
    "    f = int(k)\n",
    "    c = f + 1 if f + 1 < len(sorted_data) else f\n",
    "    return sorted_data[f] + (sorted_data[c] - sorted_data[f]) * (k - f)\n",
    "\n",
    "def apim_post_responses(payload: dict, timeout_s: float = 60.0) -> dict:\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if _APIM_SUB_KEY:\n",
    "        headers[\"Ocp-Apim-Subscription-Key\"] = _APIM_SUB_KEY\n",
    "    t0 = time.perf_counter()\n",
    "    resp = requests.post(APIM_URL, headers=headers, json=payload, timeout=timeout_s)\n",
    "    latency_s = time.perf_counter() - t0\n",
    "    routed = resp.headers.get(\"x-routed-backend\", \"unknown\")\n",
    "    \n",
    "    # Capture error details before raising\n",
    "    if not resp.ok:\n",
    "        error_detail = f\"{resp.status_code} {resp.reason}: {resp.text[:500]}\"\n",
    "        raise requests.HTTPError(error_detail, response=resp)\n",
    "    \n",
    "    data = resp.json()\n",
    "    return {\"latency_s\": latency_s, \"routed\": routed, \"response\": data}\n",
    "\n",
    "def extract_text_from_responses_json(data: dict) -> str:\n",
    "    # Responses API commonly returns: output[0].content[0].text\n",
    "    try:\n",
    "        out0 = (data.get(\"output\") or [])[0]\n",
    "        content0 = (out0.get(\"content\") or [])[0]\n",
    "        txt = content0.get(\"text\")\n",
    "        return txt if isinstance(txt, str) else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_usage_from_responses_json(data: dict) -> dict:\n",
    "    usage = data.get(\"usage\") or {}\n",
    "    input_tokens = int(usage.get(\"input_tokens\") or 0)\n",
    "    output_tokens = int(usage.get(\"output_tokens\") or 0)\n",
    "    cached_tokens = 0\n",
    "    details = usage.get(\"input_tokens_details\") or {}\n",
    "    if isinstance(details, dict):\n",
    "        cached_tokens = int(details.get(\"cached_tokens\") or 0)\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cached_tokens\": cached_tokens,\n",
    "    }\n",
    "\n",
    "APIM_TEST_RUNS = int(os.getenv(\"APIM_TEST_RUNS\", \"20\"))\n",
    "APIM_TIMEOUT_S = float(os.getenv(\"APIM_TIMEOUT_S\", \"60\"))\n",
    "\n",
    "# Use a small fixed prompt to keep costs predictable\n",
    "payload = {\n",
    "    \"model\": \"will-be-overridden-by-apim\",\n",
    "    \"instructions\": \"Answer briefly.\",\n",
    "    \"input\": \"Say 'ok' and a random 3-digit number.\",\n",
    "    \"max_output_tokens\": 50,\n",
    "}\n",
    "\n",
    "records = []\n",
    "failures = 0\n",
    "for i in range(APIM_TEST_RUNS):\n",
    "    try:\n",
    "        result = apim_post_responses(payload, timeout_s=APIM_TIMEOUT_S)\n",
    "        text = extract_text_from_responses_json(result[\"response\"])\n",
    "        usage = extract_usage_from_responses_json(result[\"response\"])\n",
    "        routed = result[\"routed\"]\n",
    "        model_by_backend = {\"A\": MODEL_A, \"B\": MODEL_B}\n",
    "        model_cfg = model_by_backend.get(routed)\n",
    "        cost_usd = None\n",
    "        if model_cfg and \"price_key\" in model_cfg:\n",
    "            cost_usd = calculate_cost(\n",
    "                model_cfg[\"price_key\"],\n",
    "                usage[\"input_tokens\"],\n",
    "                usage[\"output_tokens\"],\n",
    "                usage[\"cached_tokens\"],\n",
    "            )\n",
    "        records.append({\n",
    "            \"i\": i,\n",
    "            \"routed\": routed,\n",
    "            \"latency_s\": result[\"latency_s\"],\n",
    "            \"text\": text,\n",
    "            **usage,\n",
    "            \"cost_usd\": cost_usd,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        failures += 1\n",
    "        records.append({\"i\": i, \"routed\": \"error\", \"latency_s\": None, \"text\": str(e)})\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\"No APIM calls were recorded\")\n",
    "\n",
    "routed_counts = Counter([r[\"routed\"] for r in records if r.get(\"routed\")])\n",
    "latencies = [r[\"latency_s\"] for r in records if isinstance(r.get(\"latency_s\"), (int, float))]\n",
    "\n",
    "print(f\"APIM URL: {APIM_URL}\")\n",
    "print(f\"Runs: {APIM_TEST_RUNS} | Failures: {failures}\")\n",
    "print(\"Routing counts:\")\n",
    "for k, v in sorted(routed_counts.items(), key=lambda kv: kv[0]):\n",
    "    print(f\"  {k}: {v} ({v / APIM_TEST_RUNS:.1%})\")\n",
    "\n",
    "if latencies:\n",
    "    print(f\"Latency avg: {sum(latencies)/len(latencies):.3f}s | p50: {percentile(latencies, 50):.3f}s | p95: {percentile(latencies, 95):.3f}s\")\n",
    "else:\n",
    "    print(\"No successful latencies to summarize\")\n",
    "\n",
    "# Show error details if any failures occurred\n",
    "error_records = [r for r in records if r.get(\"routed\") == \"error\"]\n",
    "if error_records:\n",
    "    print(f\"\\n‚ùå Error details (first 5 of {len(error_records)}):\")\n",
    "    for r in error_records[:5]:\n",
    "        print(f\"  [{r['i']}] {r.get('text', 'unknown error')[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d379bb",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- This test is meaningful only if the APIM policy cell ran successfully and your APIM instance is reachable.\n",
    "- If you see `x-routed-backend=unknown`, verify the APIM policy was applied to the API you‚Äôre calling and that outbound headers are not stripped by another policy.\n",
    "- Increase `APIM_TEST_RUNS` (env var) for a tighter observed distribution around your configured weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e0381",
   "metadata": {},
   "source": [
    "### Acceptance Criteria Summary\n",
    "\n",
    "The following criteria are used to judge migration readiness. All thresholds can be customized via environment variables.\n",
    "\n",
    "| Metric | Criteria | Default | Env Variable |\n",
    "|--------|----------|---------|--------------|\n",
    "| **Overall Success Rate** | Must be ‚â• threshold | 98% | `AC_MIN_SUCCESS_RATE` |\n",
    "| **Overall Error Rate** | Must be ‚â§ threshold | 2% | `AC_MAX_ERROR_RATE` |\n",
    "| **Candidate p95 Latency** | Must be ‚â§ absolute max | 5.0s | `AC_MAX_P95_LATENCY_S` |\n",
    "| **Candidate p95 vs Baseline** | Increase must be ‚â§ threshold | +20% | `AC_MAX_CANDIDATE_P95_INCREASE` |\n",
    "| **Candidate Avg Cost** | Must be ‚â§ absolute max | (optional) | `AC_MAX_AVG_COST_USD` |\n",
    "| **Candidate Cost vs Baseline** | Increase must be ‚â§ threshold | (optional) | `AC_MAX_CANDIDATE_COST_INCREASE` |\n",
    "\n",
    "**Verdict Logic:**\n",
    "- ‚úÖ **PASS**: All criteria met ‚Üí safe to increase candidate traffic weight\n",
    "- ‚ùå **FAIL**: Any criterion violated ‚Üí investigate before ramping up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce5c6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Acceptance criteria evaluation\n",
      "====================================================================================================\n",
      "Baseline(A): gpt-4o (baseline) | Candidate(B): gpt-5.1 (candidate)\n",
      "Overall success rate: 100.00% | Overall error rate: 0.00%\n",
      "====================================================================================================\n",
      "| Metric | Baseline (A) | Candidate (B) | Criteria | Verdict |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Traffic share (B) | ‚Äî | 20.00% | 10.00% | (info) |\n",
      "| p95 latency | 1.547s | 2.260s | <= 5.000s and <= +20% vs A | PASS |\n",
      "| Error rate (overall) | ‚Äî | 0.00% | <= 2.00% (and success >= 98.00%) | PASS |\n",
      "| Avg cost / req | $0.000125 | $0.000101 | (optional) | PASS |\n",
      "\n",
      "‚ùå MIGRATION: FAIL\n",
      "Reasons:\n",
      " - Candidate p95 +46.1% > +20.0% vs baseline\n"
     ]
    }
   ],
   "source": [
    "# Acceptance criteria: auto-judge migration readiness from APIM test results\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "def _fmt_pct(x: float) -> str:\n",
    "    return f\"{x*100:.2f}%\"\n",
    "\n",
    "def _fmt_s(x: float) -> str:\n",
    "    return f\"{x:.3f}s\"\n",
    "\n",
    "def _fmt_usd(x: Optional[float]) -> str:\n",
    "    return \"‚Äî\" if x is None else f\"${x:.6f}\"\n",
    "\n",
    "def _safe_mean(xs: List[float]) -> Optional[float]:\n",
    "    xs2 = [x for x in xs if isinstance(x, (int, float))]\n",
    "    return (sum(xs2) / len(xs2)) if xs2 else None\n",
    "\n",
    "def _safe_p(xs: List[float], p: float) -> Optional[float]:\n",
    "    xs2 = [x for x in xs if isinstance(x, (int, float))]\n",
    "    return percentile(xs2, p) if xs2 else None\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AcceptanceCriteria:\n",
    "    max_error_rate: float = float(os.getenv(\"AC_MAX_ERROR_RATE\", \"0.02\"))  # e.g., 2%\n",
    "    max_p95_latency_s: float = float(os.getenv(\"AC_MAX_P95_LATENCY_S\", \"5.0\"))\n",
    "    max_avg_cost_usd: Optional[float] = (\n",
    "    float(os.getenv(\"AC_MAX_AVG_COST_USD\")) if os.getenv(\"AC_MAX_AVG_COST_USD\") else None\n",
    "    )\n",
    "    max_candidate_p95_increase: float = float(os.getenv(\"AC_MAX_CANDIDATE_P95_INCREASE\", \"0.20\"))  # +20%\n",
    "    max_candidate_cost_increase: Optional[float] = (\n",
    "    float(os.getenv(\"AC_MAX_CANDIDATE_COST_INCREASE\")) if os.getenv(\"AC_MAX_CANDIDATE_COST_INCREASE\") else None\n",
    "    )\n",
    "    min_success_rate: float = float(os.getenv(\"AC_MIN_SUCCESS_RATE\", \"0.98\"))  # overall successes\n",
    "\n",
    "criteria = AcceptanceCriteria()\n",
    "\n",
    "backend_labels = {\"A\": MODEL_A.get(\"label\", \"A\"), \"B\": MODEL_B.get(\"label\", \"B\")}\n",
    "expected_b_share = (APIM_WEIGHTS[\"B\"] / (APIM_WEIGHTS[\"A\"] + APIM_WEIGHTS[\"B\"])) if (APIM_WEIGHTS[\"A\"] + APIM_WEIGHTS[\"B\"]) else None\n",
    "\n",
    "def summarize_backend(records: list, backend: str) -> dict:\n",
    "    subset = [r for r in records if r.get(\"routed\") == backend]\n",
    "    n = len(subset)\n",
    "    lat = [r.get(\"latency_s\") for r in subset if isinstance(r.get(\"latency_s\"), (int, float))]\n",
    "    costs = [r.get(\"cost_usd\") for r in subset if isinstance(r.get(\"cost_usd\"), (int, float))]\n",
    "    input_tokens = [r.get(\"input_tokens\") for r in subset if isinstance(r.get(\"input_tokens\"), (int, float))]\n",
    "    output_tokens = [r.get(\"output_tokens\") for r in subset if isinstance(r.get(\"output_tokens\"), (int, float))]\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"lat_avg\": _safe_mean(lat),\n",
    "        \"lat_p50\": _safe_p(lat, 50),\n",
    "        \"lat_p95\": _safe_p(lat, 95),\n",
    "        \"avg_cost_usd\": _safe_mean(costs) if costs else None,\n",
    "        \"avg_input_tokens\": _safe_mean(input_tokens),\n",
    "        \"avg_output_tokens\": _safe_mean(output_tokens),\n",
    "    }\n",
    "\n",
    "total_runs = len(records)\n",
    "successes = len([r for r in records if r.get(\"routed\") not in (None, \"error\") and isinstance(r.get(\"latency_s\"), (int, float))])\n",
    "overall_success_rate = (successes / total_runs) if total_runs else 0.0\n",
    "overall_error_rate = 1.0 - overall_success_rate\n",
    "\n",
    "sum_a = summarize_backend(records, \"A\")\n",
    "sum_b = summarize_backend(records, \"B\")\n",
    "\n",
    "def judge() -> Tuple[bool, List[str]]:\n",
    "    reasons: List[str] = []\n",
    "    ok = True\n",
    "\n",
    "    if overall_success_rate < criteria.min_success_rate:\n",
    "        ok = False\n",
    "        reasons.append(f\"Overall success rate {_fmt_pct(overall_success_rate)} < {_fmt_pct(criteria.min_success_rate)}\")\n",
    "\n",
    "    # Absolute criteria (candidate-focused, but also checks baseline stability if present)\n",
    "    lat_p95_a = sum_a.get(\"lat_p95\")\n",
    "    lat_p95_b = sum_b.get(\"lat_p95\")\n",
    "    if lat_p95_b is not None and lat_p95_b > criteria.max_p95_latency_s:\n",
    "        ok = False\n",
    "        reasons.append(f\"Candidate p95 {_fmt_s(lat_p95_b)} > max {_fmt_s(criteria.max_p95_latency_s)}\")\n",
    "\n",
    "    # Relative p95 increase: candidate vs baseline (only if both exist)\n",
    "    if lat_p95_a is not None and lat_p95_b is not None and lat_p95_a > 0:\n",
    "        rel_inc = (lat_p95_b / lat_p95_a) - 1.0\n",
    "        if rel_inc > criteria.max_candidate_p95_increase:\n",
    "            ok = False\n",
    "            reasons.append(f\"Candidate p95 +{rel_inc*100:.1f}% > +{criteria.max_candidate_p95_increase*100:.1f}% vs baseline\")\n",
    "\n",
    "    # Cost criteria (only if we can compute cost; requires price_key mapping)\n",
    "    cost_a = sum_a.get(\"avg_cost_usd\")\n",
    "    cost_b = sum_b.get(\"avg_cost_usd\")\n",
    "    if criteria.max_avg_cost_usd is not None and cost_b is not None and cost_b > criteria.max_avg_cost_usd:\n",
    "        ok = False\n",
    "        reasons.append(f\"Candidate avg cost {_fmt_usd(cost_b)} > max {_fmt_usd(criteria.max_avg_cost_usd)}\")\n",
    "    if criteria.max_candidate_cost_increase is not None and cost_a is not None and cost_b is not None and cost_a > 0:\n",
    "        rel_cost = (cost_b / cost_a) - 1.0\n",
    "        if rel_cost > criteria.max_candidate_cost_increase:\n",
    "            ok = False\n",
    "            reasons.append(f\"Candidate cost +{rel_cost*100:.1f}% > +{criteria.max_candidate_cost_increase*100:.1f}% vs baseline\")\n",
    "\n",
    "    # Error rate criterion (overall)\n",
    "    if overall_error_rate > criteria.max_error_rate:\n",
    "        ok = False\n",
    "        reasons.append(f\"Overall error rate {_fmt_pct(overall_error_rate)} > max {_fmt_pct(criteria.max_error_rate)}\")\n",
    "\n",
    "    return ok, reasons\n",
    "\n",
    "ok, reasons = judge()\n",
    "\n",
    "def print_markdown_table(rows: list, headers: list) -> None:\n",
    "    # Simple markdown table printer (no external deps)\n",
    "    print(\"| \" + \" | \".join(headers) + \" |\")\n",
    "    print(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "    for r in rows:\n",
    "        print(\"| \" + \" | \".join(r) + \" |\")\n",
    "\n",
    "rows = []\n",
    "rows.append([\n",
    "    \"Traffic share (B)\",\n",
    "    \"‚Äî\",\n",
    "    _fmt_pct((sum_b['n'] / total_runs) if total_runs else 0.0),\n",
    "    _fmt_pct(expected_b_share) if expected_b_share is not None else \"‚Äî\",\n",
    "    \"(info)\",\n",
    "])\n",
    "rows.append([\n",
    "    \"p95 latency\",\n",
    "    _fmt_s(sum_a['lat_p95']) if sum_a['lat_p95'] is not None else \"‚Äî\",\n",
    "    _fmt_s(sum_b['lat_p95']) if sum_b['lat_p95'] is not None else \"‚Äî\",\n",
    "    f\"<= {_fmt_s(criteria.max_p95_latency_s)} and <= +{criteria.max_candidate_p95_increase*100:.0f}% vs A\",\n",
    "    \"PASS\" if (sum_b['lat_p95'] is None or sum_b['lat_p95'] <= criteria.max_p95_latency_s) else \"FAIL\",\n",
    "])\n",
    "rows.append([\n",
    "    \"Error rate (overall)\",\n",
    "    \"‚Äî\",\n",
    "    _fmt_pct(overall_error_rate),\n",
    "    f\"<= {_fmt_pct(criteria.max_error_rate)} (and success >= {_fmt_pct(criteria.min_success_rate)})\",\n",
    "    \"PASS\" if (overall_error_rate <= criteria.max_error_rate and overall_success_rate >= criteria.min_success_rate) else \"FAIL\",\n",
    "])\n",
    "rows.append([\n",
    "    \"Avg cost / req\",\n",
    "    _fmt_usd(sum_a.get('avg_cost_usd')),\n",
    "    _fmt_usd(sum_b.get('avg_cost_usd')),\n",
    "    (f\"<= {_fmt_usd(criteria.max_avg_cost_usd)}\" if criteria.max_avg_cost_usd is not None else \"(optional)\") + (f\" and <= +{criteria.max_candidate_cost_increase*100:.0f}% vs A\" if criteria.max_candidate_cost_increase is not None else \"\"),\n",
    "    \"PASS\" if (criteria.max_avg_cost_usd is None or sum_b.get('avg_cost_usd') is None or sum_b.get('avg_cost_usd') <= criteria.max_avg_cost_usd) else \"FAIL\",\n",
    "])\n",
    "\n",
    "print(\"\\nüß™ Acceptance criteria evaluation\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Baseline(A): {backend_labels['A']} | Candidate(B): {backend_labels['B']}\")\n",
    "print(f\"Overall success rate: {_fmt_pct(overall_success_rate)} | Overall error rate: {_fmt_pct(overall_error_rate)}\")\n",
    "print(\"=\" * 100)\n",
    "print_markdown_table(rows, headers=[\"Metric\", \"Baseline (A)\", \"Candidate (B)\", \"Criteria\", \"Verdict\"])\n",
    "\n",
    "print(\"\\n‚úÖ MIGRATION: PASS\" if ok else \"\\n‚ùå MIGRATION: FAIL\")\n",
    "if reasons:\n",
    "    print(\"Reasons:\")\n",
    "    for r in reasons:\n",
    "        print(\" - \" + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75154fc5",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "You now have a practical migration workflow:\n",
    "\n",
    "- A checklist for GPT-4.x ‚Üí GPT-5.x migration planning\n",
    "- Responses API migration notes (parameter and payload differences)\n",
    "- A simple **weight-based canary router** with measurable outcomes\n",
    "\n",
    "Suggested next steps:\n",
    "1. Replace the toy scenarios with your enterprise eval set and stricter validators (JSON schema, regex, unit tests).\n",
    "2. Run canary at small weight, then ramp based on acceptance criteria (accuracy, latency percentiles, cost).\n",
    "3. If you route across regions (or via APIM), capture response headers (e.g., region) during testing to validate the backend selection behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
