{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a1b3b2",
   "metadata": {},
   "source": [
    "# Observability: Tracing and Logging\n",
    "\n",
    "----\n",
    "\n",
    "This notebook focuses on **observability patterns** for AI applications.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "- **Distributed Tracing**: Track requests across services\n",
    "- **Structured Logging**: Log effectively for debugging\n",
    "- **Metrics Collection**: Monitor key performance indicators\n",
    "- **Azure Monitor Integration**: Use Azure-native observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860910d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Why Observability Matters](#why-observability-matters)\n",
    "- [Setup](#setup)\n",
    "- [Part 1: OpenTelemetry + Azure Monitor Setup](#part-1-opentelemetry--azure-monitor-setup)\n",
    "- [Part 2: Real LLM Calls with Tracing](#part-2-real-llm-calls-with-opentelemetry-tracing)\n",
    "- [Part 3: Azure Managed Grafana Dashboard](#part-3-azure-managed-grafana-dashboard)\n",
    "- [Best Practices Summary](#best-practices-summary)\n",
    "- [Cleanup Resources](#cleanup-resources)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f823",
   "metadata": {},
   "source": [
    "## Why Observability Matters\n",
    "\n",
    "### The Three Pillars of Observability\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Observability Pillars                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚      LOGS         â”‚       TRACES        â”‚       METRICS       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ What happened?    â”‚ Where did it go?    â”‚ How is it doing?    â”‚\n",
    "â”‚ â€¢ Error details   â”‚ â€¢ Request flow      â”‚ â€¢ Latency           â”‚\n",
    "â”‚ â€¢ Debug info      â”‚ â€¢ Service hops      â”‚ â€¢ Error rates       â”‚\n",
    "â”‚ â€¢ Audit trail     â”‚ â€¢ Timing breakdown  â”‚ â€¢ Throughput        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Essential AI Metrics\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Essential AI Metrics                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Performance:          Quality:           Cost:                 â”‚\n",
    "â”‚  â€¢ TTFT               â€¢ Success rate     â€¢ Tokens used          â”‚\n",
    "â”‚  â€¢ Total latency      â€¢ Error types      â€¢ $ per request        â”‚\n",
    "â”‚  â€¢ Tokens/second      â€¢ Retry count      â€¢ $ per token          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Capacity:            Reliability:                              â”‚\n",
    "â”‚  â€¢ RPM usage          â€¢ 429 rate                                â”‚\n",
    "â”‚  â€¢ TPM usage          â€¢ 5xx rate                                â”‚\n",
    "â”‚  â€¢ Queue depth        â€¢ Circuit state                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Environment Variables Required\n",
    "\n",
    "* You need to create an Azure Application Insights resource via Azure portal\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `APPLICATIONINSIGHTS_CONNECTION_STRING` | **Required**. Azure Application Insights connection string |\n",
    "| `APPLICATIONINSIGHTS_RESOURCE_ID` | **Required**. Azure Application Insights resource ID |\n",
    "| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint URL |\n",
    "| `AZURE_OPENAI_API_KEY` | Azure OpenAI API key |\n",
    "| `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME` | Deployment name for chat model |\n",
    "\n",
    "\n",
    "### AI-Specific Observability Needs\n",
    "\n",
    "| Aspect | What to Monitor | Why |\n",
    "|--------|-----------------|-----|\n",
    "| **Token Usage** | Input/output tokens | Cost tracking |\n",
    "| **Latency** | TTFT, total time | User experience |\n",
    "| **Quality** | Response relevance | Model performance |\n",
    "| **Errors** | Rate limits, failures | Reliability |\n",
    "| **Cost** | $ per request | Budget management |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28a532",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7bb3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Azure CLI found: /anaconda/envs/azureml_py38/bin//az\n",
      "âœ… PATH looks good already\n",
      "\n",
      "PATH (first 150 chars): /anaconda/envs/azureml_py38/bin/:/afh/code/agent-operator-lab/.venv/bin:/home/azureuser/.vscode-server/cli/servers/Stable-c9d77990917f3102ada88be140d2...\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from contextlib import contextmanager\n",
    "from functools import wraps\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'ğŸ” Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"âœ… Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('âœ… PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸ Could not find '{config_file}'.\")\n",
    "    print('ğŸ’¡ Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "# Project variables from config\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "\n",
    "# Azure OpenAI variables from env\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"âœ… Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nğŸ“Œ Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"ğŸ“Œ Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"ğŸ“Œ Location: {LOCATION}\")\n",
    "print(f\"ğŸ“Œ Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"ğŸ“Œ Chat deployment: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16596f",
   "metadata": {},
   "source": [
    "## Part 1: OpenTelemetry + Azure Application Insights Setup\n",
    "\n",
    "This section configures **OpenTelemetry** with **Azure Application Insights**:\n",
    "- Uses `azure-monitor-opentelemetry` distro for automatic instrumentation\n",
    "- Standard OpenTelemetry APIs for traces, metrics, and logs\n",
    "- Automatic export to Azure Monitor\n",
    "\n",
    "> **Prerequisites**: `APPLICATIONINSIGHTS_CONNECTION_STRING` `APPLICATIONINSIGHTS_RESOURCE_ID` environment variables must be set.\n",
    "\n",
    "> **Why OpenTelemetry?** It's the industry standard for observability, vendor-neutral, and deeply integrated with Azure Monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a540c232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current MeterProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Configuring Azure Monitor OpenTelemetry...\n",
      "âœ… Azure Monitor configured successfully\n",
      "\n",
      "âœ… Part 1: OpenTelemetry + Azure Monitor Setup Complete\n",
      "============================================================\n",
      "ğŸ“¦ Configured components:\n",
      "   â€¢ OpenTelemetry Tracer: <opentelemetry.sdk.trace.Tracer object at 0x74b572888750>\n",
      "   â€¢ OpenTelemetry Meter: <opentelemetry.sdk.metrics._internal.Meter object at 0x74b599e9a850>\n",
      "   â€¢ Azure Application Insights: Connected âœ…\n",
      "\n",
      "ğŸ“Š Metrics instruments created:\n",
      "   â€¢ ai_requests_total (Counter)\n",
      "   â€¢ ai_tokens_total (Counter)\n",
      "   â€¢ ai_errors_total (Counter)\n",
      "   â€¢ ai_latency_ms (Histogram)\n",
      "\n",
      "ğŸ“¤ All telemetry will be exported to Azure Application Insights\n"
     ]
    }
   ],
   "source": [
    "# Part 1: OpenTelemetry + Azure Monitor Setup\n",
    "# =============================================\n",
    "# Using azure-monitor-opentelemetry for Azure Application Insights\n",
    "import sys\n",
    "import httpx\n",
    "from openai import AzureOpenAI, RateLimitError, APIStatusError\n",
    "\n",
    "# OpenTelemetry imports\n",
    "from opentelemetry import trace, metrics\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Helper: Create mock HTTP response for error injection\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def create_mock_response(status_code: int, json_body: dict, headers: dict = None) -> httpx.Response:\n",
    "    \"\"\"Create a mock httpx.Response with a request attached for OpenAI exceptions.\"\"\"\n",
    "    request = httpx.Request(\"POST\", \"https://mock-api.openai.com/v1/chat/completions\")\n",
    "    return httpx.Response(status_code, headers=headers or {}, json=json_body, request=request)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Azure Monitor OpenTelemetry Configuration (Required)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "SERVICE_NAME = \"ai-gateway-observability\"\n",
    "CONNECTION_STRING = os.environ.get(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n",
    "\n",
    "if not CONNECTION_STRING:\n",
    "    raise ValueError(\n",
    "        \"âŒ APPLICATIONINSIGHTS_CONNECTION_STRING environment variable is required.\\n\"\n",
    "        \"   Please set it in your .env file or environment.\\n\"\n",
    "        \"   You can find it in Azure Portal â†’ Application Insights â†’ Overview â†’ Connection String\"\n",
    "    )\n",
    "\n",
    "APPLICATIONINSIGHTS_RESOURCE_ID = os.environ.get(\"APPLICATIONINSIGHTS_RESOURCE_ID\")\n",
    "\n",
    "if not APPLICATIONINSIGHTS_RESOURCE_ID:\n",
    "    raise ValueError(\n",
    "        \"âŒ APPLICATIONINSIGHTS_RESOURCE_ID environment variable is required.\\n\"\n",
    "        \"   Please set it in your .env file or environment.\\n\"\n",
    "        \"   You can find it in Azure Portal â†’ Application Insights â†’ Properties â†’ Resource ID\"\n",
    "    )\n",
    "\n",
    "print(\"ğŸ”§ Configuring Azure Monitor OpenTelemetry...\")\n",
    "configure_azure_monitor(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    logger_name=\"ai_gateway\",\n",
    "    enable_live_metrics=True,\n",
    ")\n",
    "print(f\"âœ… Azure Monitor configured successfully\")\n",
    "\n",
    "# Get tracer and meter instances\n",
    "tracer = trace.get_tracer(SERVICE_NAME, \"1.0.0\")\n",
    "meter = metrics.get_meter(SERVICE_NAME, \"1.0.0\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Create OpenTelemetry Metrics Instruments\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Counters\n",
    "request_counter = meter.create_counter(\n",
    "    name=\"ai_requests_total\",\n",
    "    description=\"Total number of AI requests\",\n",
    "    unit=\"1\"\n",
    ")\n",
    "token_counter = meter.create_counter(\n",
    "    name=\"ai_tokens_total\", \n",
    "    description=\"Total tokens used\",\n",
    "    unit=\"tokens\"\n",
    ")\n",
    "error_counter = meter.create_counter(\n",
    "    name=\"ai_errors_total\",\n",
    "    description=\"Total AI errors\",\n",
    "    unit=\"1\"\n",
    ")\n",
    "\n",
    "# Histograms\n",
    "latency_histogram = meter.create_histogram(\n",
    "    name=\"ai_latency_ms\",\n",
    "    description=\"AI request latency in milliseconds\",\n",
    "    unit=\"ms\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Helper Functions for Recording Metrics\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def record_success(model: str, input_tokens: int, output_tokens: int, latency_ms: float, span=None):\n",
    "    \"\"\"Record a successful AI request to OpenTelemetry metrics.\"\"\"\n",
    "    request_counter.add(1, {\"model\": model, \"status\": \"success\"})\n",
    "    token_counter.add(input_tokens, {\"model\": model, \"type\": \"input\"})\n",
    "    token_counter.add(output_tokens, {\"model\": model, \"type\": \"output\"})\n",
    "    latency_histogram.record(latency_ms, {\"model\": model})\n",
    "    \n",
    "    if span:\n",
    "        span.set_attribute(\"ai.input_tokens\", input_tokens)\n",
    "        span.set_attribute(\"ai.output_tokens\", output_tokens)\n",
    "        span.set_attribute(\"ai.latency_ms\", latency_ms)\n",
    "        span.set_attribute(\"ai.model\", model)\n",
    "\n",
    "\n",
    "def record_error(model: str, error_type: str, span=None):\n",
    "    \"\"\"Record an AI error to OpenTelemetry metrics.\"\"\"\n",
    "    error_counter.add(1, {\"model\": model, \"error_type\": error_type})\n",
    "    request_counter.add(1, {\"model\": model, \"status\": \"error\"})\n",
    "    \n",
    "    if span:\n",
    "        span.set_attribute(\"error.type\", error_type)\n",
    "\n",
    "\n",
    "print(\"\\nâœ… Part 1: OpenTelemetry + Azure Monitor Setup Complete\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“¦ Configured components:\")\n",
    "print(f\"   â€¢ OpenTelemetry Tracer: {tracer}\")\n",
    "print(f\"   â€¢ OpenTelemetry Meter: {meter}\")\n",
    "print(f\"   â€¢ Azure Application Insights: Connected âœ…\")\n",
    "print(\"\\nğŸ“Š Metrics instruments created:\")\n",
    "print(\"   â€¢ ai_requests_total (Counter)\")\n",
    "print(\"   â€¢ ai_tokens_total (Counter)\")\n",
    "print(\"   â€¢ ai_errors_total (Counter)\")\n",
    "print(\"   â€¢ ai_latency_ms (Histogram)\")\n",
    "print(\"\\nğŸ“¤ All telemetry will be exported to Azure Application Insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769415ea",
   "metadata": {},
   "source": [
    "## Part 2: Real LLM Calls with OpenTelemetry Tracing\n",
    "\n",
    "Using OpenTelemetry to trace real LLM calls including:\n",
    "- **Basic Chat Completion**: Simple Q&A requests\n",
    "- **Function Calling (Tool Use)**: Calculate monthly computing usage\n",
    "- **Remote MCP Calls**: Query Azure documentation via MCP server\n",
    "- **Error injection** to demonstrate error tracing (429, 500, 502, 503)\n",
    "\n",
    "### Request Flow with Tracing\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Multi-Modal LLM Request Flow                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  user-1~4: Basic Chat Completion (with error injection)            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚  â”‚  User    â”‚â”€â”€â”€â–¶â”‚   LLM    â”‚â”€â”€â”€â–¶â”‚ Response â”‚                      â”‚\n",
    "â”‚  â”‚  Query   â”‚    â”‚  (GPT-4) â”‚    â”‚ or Error â”‚                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚       â”‚              Error Injection: 429 â†’ 500 â†’ 502 â†’ 503        â”‚\n",
    "â”‚       â”‚                                                             â”‚\n",
    "â”‚  user-5~8: More Chat Completions (success after errors)            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚  â”‚  User    â”‚â”€â”€â”€â–¶â”‚   LLM    â”‚â”€â”€â”€â–¶â”‚ Response â”‚                      â”‚\n",
    "â”‚  â”‚  Query   â”‚    â”‚  (GPT-4) â”‚    â”‚          â”‚                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  user-9: Function Calling (Tool Use)                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚  â”‚  User    â”‚â”€â”€â”€â–¶â”‚   LLM    â”‚â”€â”€â”€â–¶â”‚ Function â”‚â”€â”€â”€â–¶â”‚ Response â”‚     â”‚\n",
    "â”‚  â”‚  Query   â”‚    â”‚  (GPT-4) â”‚    â”‚  Call    â”‚    â”‚          â”‚     â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  user-10: MCP Server Call (Azure Documentation)                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚  â”‚  User    â”‚â”€â”€â”€â–¶â”‚  Agent   â”‚â”€â”€â”€â–¶â”‚   MCP    â”‚â”€â”€â”€â–¶â”‚ Response â”‚     â”‚\n",
    "â”‚  â”‚  Query   â”‚    â”‚          â”‚    â”‚  Server  â”‚    â”‚          â”‚     â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Error Scenarios Tested\n",
    "\n",
    "| Error Code | Description | Use Case |\n",
    "|------------|-------------|----------|\n",
    "| **429** | Rate Limit Exceeded | Too many requests |\n",
    "| **500** | Internal Server Error | Backend failure |\n",
    "| **502** | Bad Gateway | Proxy/gateway issue |\n",
    "| **503** | Service Unavailable | Service down |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a082a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Part 2: Real LLM Calls with OpenTelemetry Tracing\n",
      "============================================================\n",
      "   Includes: Basic Chat, Function Calling, MCP Server Calls\n",
      "   Error Scenarios: 429, 500, 502, 503\n",
      "\n",
      "ğŸ“ Making LLM requests with OpenTelemetry tracing:\n",
      "------------------------------------------------------------\n",
      "   Phase 1: Error injection (429 â†’ 500 â†’ 502 â†’ 503)\n",
      "   Phase 2: Successful completions\n",
      "   Phase 3: Function calling & MCP server call\n",
      "------------------------------------------------------------\n",
      "   âš ï¸  1. user-1   [chat]: Rate limited (429)\n",
      "       â””â”€ Trace: 3d44b2f074dbb4c774197b2bb6adf108 [ERROR]\n",
      "   âŒ  2. user-2   [chat]: Server error (500)\n",
      "       â””â”€ Trace: bf095fac94e201502f571d763858fdeb [ERROR]\n",
      "   âŒ  3. user-3   [chat]: Server error (502)\n",
      "       â””â”€ Trace: 0eee1a4bca195cde7e5315d4c154521c [ERROR]\n",
      "   âŒ  4. user-4   [chat]: Server error (503)\n",
      "       â””â”€ Trace: af10e5b5000c438701e737a6c49bf818 [ERROR]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ…  5. user-5   [chat]: '**Machine learning** is a branch of arti...'\n",
      "       â””â”€ Trace: 5c1db7a3909c5abe09efdd65b652141b\n",
      "   âœ…  6. user-6   [chat]: '**Deep learning** is a subfield of *mach...'\n",
      "       â””â”€ Trace: 0d4d770dd61a29ac9b90848983626d64\n",
      "   âœ…  7. user-7   [chat]: 'A **neural network** is a type of comput...'\n",
      "       â””â”€ Trace: 2980fee4052e7756c7ba720cbb6bb357\n",
      "   âœ…  8. user-8   [chat]: '**AI** stands for **Artificial Intellige...'\n",
      "       â””â”€ Trace: e605f9afccdfa403279419467911571a\n",
      "   ğŸ”§  9. user-9   [function_call]: Tool calls=1\n",
      "       â””â”€ calculate_monthly_usage: users=50, vcpus=35, monthly_hrs=5500\n",
      "       â””â”€ Response: 'Let's break down the calculation and recommendation:\n",
      "\n",
      "1. Tot...'\n",
      "       â””â”€ Trace: ca229987626bda22d5f1b4f9b3804c72\n",
      "   ğŸŒ 10. user-10  [mcp_call]: MCP Server Response\n",
      "       â””â”€ Response: 'Given your requirement of **~8 vCPUs for stable operation** and **~5,500 compute...'\n",
      "       â””â”€ Trace: dfa83bbf35f828f8f3358e650ad59a6e\n",
      "\n",
      "ğŸ“‹ Telemetry Summary:\n",
      "------------------------------------------------------------\n",
      "   Total requests: 10\n",
      "   âœ… Successful:   6\n",
      "   âŒ Failed:       4\n",
      "   Success rate:   60.0%\n",
      "   Tokens:         206 in / 427 out\n",
      "   Avg latency:    3685ms\n",
      "\n",
      "   Error breakdown:\n",
      "      â€¢ APIStatusError_500: 1\n",
      "      â€¢ APIStatusError_502: 1\n",
      "      â€¢ APIStatusError_503: 1\n",
      "      â€¢ RateLimitError_429: 1\n",
      "\n",
      "ğŸ“Š Request Type Breakdown:\n",
      "   â€¢ Chat completions: 8 (4 errors: 429/500/502/503, 4 success)\n",
      "   â€¢ Function calls:   1 (with tool execution)\n",
      "   â€¢ MCP server calls: 1 (remote documentation query)\n",
      "\n",
      "ğŸ“¤ Telemetry automatically exported to Azure Application Insights!\n",
      "   View traces in: Azure Portal â†’ Application Insights â†’ Transaction search\n",
      "   View errors in: Azure Portal â†’ Application Insights â†’ Failures\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Real LLM Calls with OpenTelemetry Tracing\n",
    "# ===================================================\n",
    "# Includes: Basic Chat, Function Calling, MCP Server Calls, and Error Scenarios\n",
    "\n",
    "import asyncio\n",
    "from agent_framework import MCPStreamableHTTPTool\n",
    "from agent_framework.azure import AzureAIClient\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Function Definitions for Tool Use (Function Calling)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def calculate_monthly_usage(num_users: int, morning_hours: float, afternoon_hours: float, days_per_month: int = 22) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate monthly computing usage based on user activity patterns.\n",
    "    \n",
    "    Args:\n",
    "        num_users: Number of active users\n",
    "        morning_hours: Average usage hours per user in the morning (9AM-12PM)\n",
    "        afternoon_hours: Average usage hours per user in the afternoon (1PM-6PM)\n",
    "        days_per_month: Working days per month (default: 22)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with usage statistics\n",
    "    \"\"\"\n",
    "    daily_hours_per_user = morning_hours + afternoon_hours\n",
    "    monthly_hours_per_user = daily_hours_per_user * days_per_month\n",
    "    total_monthly_hours = monthly_hours_per_user * num_users\n",
    "    \n",
    "    # Estimate compute units (1 hour = 1 vCPU-hour)\n",
    "    peak_concurrent_users = int(num_users * 0.7)  # 70% peak concurrency\n",
    "    recommended_vcpus = max(2, peak_concurrent_users)  # Minimum 2 vCPUs\n",
    "    \n",
    "    return {\n",
    "        \"num_users\": num_users,\n",
    "        \"daily_hours_per_user\": daily_hours_per_user,\n",
    "        \"monthly_hours_per_user\": monthly_hours_per_user,\n",
    "        \"total_monthly_hours\": total_monthly_hours,\n",
    "        \"peak_concurrent_users\": peak_concurrent_users,\n",
    "        \"recommended_vcpus\": recommended_vcpus,\n",
    "        \"estimated_monthly_cost_usd\": round(total_monthly_hours * 0.05, 2)  # ~$0.05/vCPU-hour\n",
    "    }\n",
    "\n",
    "\n",
    "# Tool definition for OpenAI function calling\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_monthly_usage\",\n",
    "            \"description\": \"Calculate monthly computing resource usage based on number of users and their average daily usage patterns (morning and afternoon hours)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"num_users\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of active users\"\n",
    "                    },\n",
    "                    \"morning_hours\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Average usage hours per user in the morning (9AM-12PM)\"\n",
    "                    },\n",
    "                    \"afternoon_hours\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Average usage hours per user in the afternoon (1PM-6PM)\"\n",
    "                    },\n",
    "                    \"days_per_month\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Working days per month (default: 22)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"num_users\", \"morning_hours\", \"afternoon_hours\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Multi-Error Injector (429, 500, 502, 503)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "class MultiErrorInjector:\n",
    "    \"\"\"\n",
    "    Inject various error types for comprehensive observability testing.\n",
    "    Sequence: 429 â†’ 500 â†’ 502 â†’ 503 â†’ Success â†’ Success â†’ ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client: AzureOpenAI):\n",
    "        self.client = client\n",
    "        self.call_count = 0\n",
    "        self.error_sequence = [\n",
    "            (429, \"Rate limit exceeded\", {\"retry-after\": \"2\"}),\n",
    "            (500, \"Internal server error\", {}),\n",
    "            (502, \"Bad gateway\", {}),\n",
    "            (503, \"Service temporarily unavailable\", {}),\n",
    "        ]\n",
    "    \n",
    "    def create_completion(self, **kwargs):\n",
    "        self.call_count += 1\n",
    "        \n",
    "        if self.call_count <= len(self.error_sequence):\n",
    "            code, msg, headers = self.error_sequence[self.call_count - 1]\n",
    "            resp = create_mock_response(code, {\"error\": {\"message\": msg}}, headers)\n",
    "            \n",
    "            if code == 429:\n",
    "                raise RateLimitError(msg, response=resp, body=resp.json())\n",
    "            else:\n",
    "                raise APIStatusError(msg, response=resp, body=resp.json())\n",
    "        \n",
    "        return self.client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Function Call Handler with Tracing\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def handle_function_call_with_tracing(client: AzureOpenAI, messages: list, user: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Handle function calling flow with OpenTelemetry tracing.\n",
    "    Returns (final_response, tool_calls_made)\n",
    "    \"\"\"\n",
    "    tool_calls_made = []\n",
    "    \n",
    "    with tracer.start_as_current_span(\"function_call_flow\") as flow_span:\n",
    "        flow_span.set_attribute(\"user\", user)\n",
    "        \n",
    "        # Step 1: Initial LLM call with tools\n",
    "        with tracer.start_as_current_span(\"llm_tool_selection\") as tool_span:\n",
    "            start = time.time()\n",
    "            response = client.chat.completions.create(\n",
    "                model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                messages=messages,\n",
    "                tools=TOOLS,\n",
    "                tool_choice=\"auto\",\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            tool_span.set_attribute(\"latency_ms\", (time.time() - start) * 1000)\n",
    "            tool_span.set_attribute(\"has_tool_calls\", bool(response.choices[0].message.tool_calls))\n",
    "        \n",
    "        # Step 2: Process tool calls if any\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            messages.append(response.choices[0].message)\n",
    "            \n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                with tracer.start_as_current_span(\"tool_execution\") as exec_span:\n",
    "                    exec_span.set_attribute(\"tool_name\", tool_call.function.name)\n",
    "                    exec_span.set_attribute(\"tool_id\", tool_call.id)\n",
    "                    \n",
    "                    # Parse arguments and execute function\n",
    "                    import json as json_module\n",
    "                    args = json_module.loads(tool_call.function.arguments)\n",
    "                    exec_span.set_attribute(\"tool_args\", str(args))\n",
    "                    \n",
    "                    if tool_call.function.name == \"calculate_monthly_usage\":\n",
    "                        result = calculate_monthly_usage(**args)\n",
    "                        tool_calls_made.append({\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"args\": args,\n",
    "                            \"result\": result\n",
    "                        })\n",
    "                        exec_span.set_attribute(\"result.recommended_vcpus\", result[\"recommended_vcpus\"])\n",
    "                        exec_span.set_attribute(\"result.total_monthly_hours\", result[\"total_monthly_hours\"])\n",
    "                    else:\n",
    "                        result = {\"error\": f\"Unknown function: {tool_call.function.name}\"}\n",
    "                    \n",
    "                    # Add tool result to messages\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": json_module.dumps(result)\n",
    "                    })\n",
    "            \n",
    "            # Step 3: Final LLM call with tool results\n",
    "            with tracer.start_as_current_span(\"llm_final_response\") as final_span:\n",
    "                start = time.time()\n",
    "                final_response = client.chat.completions.create(\n",
    "                    model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                    messages=messages,\n",
    "                    max_tokens=500,\n",
    "                )\n",
    "                final_span.set_attribute(\"latency_ms\", (time.time() - start) * 1000)\n",
    "                \n",
    "            return final_response, tool_calls_made\n",
    "        \n",
    "        return response, tool_calls_made\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MCP Server Call Handler with Tracing\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "async def call_mcp_server_with_tracing(query: str, user: str) -> str:\n",
    "    \"\"\"\n",
    "    Call Microsoft Learn MCP server with OpenTelemetry tracing.\n",
    "    \"\"\"\n",
    "    with tracer.start_as_current_span(\"mcp_server_call\") as mcp_span:\n",
    "        mcp_span.set_attribute(\"user\", user)\n",
    "        mcp_span.set_attribute(\"query\", query[:100])\n",
    "        mcp_span.set_attribute(\"mcp.server_url\", \"https://learn.microsoft.com/api/mcp\")\n",
    "        \n",
    "        try:\n",
    "            async with (\n",
    "                MCPStreamableHTTPTool(\n",
    "                    name=\"Microsoft Learn MCP\",\n",
    "                    url=\"https://learn.microsoft.com/api/mcp\",\n",
    "                ) as mcp_docs,\n",
    "                AzureAIClient(\n",
    "                    credential=credential, \n",
    "                    project_endpoint=AZURE_AI_PROJECT_ENDPOINT\n",
    "                ).create_agent(\n",
    "                    name=\"AzureDocsAgent\",\n",
    "                    instructions=\"You help with Azure computing resource recommendations based on usage calculations. Be concise.\",\n",
    "                    tools=mcp_docs,\n",
    "                ) as agent,\n",
    "            ):\n",
    "                with tracer.start_as_current_span(\"mcp_agent_run\") as agent_span:\n",
    "                    start = time.time()\n",
    "                    result = await agent.run(query)\n",
    "                    latency_ms = (time.time() - start) * 1000\n",
    "                    \n",
    "                    agent_span.set_attribute(\"latency_ms\", latency_ms)\n",
    "                    agent_span.set_attribute(\"response_length\", len(str(result)))\n",
    "                    \n",
    "                    mcp_span.set_status(Status(StatusCode.OK))\n",
    "                    return str(result)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            mcp_span.set_status(Status(StatusCode.ERROR, str(e)[:100]))\n",
    "            mcp_span.record_exception(e)\n",
    "            return f\"MCP Error: {str(e)[:200]}\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"ğŸ“Š Part 2: Real LLM Calls with OpenTelemetry Tracing\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   Includes: Basic Chat, Function Calling, MCP Server Calls\")\n",
    "print(\"   Error Scenarios: 429, 500, 502, 503\")\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    # Initialize OpenAI client\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    \n",
    "    # Multi-error injector: 429 â†’ 500 â†’ 502 â†’ 503 â†’ Success...\n",
    "    injector = MultiErrorInjector(client)\n",
    "    \n",
    "    # Local stats for summary (not exported - just for notebook display)\n",
    "    local_stats = {\"requests\": 0, \"errors\": 0, \"tokens_in\": 0, \"tokens_out\": 0, \"latencies\": []}\n",
    "    \n",
    "    # Extended request data with different request types and error scenarios\n",
    "    requests_data = [\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Error Injection Phase (4 errors: 429, 500, 502, 503)\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\"prompt\": \"What is Python?\", \"user\": \"user-1\", \"type\": \"chat\", \"expected\": \"429 Rate Limit\"},\n",
    "        {\"prompt\": \"What is Azure?\", \"user\": \"user-2\", \"type\": \"chat\", \"expected\": \"500 Internal\"},\n",
    "        {\"prompt\": \"What is OpenAI?\", \"user\": \"user-3\", \"type\": \"chat\", \"expected\": \"502 Bad Gateway\"},\n",
    "        {\"prompt\": \"Explain cloud computing\", \"user\": \"user-4\", \"type\": \"chat\", \"expected\": \"503 Unavailable\"},\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Success Phase (after errors exhausted)\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\"prompt\": \"What is machine learning?\", \"user\": \"user-5\", \"type\": \"chat\", \"expected\": \"Success\"},\n",
    "        {\"prompt\": \"What is deep learning?\", \"user\": \"user-6\", \"type\": \"chat\", \"expected\": \"Success\"},\n",
    "        {\"prompt\": \"What is neural network?\", \"user\": \"user-7\", \"type\": \"chat\", \"expected\": \"Success\"},\n",
    "        {\"prompt\": \"What is AI?\", \"user\": \"user-8\", \"type\": \"chat\", \"expected\": \"Success\"},\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Function Calling - calculate computing usage\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"prompt\": \"Our team has 50 users. On average, they use computing resources for 2 hours in the morning and 3 hours in the afternoon. Calculate the total monthly usage and recommend the number of vCPUs needed for stable operation.\",\n",
    "            \"user\": \"user-9\",\n",
    "            \"type\": \"function_call\",\n",
    "            \"expected\": \"Function Call\"\n",
    "        },\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # MCP Server Call - Azure documentation query\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"prompt\": \"Based on a team of 50 users needing 8 vCPUs for stable operation with approximately 5,500 compute hours per month, what Azure VM size would you recommend? Consider cost optimization.\",\n",
    "            \"user\": \"user-10\",\n",
    "            \"type\": \"mcp_call\",\n",
    "            \"expected\": \"MCP Call\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ“ Making LLM requests with OpenTelemetry tracing:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"   Phase 1: Error injection (429 â†’ 500 â†’ 502 â†’ 503)\")\n",
    "    print(\"   Phase 2: Successful completions\")\n",
    "    print(\"   Phase 3: Function calling & MCP server call\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    error_breakdown = {}\n",
    "    \n",
    "    for i, req in enumerate(requests_data, 1):\n",
    "        request_id = f\"req-{uuid.uuid4().hex[:8]}\"\n",
    "        request_type = req.get(\"type\", \"chat\")\n",
    "        expected = req.get(\"expected\", \"\")\n",
    "        \n",
    "        # Use OpenTelemetry tracing directly\n",
    "        with tracer.start_as_current_span(\n",
    "            f\"{request_type}_completion\",\n",
    "            attributes={\n",
    "                \"request_id\": request_id,\n",
    "                \"user\": req[\"user\"],\n",
    "                \"request_type\": request_type,\n",
    "                \"prompt_length\": len(req[\"prompt\"]),\n",
    "            }\n",
    "        ) as span:\n",
    "            local_stats[\"requests\"] += 1\n",
    "            \n",
    "            try:\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                # Type 1: Basic Chat Completion (with error injection)\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                if request_type == \"chat\":\n",
    "                    with tracer.start_as_current_span(\"validate_input\") as validate_span:\n",
    "                        validate_span.set_attribute(\"input_length\", len(req[\"prompt\"]))\n",
    "                    \n",
    "                    with tracer.start_as_current_span(\"llm_call\", attributes={\"model\": AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}) as llm_span:\n",
    "                        start = time.time()\n",
    "                        response = injector.create_completion(\n",
    "                            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                            messages=[{\"role\": \"user\", \"content\": req[\"prompt\"]}],\n",
    "                            max_tokens=50,\n",
    "                        )\n",
    "                        latency_ms = (time.time() - start) * 1000\n",
    "                        \n",
    "                        # Record metrics directly via OpenTelemetry\n",
    "                        record_success(\n",
    "                            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                            input_tokens=response.usage.prompt_tokens,\n",
    "                            output_tokens=response.usage.completion_tokens,\n",
    "                            latency_ms=latency_ms,\n",
    "                            span=llm_span\n",
    "                        )\n",
    "                        \n",
    "                        # Update local stats for summary\n",
    "                        local_stats[\"tokens_in\"] += response.usage.prompt_tokens\n",
    "                        local_stats[\"tokens_out\"] += response.usage.completion_tokens\n",
    "                        local_stats[\"latencies\"].append(latency_ms)\n",
    "                    \n",
    "                    answer = response.choices[0].message.content[:40]\n",
    "                    print(f\"   âœ… {i:2d}. {req['user']:8} [chat]: '{answer}...'\")\n",
    "                    print(f\"       â””â”€ Trace: {span.get_span_context().trace_id:032x}\")\n",
    "                \n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                # Type 2: Function Calling (Tool Use)\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                elif request_type == \"function_call\":\n",
    "                    messages = [{\"role\": \"user\", \"content\": req[\"prompt\"]}]\n",
    "                    \n",
    "                    start = time.time()\n",
    "                    response, tool_calls = handle_function_call_with_tracing(client, messages, req[\"user\"])\n",
    "                    latency_ms = (time.time() - start) * 1000\n",
    "                    \n",
    "                    # Record metrics directly via OpenTelemetry\n",
    "                    record_success(\n",
    "                        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                        input_tokens=response.usage.prompt_tokens,\n",
    "                        output_tokens=response.usage.completion_tokens,\n",
    "                        latency_ms=latency_ms,\n",
    "                        span=span\n",
    "                    )\n",
    "                    \n",
    "                    # Update local stats\n",
    "                    local_stats[\"tokens_in\"] += response.usage.prompt_tokens\n",
    "                    local_stats[\"tokens_out\"] += response.usage.completion_tokens\n",
    "                    local_stats[\"latencies\"].append(latency_ms)\n",
    "                    \n",
    "                    span.set_attribute(\"tool_calls_count\", len(tool_calls))\n",
    "                    \n",
    "                    answer = response.choices[0].message.content[:60] if response.choices[0].message.content else \"No content\"\n",
    "                    print(f\"   ğŸ”§ {i:2d}. {req['user']:8} [function_call]: Tool calls={len(tool_calls)}\")\n",
    "                    \n",
    "                    if tool_calls:\n",
    "                        for tc in tool_calls:\n",
    "                            print(f\"       â””â”€ {tc['name']}: users={tc['args'].get('num_users')}, \"\n",
    "                                  f\"vcpus={tc['result'].get('recommended_vcpus')}, \"\n",
    "                                  f\"monthly_hrs={tc['result'].get('total_monthly_hours')}\")\n",
    "                    \n",
    "                    print(f\"       â””â”€ Response: '{answer}...'\")\n",
    "                    print(f\"       â””â”€ Trace: {span.get_span_context().trace_id:032x}\")\n",
    "                \n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                # Type 3: MCP Server Call\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                elif request_type == \"mcp_call\":\n",
    "                    span.set_attribute(\"mcp.server\", \"Microsoft Learn\")\n",
    "                    \n",
    "                    start = time.time()\n",
    "                    # Run async MCP call\n",
    "                    mcp_result = await call_mcp_server_with_tracing(req[\"prompt\"], req[\"user\"])\n",
    "                    latency_ms = (time.time() - start) * 1000\n",
    "                    \n",
    "                    # Record latency (MCP doesn't return token counts)\n",
    "                    latency_histogram.record(latency_ms, {\"model\": \"mcp_agent\"})\n",
    "                    local_stats[\"latencies\"].append(latency_ms)\n",
    "                    \n",
    "                    span.set_attribute(\"latency_ms\", latency_ms)\n",
    "                    span.set_attribute(\"mcp_response_length\", len(mcp_result))\n",
    "                    \n",
    "                    answer = mcp_result[:80] if mcp_result else \"No response\"\n",
    "                    print(f\"   ğŸŒ {i:2d}. {req['user']:8} [mcp_call]: MCP Server Response\")\n",
    "                    print(f\"       â””â”€ Response: '{answer}...'\")\n",
    "                    print(f\"       â””â”€ Trace: {span.get_span_context().trace_id:032x}\")\n",
    "                \n",
    "            except RateLimitError as e:\n",
    "                error_type = \"RateLimitError_429\"\n",
    "                error_breakdown[error_type] = error_breakdown.get(error_type, 0) + 1\n",
    "                local_stats[\"errors\"] += 1\n",
    "                record_error(AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, error_type, span)\n",
    "                span.set_status(Status(StatusCode.ERROR, \"Rate limit exceeded\"))\n",
    "                print(f\"   âš ï¸ {i:2d}. {req['user']:8} [{request_type}]: Rate limited (429)\")\n",
    "                print(f\"       â””â”€ Trace: {span.get_span_context().trace_id:032x} [ERROR]\")\n",
    "                \n",
    "            except APIStatusError as e:\n",
    "                error_type = f\"APIStatusError_{e.status_code}\"\n",
    "                error_breakdown[error_type] = error_breakdown.get(error_type, 0) + 1\n",
    "                local_stats[\"errors\"] += 1\n",
    "                record_error(AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, error_type, span)\n",
    "                span.set_status(Status(StatusCode.ERROR, str(e)[:50]))\n",
    "                print(f\"   âŒ {i:2d}. {req['user']:8} [{request_type}]: Server error ({e.status_code})\")\n",
    "                print(f\"       â””â”€ Trace: {span.get_span_context().trace_id:032x} [ERROR]\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_type = f\"Exception_{type(e).__name__}\"\n",
    "                error_breakdown[error_type] = error_breakdown.get(error_type, 0) + 1\n",
    "                local_stats[\"errors\"] += 1\n",
    "                record_error(AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, error_type, span)\n",
    "                span.set_status(Status(StatusCode.ERROR, str(e)[:50]))\n",
    "                span.record_exception(e)\n",
    "                print(f\"   âŒ {i:2d}. {req['user']:8} [{request_type}]: {type(e).__name__}: {str(e)[:50]}\")\n",
    "                print(f\"       â””â”€ Trace: {span.get_span_context().trace_id:032x} [ERROR]\")\n",
    "    \n",
    "    # Summary\n",
    "    total_requests = local_stats['requests']\n",
    "    total_errors = local_stats['errors']\n",
    "    success_rate = ((total_requests - total_errors) / total_requests * 100) if total_requests > 0 else 0\n",
    "    avg_latency = sum(local_stats[\"latencies\"]) / len(local_stats[\"latencies\"]) if local_stats[\"latencies\"] else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Telemetry Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   Total requests: {total_requests}\")\n",
    "    print(f\"   âœ… Successful:   {total_requests - total_errors}\")\n",
    "    print(f\"   âŒ Failed:       {total_errors}\")\n",
    "    print(f\"   Success rate:   {success_rate:.1f}%\")\n",
    "    print(f\"   Tokens:         {local_stats['tokens_in']} in / {local_stats['tokens_out']} out\")\n",
    "    print(f\"   Avg latency:    {avg_latency:.0f}ms\")\n",
    "    \n",
    "    if error_breakdown:\n",
    "        print(f\"\\n   Error breakdown:\")\n",
    "        for err_type, count in sorted(error_breakdown.items()):\n",
    "            print(f\"      â€¢ {err_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Request Type Breakdown:\")\n",
    "    print(f\"   â€¢ Chat completions: 8 (4 errors: 429/500/502/503, 4 success)\")\n",
    "    print(f\"   â€¢ Function calls:   1 (with tool execution)\")\n",
    "    print(f\"   â€¢ MCP server calls: 1 (remote documentation query)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“¤ Telemetry automatically exported to Azure Application Insights!\")\n",
    "    print(f\"   View traces in: Azure Portal â†’ Application Insights â†’ Transaction search\")\n",
    "    print(f\"   View errors in: Azure Portal â†’ Application Insights â†’ Failures\")\n",
    "else:\n",
    "    print(\"âš ï¸ AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49817cc2",
   "metadata": {},
   "source": [
    "### Simulation Results on Azure Application Insights (End-to-end transaction details)\n",
    "\n",
    "![../images/simulation_result_application_insight.png](../images/simulation_result_application_insight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6db7d",
   "metadata": {},
   "source": [
    "## Part 3: Grafana Visualization\n",
    "\n",
    "OpenTelemetry metrics exported to Azure Monitor can be visualized in Grafana via the Azure Monitor data source.\n",
    "\n",
    "### 3.1: Azure Managed Grafana Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4dd87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Part 3.1: Azure Managed Grafana Setup\n",
      "============================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚              OpenTelemetry â†’ Azure Monitor â†’ Grafana                â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
      "â”‚  â”‚ Application â”‚â”€â”€â”€â–¶â”‚  Azure Monitor   â”‚â”€â”€â”€â–¶â”‚    Grafana      â”‚    â”‚\n",
      "â”‚  â”‚ OpenTelemetryâ”‚    â”‚ (App Insights)   â”‚    â”‚  (Dashboards)   â”‚    â”‚\n",
      "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  OpenTelemetry SDK exports:                                         â”‚\n",
      "â”‚  â€¢ Traces â†’ customEvents, dependencies, requests                    â”‚\n",
      "â”‚  â€¢ Metrics â†’ customMetrics                                          â”‚\n",
      "â”‚  â€¢ Logs â†’ traces table                                              â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  Grafana queries via Azure Monitor Data Source:                     â”‚\n",
      "â”‚  â€¢ KQL queries against Log Analytics                                â”‚\n",
      "â”‚  â€¢ customMetrics | where name == \"ai_requests_total\"                â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ’¡ Set CREATE_GRAFANA=True to create a new Grafana workspace.\n",
      "   Or use an existing Grafana instance.\n",
      "\n",
      "ğŸ“ To connect existing Grafana to Application Insights:\n",
      "   1. Add Azure Monitor data source in Grafana\n",
      "   2. Configure with your subscription and App Insights resource\n",
      "   3. Query OpenTelemetry metrics from customMetrics table\n"
     ]
    }
   ],
   "source": [
    "# Part 3.1: Azure Managed Grafana Setup\n",
    "# ======================================\n",
    "# OpenTelemetry metrics exported to Azure Monitor can be visualized in Grafana\n",
    "\n",
    "CREATE_GRAFANA = False  # Set to True to create a new Grafana workspace\n",
    "\n",
    "print(\"ğŸ“Š Part 3.1: Azure Managed Grafana Setup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              OpenTelemetry â†’ Azure Monitor â†’ Grafana                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Application â”‚â”€â”€â”€â–¶â”‚  Azure Monitor   â”‚â”€â”€â”€â–¶â”‚    Grafana      â”‚    â”‚\n",
    "â”‚  â”‚ OpenTelemetryâ”‚    â”‚ (App Insights)   â”‚    â”‚  (Dashboards)   â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  OpenTelemetry SDK exports:                                         â”‚\n",
    "â”‚  â€¢ Traces â†’ customEvents, dependencies, requests                    â”‚\n",
    "â”‚  â€¢ Metrics â†’ customMetrics                                          â”‚\n",
    "â”‚  â€¢ Logs â†’ traces table                                              â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  Grafana queries via Azure Monitor Data Source:                     â”‚\n",
    "â”‚  â€¢ KQL queries against Log Analytics                                â”‚\n",
    "â”‚  â€¢ customMetrics | where name == \"ai_requests_total\"                â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# Grafana configuration - check if already exists in environment\n",
    "EXISTING_GRAFANA_NAME = os.environ.get(\"GRAFANA_NAME\")\n",
    "GRAFANA_NAME = EXISTING_GRAFANA_NAME or f\"grafana-ot-{LOCATION[:4]}\"\n",
    "GRAFANA_LOCATION = os.environ.get(\"AZURE_LOCATION\", \"eastus2\")\n",
    "\n",
    "\n",
    "def run_az(args: list) -> str:\n",
    "    \"\"\"Run Azure CLI command and return output.\"\"\"\n",
    "    result = subprocess.run([\"az\"] + args, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"Azure CLI error: {result.stderr}\")\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def get_grafana_workspace(name: str) -> dict:\n",
    "    \"\"\"Get existing Azure Managed Grafana workspace info.\"\"\"\n",
    "    try:\n",
    "        output = run_az([\n",
    "            \"grafana\", \"show\",\n",
    "            \"-g\", RESOURCE_GROUP,\n",
    "            \"-n\", name,\n",
    "            \"-o\", \"json\"\n",
    "        ])\n",
    "        return json.loads(output)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def setup_grafana_workspace() -> dict:\n",
    "    \"\"\"Create Azure Managed Grafana workspace.\"\"\"\n",
    "    print(f\"ğŸ”§ Creating Azure Managed Grafana: {GRAFANA_NAME}\")\n",
    "    print(f\"   Resource Group: {RESOURCE_GROUP}\")\n",
    "    print(f\"   Location: {GRAFANA_LOCATION}\")\n",
    "    \n",
    "    try:\n",
    "        output = run_az([\n",
    "            \"grafana\", \"create\",\n",
    "            \"-g\", RESOURCE_GROUP,\n",
    "            \"-n\", GRAFANA_NAME,\n",
    "            \"-l\", GRAFANA_LOCATION,\n",
    "            \"-o\", \"json\"\n",
    "        ])\n",
    "        grafana_info = json.loads(output)\n",
    "        \n",
    "        endpoint = grafana_info.get(\"properties\", {}).get(\"endpoint\", \"N/A\")\n",
    "        \n",
    "        print(f\"   âœ… Grafana workspace created!\")\n",
    "        print(f\"   ğŸ”— Endpoint: {endpoint}\")\n",
    "        \n",
    "        os.environ[\"GRAFANA_NAME\"] = GRAFANA_NAME\n",
    "        os.environ[\"GRAFANA_ENDPOINT\"] = endpoint\n",
    "        \n",
    "        return grafana_info\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Could not create Grafana: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "if CREATE_GRAFANA:\n",
    "    # Check if Grafana already exists in environment\n",
    "    if EXISTING_GRAFANA_NAME:\n",
    "        print(f\"âœ… Found existing GRAFANA_NAME in environment: {EXISTING_GRAFANA_NAME}\")\n",
    "        print(f\"   Skipping creation, fetching existing workspace info...\")\n",
    "        grafana_workspace = get_grafana_workspace(EXISTING_GRAFANA_NAME)\n",
    "        if grafana_workspace:\n",
    "            endpoint = grafana_workspace.get(\"properties\", {}).get(\"endpoint\", \"N/A\")\n",
    "            print(f\"   ğŸ”— Endpoint: {endpoint}\")\n",
    "            os.environ[\"GRAFANA_ENDPOINT\"] = endpoint\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Could not fetch workspace info. It may not exist or you lack permissions.\")\n",
    "    else:\n",
    "        grafana_workspace = setup_grafana_workspace()\n",
    "else:\n",
    "    print(\"ğŸ’¡ Set CREATE_GRAFANA=True to create a new Grafana workspace.\")\n",
    "    print(\"   Or use an existing Grafana instance.\")\n",
    "    print(\"\\nğŸ“ To connect existing Grafana to Application Insights:\")\n",
    "    print(\"   1. Add Azure Monitor data source in Grafana\")\n",
    "    print(\"   2. Configure with your subscription and App Insights resource\")\n",
    "    print(\"   3. Query OpenTelemetry metrics from customMetrics table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c2e7d",
   "metadata": {},
   "source": [
    "### 3.2: Traffic Simulation with random traffic error injector for Dashboard Metrics\n",
    "\n",
    "Generate realistic AI workload traffic with mixed success/error patterns for Grafana visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8c80ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Part 3.2: Traffic Simulation for Grafana\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Simulating 15 requests (25% error rate):\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 1/15 âœ… 925ms\n",
      "   [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 2/15 âœ… 1030ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 3/15 âœ… 964ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 4/15 âœ… 1493ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 5/15 âœ… 1093ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 6/15 âŒ Error 503\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 7/15 âœ… 877ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 8/15 âŒ Error 502\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 9/15 âœ… 880ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 10/15 âœ… 1216ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 11/15 âœ… 878ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 12/15 âœ… 909ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 13/15 âœ… 972ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 14/15 âœ… 873ms\n",
      "   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 15/15 âœ… 843ms\n",
      "\n",
      "ğŸ“‹ Traffic Simulation Summary:\n",
      "------------------------------------------------------------\n",
      "   Total requests:  15\n",
      "   âœ… Successful:    13 (86.7%)\n",
      "   âŒ Failed:        2\n",
      "   â±ï¸  Avg latency:   996ms\n",
      "\n",
      "   Error breakdown:\n",
      "      â€¢ 502: 1\n",
      "      â€¢ 503: 1\n",
      "\n",
      "âœ… Traffic simulation complete!\n",
      "   â†’ Telemetry automatically exported to Azure Monitor via OpenTelemetry\n"
     ]
    }
   ],
   "source": [
    "# Part 3.2: Traffic Simulation for Dashboard Metrics\n",
    "# ===================================================\n",
    "# Generate realistic AI workload traffic with OpenTelemetry tracing\n",
    "\n",
    "import random\n",
    "\n",
    "class TrafficErrorInjector:\n",
    "    \"\"\"Inject random errors for realistic traffic simulation.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: AzureOpenAI, error_rate: float = 0.2):\n",
    "        self.client = client\n",
    "        self.error_rate = error_rate\n",
    "        self.error_types = [\n",
    "            (429, \"Rate limit exceeded\", {\"retry-after\": \"1\"}),\n",
    "            (500, \"Internal server error\", {}),\n",
    "            (502, \"Bad gateway\", {}),\n",
    "            (503, \"Service unavailable\", {}),\n",
    "        ]\n",
    "    \n",
    "    def create_completion(self, **kwargs):\n",
    "        if random.random() < self.error_rate:\n",
    "            code, msg, headers = random.choice(self.error_types)\n",
    "            resp = create_mock_response(code, {\"error\": {\"message\": msg}}, headers)\n",
    "            if code == 429:\n",
    "                raise RateLimitError(msg, response=resp, body=resp.json())\n",
    "            raise APIStatusError(msg, response=resp, body=resp.json())\n",
    "        \n",
    "        return self.client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "# Simulation configuration\n",
    "SIMULATION_REQUESTS = 15\n",
    "ERROR_RATE = 0.25  # 25% error rate\n",
    "\n",
    "print(\"ğŸ“Š Part 3.2: Traffic Simulation for Grafana\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:\n",
    "    # Create traffic-specific tracer and meter\n",
    "    traffic_tracer = trace.get_tracer(\"ai_traffic_simulator\")\n",
    "    traffic_meter = metrics.get_meter(\"ai_traffic_simulator\")\n",
    "    \n",
    "    # Create metrics instruments for traffic simulation\n",
    "    traffic_requests = traffic_meter.create_counter(\"traffic_requests_total\", description=\"Traffic simulation requests\")\n",
    "    traffic_tokens = traffic_meter.create_counter(\"traffic_tokens_total\", description=\"Traffic simulation tokens\")\n",
    "    traffic_errors = traffic_meter.create_counter(\"traffic_errors_total\", description=\"Traffic simulation errors\")\n",
    "    traffic_latency = traffic_meter.create_histogram(\"traffic_latency_ms\", description=\"Traffic simulation latency\")\n",
    "    \n",
    "    traffic_injector = TrafficErrorInjector(client, error_rate=ERROR_RATE)\n",
    "    \n",
    "    prompts = [\n",
    "        \"What is Python?\", \"Explain REST APIs\", \"What is Docker?\",\n",
    "        \"Define microservices\", \"What is Kubernetes?\", \"Explain CI/CD\",\n",
    "        \"What is serverless?\", \"Define cloud native\", \"What is DevOps?\",\n",
    "        \"Explain infrastructure as code\", \"What is observability?\",\n",
    "        \"Define SRE\", \"What is chaos engineering?\", \"Explain blue-green deployment\",\n",
    "        \"What is canary release?\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ“ Simulating {SIMULATION_REQUESTS} requests ({ERROR_RATE*100:.0f}% error rate):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    traffic_stats = {\"success\": 0, \"failed\": 0, \"errors\": {}, \"latencies\": []}\n",
    "    \n",
    "    for i in range(SIMULATION_REQUESTS):\n",
    "        prompt = prompts[i % len(prompts)]\n",
    "        request_id = f\"sim-{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        with traffic_tracer.start_as_current_span(\"traffic_request\") as span:\n",
    "            span.set_attribute(\"request_id\", request_id)\n",
    "            span.set_attribute(\"request_index\", i + 1)\n",
    "            \n",
    "            try:\n",
    "                with traffic_tracer.start_as_current_span(\"llm_call\") as llm_span:\n",
    "                    start = time.time()\n",
    "                    response = traffic_injector.create_completion(\n",
    "                        model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        max_tokens=20,\n",
    "                    )\n",
    "                    latency_ms = (time.time() - start) * 1000\n",
    "                    \n",
    "                    llm_span.set_attribute(\"latency_ms\", latency_ms)\n",
    "                    llm_span.set_attribute(\"model\", AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)\n",
    "                    llm_span.set_attribute(\"tokens.input\", response.usage.prompt_tokens)\n",
    "                    llm_span.set_attribute(\"tokens.output\", response.usage.completion_tokens)\n",
    "                    \n",
    "                    # Record metrics\n",
    "                    traffic_requests.add(1, {\"model\": AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, \"status\": \"success\"})\n",
    "                    traffic_tokens.add(response.usage.total_tokens, {\"model\": AZURE_OPENAI_CHAT_DEPLOYMENT_NAME})\n",
    "                    traffic_latency.record(latency_ms, {\"model\": AZURE_OPENAI_CHAT_DEPLOYMENT_NAME})\n",
    "                \n",
    "                traffic_stats[\"success\"] += 1\n",
    "                traffic_stats[\"latencies\"].append(latency_ms)\n",
    "                status = \"âœ…\"\n",
    "                msg = f\"{latency_ms:.0f}ms\"\n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "            except (RateLimitError, APIStatusError) as e:\n",
    "                error_type = \"429\" if isinstance(e, RateLimitError) else str(e.status_code)\n",
    "                traffic_stats[\"failed\"] += 1\n",
    "                traffic_stats[\"errors\"][error_type] = traffic_stats[\"errors\"].get(error_type, 0) + 1\n",
    "                \n",
    "                # Record error metrics\n",
    "                traffic_errors.add(1, {\"model\": AZURE_OPENAI_CHAT_DEPLOYMENT_NAME, \"error_type\": error_type})\n",
    "                \n",
    "                span.set_status(Status(StatusCode.ERROR, f\"HTTP {error_type}\"))\n",
    "                span.set_attribute(\"error.type\", error_type)\n",
    "                \n",
    "                status = \"âŒ\" if error_type != \"429\" else \"âš ï¸\"\n",
    "                msg = f\"Error {error_type}\"\n",
    "        \n",
    "        # Progress bar style output\n",
    "        progress = (i + 1) / SIMULATION_REQUESTS\n",
    "        bar = \"â–ˆ\" * int(progress * 20) + \"â–‘\" * (20 - int(progress * 20))\n",
    "        print(f\"   [{bar}] {i+1}/{SIMULATION_REQUESTS} {status} {msg}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nğŸ“‹ Traffic Simulation Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    success_rate = traffic_stats[\"success\"] / SIMULATION_REQUESTS * 100\n",
    "    avg_latency = sum(traffic_stats[\"latencies\"]) / len(traffic_stats[\"latencies\"]) if traffic_stats[\"latencies\"] else 0\n",
    "    \n",
    "    print(f\"   Total requests:  {SIMULATION_REQUESTS}\")\n",
    "    print(f\"   âœ… Successful:    {traffic_stats['success']} ({success_rate:.1f}%)\")\n",
    "    print(f\"   âŒ Failed:        {traffic_stats['failed']}\")\n",
    "    print(f\"   â±ï¸  Avg latency:   {avg_latency:.0f}ms\")\n",
    "    \n",
    "    if traffic_stats[\"errors\"]:\n",
    "        print(f\"\\n   Error breakdown:\")\n",
    "        for err, count in sorted(traffic_stats[\"errors\"].items()):\n",
    "            print(f\"      â€¢ {err}: {count}\")\n",
    "    \n",
    "    print(\"\\nâœ… Traffic simulation complete!\")\n",
    "    print(\"   â†’ Telemetry automatically exported to Azure Monitor via OpenTelemetry\")\n",
    "else:\n",
    "    print(\"âš ï¸ AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e959a",
   "metadata": {},
   "source": [
    "### 3.3: Grafana Dashboard JSON\n",
    "\n",
    "Export dashboard configuration for import into Azure Managed Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c01d4ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Part 3.3: Grafana Dashboard Export\n",
      "============================================================\n",
      "ğŸ“Œ Configuration:\n",
      "   Subscription ID: 3d...c6ca\n",
      "   App Insights Resource ID: /sub\n",
      "\n",
      "ğŸ“‹ Dashboard Panels:\n",
      "   1. Total Requests (stat)\n",
      "   2. Success Rate (gauge)\n",
      "   3. Avg Latency (stat)\n",
      "   4. Total Tokens (stat)\n",
      "   5. Latency Over Time (timeseries)\n",
      "   6. Request Status Distribution (piechart)\n",
      "   7. Error Distribution by Type (barchart)\n",
      "   8. Requests Over Time (timeseries)\n",
      "\n",
      "âœ… Dashboard JSON saved to: grafana_dashboard.json\n",
      "\n",
      "ğŸ’¡ To import into Azure Managed Grafana:\n",
      "   1. Open Grafana: https://grafana-obs-swed-gecwcfc4fbamhzae.cse.grafana.azure.com\n",
      "   2. Go to: Dashboards â†’ Import\n",
      "   3. Upload the JSON file\n",
      "   4. Select Azure Monitor data source when prompted\n",
      "\n",
      "ğŸ“ Sample KQL query to test in Application Insights:\n",
      "   customMetrics | where name startswith 'ai_' | take 10\n"
     ]
    }
   ],
   "source": [
    "# Part 3.3: Grafana Dashboard JSON Export\n",
    "# ========================================\n",
    "# Dashboard configured with actual Azure Monitor data source and OpenTelemetry metrics\n",
    "\n",
    "print(\"ğŸ“Š Part 3.3: Grafana Dashboard Export\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get subscription ID\n",
    "SUBSCRIPTION_ID = os.environ.get(\"AZURE_SUBSCRIPTION_ID\", config.get(\"AZURE_SUBSCRIPTION_ID\", \"\"))\n",
    "\n",
    "# Try to get Application Insights name from various sources\n",
    "APPLICATIONINSIGHTS_RESOURCE_ID = os.environ.get(\"APPLICATIONINSIGHTS_RESOURCE_ID\", \"\")\n",
    "\n",
    "print(f\"ğŸ“Œ Configuration:\")\n",
    "print(f\"   Subscription ID: {SUBSCRIPTION_ID[:2]}...{SUBSCRIPTION_ID[-4:] if len(SUBSCRIPTION_ID) > 12 else SUBSCRIPTION_ID}\")\n",
    "print(f\"   App Insights Resource ID: {APPLICATIONINSIGHTS_RESOURCE_ID[:4]}\")\n",
    "\n",
    "# KQL Queries - use actual newlines for Grafana compatibility\n",
    "QUERIES = {\n",
    "    \"total_requests\": \"customMetrics\\n| where name == 'ai_requests_total'\\n| summarize total = sum(value) by bin(timestamp, 1m)\\n| order by timestamp desc\\n| take 1\",\n",
    "    \"success_rate\": \"customMetrics\\n| where name == 'ai_requests_total'\\n| extend status = tostring(customDimensions['status'])\\n| summarize success = countif(status == 'success'), total = count()\\n| project success_rate = round(100.0 * success / total, 2)\",\n",
    "    \"avg_latency\": \"customMetrics\\n| where name == 'ai_latency_ms'\\n| summarize avg_latency = avg(value)\",\n",
    "    \"total_tokens\": \"customMetrics\\n| where name == 'ai_tokens_total'\\n| summarize total_tokens = sum(value)\",\n",
    "    \"latency_over_time\": \"customMetrics\\n| where name == 'ai_latency_ms'\\n| summarize avg_latency = avg(value), p95_latency = percentile(value, 95) by bin(timestamp, 1m)\\n| order by timestamp asc\",\n",
    "    \"status_distribution\": \"customMetrics\\n| where name == 'ai_requests_total'\\n| extend status = tostring(customDimensions['status'])\\n| summarize count = count() by status\",\n",
    "    \"error_distribution\": \"customMetrics\\n| where name == 'ai_errors_total'\\n| extend error_type = tostring(customDimensions['error_type'])\\n| summarize count = count() by error_type\\n| order by count desc\",\n",
    "    \"requests_over_time\": \"customMetrics\\n| where name == 'ai_requests_total'\\n| extend status = tostring(customDimensions['status'])\\n| summarize count = count() by status, bin(timestamp, 1m)\\n| order by timestamp asc\"\n",
    "}\n",
    "\n",
    "# Build the dashboard - use ${datasource} variable reference for flexibility\n",
    "GRAFANA_DASHBOARD = {\n",
    "    \"dashboard\": {\n",
    "        \"id\": None,\n",
    "        \"uid\": \"ai-gateway-otel\",\n",
    "        \"title\": \"AI Gateway Observability Dashboard (OpenTelemetry)\",\n",
    "        \"tags\": [\"ai\", \"gateway\", \"openai\", \"azure\", \"opentelemetry\"],\n",
    "        \"timezone\": \"browser\",\n",
    "        \"schemaVersion\": 39,\n",
    "        \"refresh\": \"30s\",\n",
    "        \"time\": {\"from\": \"now-1h\", \"to\": \"now\"},\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Total Requests\",\n",
    "                \"type\": \"stat\",\n",
    "                \"gridPos\": {\"h\": 4, \"w\": 6, \"x\": 0, \"y\": 0},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"color\": {\"mode\": \"thresholds\"},\n",
    "                        \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"blue\", \"value\": None}]}\n",
    "                    }\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"colorMode\": \"value\",\n",
    "                    \"graphMode\": \"area\",\n",
    "                    \"justifyMode\": \"auto\",\n",
    "                    \"orientation\": \"auto\",\n",
    "                    \"textMode\": \"auto\",\n",
    "                    \"reduceOptions\": {\"calcs\": [\"sum\"], \"fields\": \"\", \"values\": False}\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"total_requests\"],\n",
    "                        \"resultFormat\": \"table\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Success Rate\",\n",
    "                \"type\": \"gauge\",\n",
    "                \"gridPos\": {\"h\": 4, \"w\": 6, \"x\": 6, \"y\": 0},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"max\": 100,\n",
    "                        \"min\": 0,\n",
    "                        \"unit\": \"percent\",\n",
    "                        \"color\": {\"mode\": \"thresholds\"},\n",
    "                        \"thresholds\": {\n",
    "                            \"mode\": \"absolute\",\n",
    "                            \"steps\": [\n",
    "                                {\"color\": \"red\", \"value\": None},\n",
    "                                {\"color\": \"yellow\", \"value\": 80},\n",
    "                                {\"color\": \"green\", \"value\": 95}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"reduceOptions\": {\"calcs\": [\"lastNotNull\"], \"fields\": \"\", \"values\": False},\n",
    "                    \"showThresholdLabels\": False,\n",
    "                    \"showThresholdMarkers\": True\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"success_rate\"],\n",
    "                        \"resultFormat\": \"table\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Avg Latency\",\n",
    "                \"type\": \"stat\",\n",
    "                \"gridPos\": {\"h\": 4, \"w\": 6, \"x\": 12, \"y\": 0},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"unit\": \"ms\",\n",
    "                        \"color\": {\"mode\": \"thresholds\"},\n",
    "                        \"thresholds\": {\n",
    "                            \"mode\": \"absolute\",\n",
    "                            \"steps\": [\n",
    "                                {\"color\": \"green\", \"value\": None},\n",
    "                                {\"color\": \"yellow\", \"value\": 500},\n",
    "                                {\"color\": \"red\", \"value\": 1000}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"colorMode\": \"value\",\n",
    "                    \"graphMode\": \"area\",\n",
    "                    \"justifyMode\": \"auto\",\n",
    "                    \"orientation\": \"auto\",\n",
    "                    \"textMode\": \"auto\",\n",
    "                    \"reduceOptions\": {\"calcs\": [\"mean\"], \"fields\": \"\", \"values\": False}\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"avg_latency\"],\n",
    "                        \"resultFormat\": \"table\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Total Tokens\",\n",
    "                \"type\": \"stat\",\n",
    "                \"gridPos\": {\"h\": 4, \"w\": 6, \"x\": 18, \"y\": 0},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"color\": {\"mode\": \"thresholds\"},\n",
    "                        \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"purple\", \"value\": None}]}\n",
    "                    }\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"colorMode\": \"value\",\n",
    "                    \"graphMode\": \"area\",\n",
    "                    \"justifyMode\": \"auto\",\n",
    "                    \"orientation\": \"auto\",\n",
    "                    \"textMode\": \"auto\",\n",
    "                    \"reduceOptions\": {\"calcs\": [\"sum\"], \"fields\": \"\", \"values\": False}\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"total_tokens\"],\n",
    "                        \"resultFormat\": \"table\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"title\": \"Latency Over Time\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 4},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"unit\": \"ms\",\n",
    "                        \"color\": {\"mode\": \"palette-classic\"},\n",
    "                        \"custom\": {\n",
    "                            \"lineWidth\": 2,\n",
    "                            \"fillOpacity\": 20,\n",
    "                            \"showPoints\": \"auto\",\n",
    "                            \"spanNulls\": False\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"legend\": {\"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": True},\n",
    "                    \"tooltip\": {\"mode\": \"single\", \"sort\": \"none\"}\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"latency_over_time\"],\n",
    "                        \"resultFormat\": \"time_series\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"title\": \"Request Status Distribution\",\n",
    "                \"type\": \"piechart\",\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 4},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\"color\": {\"mode\": \"palette-classic\"}},\n",
    "                    \"overrides\": [\n",
    "                        {\n",
    "                            \"matcher\": {\"id\": \"byName\", \"options\": \"success\"},\n",
    "                            \"properties\": [{\"id\": \"color\", \"value\": {\"fixedColor\": \"green\", \"mode\": \"fixed\"}}]\n",
    "                        },\n",
    "                        {\n",
    "                            \"matcher\": {\"id\": \"byName\", \"options\": \"error\"},\n",
    "                            \"properties\": [{\"id\": \"color\", \"value\": {\"fixedColor\": \"red\", \"mode\": \"fixed\"}}]\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"legend\": {\"displayMode\": \"list\", \"placement\": \"right\", \"showLegend\": True},\n",
    "                    \"pieType\": \"donut\",\n",
    "                    \"reduceOptions\": {\"calcs\": [\"sum\"], \"fields\": \"\", \"values\": False},\n",
    "                    \"tooltip\": {\"mode\": \"single\", \"sort\": \"none\"}\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"status_distribution\"],\n",
    "                        \"resultFormat\": \"table\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 7,\n",
    "                \"title\": \"Error Distribution by Type\",\n",
    "                \"type\": \"barchart\",\n",
    "                \"gridPos\": {\"h\": 6, \"w\": 12, \"x\": 0, \"y\": 12},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"color\": {\"mode\": \"palette-classic\"},\n",
    "                        \"custom\": {\"fillOpacity\": 80}\n",
    "                    }\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"legend\": {\"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": True},\n",
    "                    \"orientation\": \"horizontal\",\n",
    "                    \"xTickLabelRotation\": 0\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"error_distribution\"],\n",
    "                        \"resultFormat\": \"table\"\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 8,\n",
    "                \"title\": \"Requests Over Time\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"gridPos\": {\"h\": 6, \"w\": 12, \"x\": 12, \"y\": 12},\n",
    "                \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"color\": {\"mode\": \"palette-classic\"},\n",
    "                        \"custom\": {\n",
    "                            \"lineWidth\": 2,\n",
    "                            \"fillOpacity\": 20,\n",
    "                            \"showPoints\": \"auto\",\n",
    "                            \"stacking\": {\"mode\": \"normal\", \"group\": \"A\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"overrides\": [\n",
    "                        {\n",
    "                            \"matcher\": {\"id\": \"byName\", \"options\": \"success\"},\n",
    "                            \"properties\": [{\"id\": \"color\", \"value\": {\"fixedColor\": \"green\", \"mode\": \"fixed\"}}]\n",
    "                        },\n",
    "                        {\n",
    "                            \"matcher\": {\"id\": \"byName\", \"options\": \"error\"},\n",
    "                            \"properties\": [{\"id\": \"color\", \"value\": {\"fixedColor\": \"red\", \"mode\": \"fixed\"}}]\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"options\": {\n",
    "                    \"legend\": {\"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": True},\n",
    "                    \"tooltip\": {\"mode\": \"multi\", \"sort\": \"none\"}\n",
    "                },\n",
    "                \"targets\": [{\n",
    "                    \"refId\": \"A\",\n",
    "                    \"datasource\": {\"type\": \"grafana-azure-monitor-datasource\", \"uid\": \"${datasource}\"},\n",
    "                    \"queryType\": \"Azure Log Analytics\",\n",
    "                    \"azureLogAnalytics\": {\n",
    "                        \"resources\": [APPLICATIONINSIGHTS_RESOURCE_ID],\n",
    "                        \"query\": QUERIES[\"requests_over_time\"],\n",
    "                        \"resultFormat\": \"time_series\"\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        ],\n",
    "        \"templating\": {\n",
    "            \"list\": [\n",
    "                {\n",
    "                    \"name\": \"datasource\",\n",
    "                    \"type\": \"datasource\",\n",
    "                    \"label\": \"Azure Monitor\",\n",
    "                    \"query\": \"grafana-azure-monitor-datasource\",\n",
    "                    \"current\": {},\n",
    "                    \"hide\": 0\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Dashboard Panels:\")\n",
    "for panel in GRAFANA_DASHBOARD[\"dashboard\"][\"panels\"]:\n",
    "    print(f\"   {panel['id']}. {panel['title']} ({panel['type']})\")\n",
    "\n",
    "# Save to file - Grafana UI import needs the dashboard object directly (not wrapped)\n",
    "dashboard_file = \"grafana_dashboard.json\"\n",
    "with open(dashboard_file, 'w') as f:\n",
    "    # Export only the dashboard object, not the wrapper\n",
    "    json.dump(GRAFANA_DASHBOARD[\"dashboard\"], f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Dashboard JSON saved to: {dashboard_file}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ To import into Azure Managed Grafana:\")\n",
    "print(f\"   1. Open Grafana: {os.environ.get('GRAFANA_ENDPOINT', 'https://<your-grafana>.grafana.azure.com')}\")\n",
    "print(\"   2. Go to: Dashboards â†’ Import\")\n",
    "print(\"   3. Upload the JSON file\")\n",
    "print(\"   4. Select Azure Monitor data source when prompted\")\n",
    "\n",
    "print(\"\\nğŸ“ Sample KQL query to test in Application Insights:\")\n",
    "print(\"   customMetrics | where name startswith 'ai_' | take 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40aa212",
   "metadata": {},
   "source": [
    "### Simulation Results on Azure Managed Grafana Dashboard\n",
    "\n",
    "![../images/simulation_result_grafana.png](../images/simulation_result_grafana.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0285595",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### OpenTelemetry + Azure Application Insights Integration\n",
    "\n",
    "| Component | OpenTelemetry API | Azure Monitor Destination |\n",
    "|-----------|-------------------|---------------------------|\n",
    "| **Traces** | `tracer.start_as_current_span()` | requests, dependencies |\n",
    "| **Metrics** | `Counter`, `Histogram` | customMetrics |\n",
    "| **Logs** | `logging` module | traces table |\n",
    "\n",
    "### Key OpenTelemetry Patterns\n",
    "\n",
    "```python\n",
    "# 1. Azure Monitor setup (required)\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "CONNECTION_STRING = os.environ.get(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n",
    "if not CONNECTION_STRING:\n",
    "    raise ValueError(\"APPLICATIONINSIGHTS_CONNECTION_STRING is required\")\n",
    "\n",
    "configure_azure_monitor(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    enable_live_metrics=True,\n",
    ")\n",
    "\n",
    "# 2. Get tracer and meter instances\n",
    "from opentelemetry import trace, metrics\n",
    "\n",
    "tracer = trace.get_tracer(\"service_name\", \"1.0.0\")\n",
    "meter = metrics.get_meter(\"service_name\", \"1.0.0\")\n",
    "\n",
    "# 3. Tracing with context\n",
    "with tracer.start_as_current_span(\"operation\", attributes={\"key\": \"value\"}) as span:\n",
    "    span.set_attribute(\"ai.model\", \"gpt-4o\")\n",
    "    # ... operation ...\n",
    "\n",
    "# 4. Metrics collection\n",
    "request_counter = meter.create_counter(\"ai_requests_total\")\n",
    "latency_histogram = meter.create_histogram(\"ai_latency_ms\")\n",
    "\n",
    "request_counter.add(1, {\"model\": \"gpt-4o\", \"status\": \"success\"})\n",
    "latency_histogram.record(latency_ms, {\"model\": \"gpt-4o\"})\n",
    "\n",
    "# 5. Error recording\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "span.set_status(Status(StatusCode.ERROR, \"Error message\"))\n",
    "span.record_exception(exception)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a5f7a",
   "metadata": {},
   "source": [
    "## Cleanup Resources\n",
    "\n",
    "If you created Azure resources (Grafana, Log Analytics, etc.) during this lab, you can clean them up to avoid ongoing costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Azure Resources\n",
    "# =======================\n",
    "# Uncomment and run this cell to delete resources created during this lab\n",
    "\n",
    "DELETE_RESOURCES = False  # Set to True to delete resources\n",
    "\n",
    "\n",
    "def delete_grafana_workspace() -> None:\n",
    "    \"\"\"Delete the Azure Managed Grafana workspace.\"\"\"\n",
    "    grafana_name = os.environ.get('GRAFANA_NAME', GRAFANA_NAME if 'GRAFANA_NAME' in dir() else '')\n",
    "    \n",
    "    if not grafana_name or not RESOURCE_GROUP:\n",
    "        print(\"âš ï¸ Grafana name or resource group not found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ—‘ï¸ Deleting Azure Managed Grafana: {grafana_name}\")\n",
    "    try:\n",
    "        run_az([\n",
    "            \"grafana\", \"delete\",\n",
    "            \"-g\", RESOURCE_GROUP,\n",
    "            \"-n\", grafana_name,\n",
    "            \"--yes\"\n",
    "        ])\n",
    "        print(f\"   âœ… Grafana workspace deleted: {grafana_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Could not delete Grafana: {e}\")\n",
    "\n",
    "\n",
    "print(\"ğŸ§¹ Resource Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if DELETE_RESOURCES:\n",
    "    print(\"\\nâš ï¸ WARNING: This will delete the following resources:\")\n",
    "    print(f\"   - Azure Managed Grafana: {os.environ.get('GRAFANA_NAME', 'N/A')}\")\n",
    "    print(\"\\nğŸ”„ Proceeding with cleanup...\")\n",
    "    \n",
    "    delete_grafana_workspace()\n",
    "    \n",
    "    print(\"\\nâœ… Cleanup complete!\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ Set DELETE_RESOURCES=True to delete Azure resources.\")\n",
    "    print(\"   Resources to clean up:\")\n",
    "    print(f\"   - Azure Managed Grafana: {os.environ.get('GRAFANA_NAME', 'N/A')}\")\n",
    "    print(\"\\nğŸ’¡ Tip: You can also delete resources from the Azure Portal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83345c2d",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **OpenTelemetry Integration**: Use `azure-monitor-opentelemetry` for one-line Azure Monitor setup\n",
    "2. **Distributed Tracing**: Track requests with `tracer.start_as_current_span()` context managers\n",
    "3. **Metrics Collection**: Use `Counter` and `Histogram` instruments for custom metrics\n",
    "4. **Error Handling**: Record errors with `Status`, `StatusCode`, and `span.record_exception()`\n",
    "5. **Azure Monitor**: Automatic export of traces, metrics, and logs to Application Insights\n",
    "6. **Grafana Dashboards**: Query OpenTelemetry metrics via KQL in customMetrics table\n",
    "\n",
    "### Observability Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 OpenTelemetry Observability Stack                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Application   â”‚      Transport       â”‚      Visualization         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ OpenTelemetry   â”‚ azure-monitor-       â”‚ â€¢ Azure Portal             â”‚\n",
    "â”‚ â€¢ Tracer        â”‚   opentelemetry      â”‚ â€¢ Application Insights     â”‚\n",
    "â”‚ â€¢ Meter         â”‚ â€¢ Auto-export        â”‚ â€¢ Log Analytics            â”‚\n",
    "â”‚ â€¢ Logger        â”‚ â€¢ Batching           â”‚ â€¢ Azure Managed Grafana    â”‚\n",
    "â”‚                 â”‚ â€¢ Retry              â”‚ â€¢ Custom Dashboards        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Azure Monitor OpenTelemetry Distro](https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable)\n",
    "- [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)\n",
    "- [Azure Managed Grafana](https://learn.microsoft.com/en-us/azure/managed-grafana/overview)\n",
    "- [KQL for Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/get-started-queries)\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- **3_reliability/**: Retry, rate limiting, timeout management\n",
    "- **2_workload_optimization/5_caching_strategies.ipynb**: Redis caching strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
