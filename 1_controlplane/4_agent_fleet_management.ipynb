{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd09c4",
   "metadata": {},
   "source": [
    "# Control Plane: Agent Fleet Management\n",
    "\n",
    "----\n",
    "\n",
    "This notebook demonstrates **batch registration** of agents/workflows and **real-time simulation** with live metrics tracking in Azure AI Foundry.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "- **Batch Register Agents**: Create 5 agents at once for fleet management\n",
    "- **Batch Register Workflows**: Create 5 workflows for multi-agent orchestration\n",
    "- **Real-time Simulation**: Run continuous daemon simulations with live metrics\n",
    "- **Sample Evaluation**: Evaluate agent performance on sample tasks \n",
    "- **Monitor in Portal**: View live metrics and traces in Azure AI Foundry Portal\n",
    "\n",
    "* Reference Repository: [@guming3d, AI-Foundry-Agent-Simulation](https://github.com/guming3d/AI-Foundry-Agent-Simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a85aa6",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Part 1: Batch Agent Registration](#part-1-batch-agent-registration)\n",
    "- [Part 2: Batch Workflow Registration](#part-2-batch-workflow-registration)\n",
    "- [Part 3: Real-time Daemon Simulation](#part-3-real-time-daemon-simulation)\n",
    "- [Part 4: Sample Evaluations](#part-4-sample-evaluations)\n",
    "- [Part 5: Portal Monitoring](#part-5-portal-monitoring)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f62229",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b55dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Azure CLI found: /anaconda/envs/azureml_py38/bin//az\n",
      "‚úÖ PATH looks good already\n",
      "\n",
      "PATH (first 150 chars): /anaconda/envs/azureml_py38/bin/:/afh/code/agent-operator-lab/.venv/bin:/home/azureuser/.vscode-server/cli/servers/Stable-c9d77990917f3102ada88be140d2...\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import asyncio\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'üîç Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"‚úÖ Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('‚úÖ PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d98e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded settings from '../0_setup/.foundry_config.json'.\n",
      "\n",
      "üìå Foundry name: foundry-rq90gs\n",
      "üìå Resource group: foundry-rg\n",
      "üìå Location: swedencentral\n",
      "üìå Project endpoint: https://foundry-rq90gs.services.ai.azure.com/api/projects/default-project\n",
      "üìå Model deployment: gpt-5.2\n"
     ]
    }
   ],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import PromptAgentDefinition\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'.\")\n",
    "    print('üí° Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "PROJECT_NAME = config.get('PROJECT_NAME', 'proj-default')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "AZURE_AI_MODEL_DEPLOYMENT_NAME = os.environ.get(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"‚úÖ Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nüìå Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"üìå Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"üìå Location: {LOCATION}\")\n",
    "print(f\"üìå Project endpoint: {AZURE_AI_PROJECT_ENDPOINT}\")\n",
    "print(f\"üìå Model deployment: {AZURE_AI_MODEL_DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b6633",
   "metadata": {},
   "source": [
    "## Part 1: Batch Agent Registration\n",
    "\n",
    "Register **10 agents** at once for fleet management. Each agent has a specific role (e.g., CustomerSupport, DataAnalyst, etc.) to simulate a realistic enterprise deployment.\n",
    "\n",
    "| Agent Type | Description | Count |\n",
    "|------------|-------------|-------|\n",
    "| CustomerSupport | Handle customer inquiries | 1 |\n",
    "| DataAnalyst | Analyze business data | 1 |\n",
    "| TechSupport | Technical troubleshooting | 1 |\n",
    "| SalesAssistant | Sales and product queries | 1 |\n",
    "| GeneralAssistant | General purpose queries | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9d1b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ Batch Agent Registration (5 Agents)\n",
      "======================================================================\n",
      "   ‚úÖ [01/05] Created: fleet-CustomerSupport-20260204081706-01\n",
      "   ‚úÖ [02/05] Created: fleet-DataAnalyst-20260204081706-01\n",
      "   ‚úÖ [03/05] Created: fleet-TechSupport-20260204081706-01\n",
      "   ‚úÖ [04/05] Created: fleet-SalesAssistant-20260204081706-01\n",
      "   ‚úÖ [05/05] Created: fleet-GeneralAssistant-20260204081706-01\n",
      "\n",
      "üìä Summary: 5/05 agents created successfully\n",
      "   Agent IDs saved for simulation\n"
     ]
    }
   ],
   "source": [
    "# Define agent types with their instructions\n",
    "AGENT_TYPES = [\n",
    "    (\"CustomerSupport\", \"You are a customer support agent. Help customers with inquiries, complaints, and account issues.\"),\n",
    "    (\"DataAnalyst\", \"You are a data analyst. Analyze data, create reports, and provide business insights.\"),\n",
    "    (\"TechSupport\", \"You are a technical support agent. Help with software issues, troubleshooting, and technical guidance.\"),\n",
    "    (\"SalesAssistant\", \"You are a sales assistant. Help with product information, pricing, and purchase recommendations.\"),\n",
    "    (\"GeneralAssistant\", \"You are a general assistant. Help with various tasks and questions.\"),\n",
    "]\n",
    "\n",
    "# Batch create 5 agents (1 per type)\n",
    "uuid_suffix = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "created_agents = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ Batch Agent Registration (5 Agents)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as project_client:\n",
    "    agent_count = 0\n",
    "    for agent_type, instructions in AGENT_TYPES:\n",
    "        for i in range(1):  # 1 agent per type\n",
    "            agent_name = f\"fleet-{agent_type}-{uuid_suffix}-{i+1:02d}\"\n",
    "            try:\n",
    "                agent = project_client.agents.create_version(\n",
    "                    agent_name=agent_name,\n",
    "                    definition=PromptAgentDefinition(\n",
    "                        model=AZURE_AI_MODEL_DEPLOYMENT_NAME,\n",
    "                        instructions=instructions,\n",
    "                    ),\n",
    "                )\n",
    "                created_agents.append({\n",
    "                    \"name\": agent.name,\n",
    "                    \"id\": agent.id,\n",
    "                    \"type\": agent_type,\n",
    "                    \"model\": AZURE_AI_MODEL_DEPLOYMENT_NAME,\n",
    "                })\n",
    "                agent_count += 1\n",
    "                print(f\"   ‚úÖ [{agent_count:02d}/05] Created: {agent_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to create {agent_name}: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\nüìä Summary: {len(created_agents)}/05 agents created successfully\")\n",
    "print(f\"   Agent IDs saved for simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf9053",
   "metadata": {},
   "source": [
    "## Part 2: Batch Workflow Registration\n",
    "\n",
    "Register **5 workflows** to orchestrate multi-agent interactions. Each workflow defines a sequence of agent calls for complex tasks.\n",
    "\n",
    "| Workflow Type | Description | Agents Used |\n",
    "|---------------|-------------|-------------|\n",
    "| CustomerJourney | End-to-end customer support | CustomerSupport ‚Üí TechSupport |\n",
    "| DataPipeline | Data analysis workflow | DataAnalyst ‚Üí GeneralAssistant |\n",
    "| SalesProcess | Sales funnel workflow | SalesAssistant ‚Üí CustomerSupport |\n",
    "| TechEscalation | Technical issue escalation | TechSupport ‚Üí DataAnalyst |\n",
    "| GeneralInquiry | General purpose workflow | GeneralAssistant ‚Üí SalesAssistant |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "039bee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ Batch Workflow Registration (5 Workflows)\n",
      "======================================================================\n",
      "   ‚úÖ [01/05] Registered: wf-CustomerJourney-20260204081706-01 (sequential)\n",
      "   ‚úÖ [02/05] Registered: wf-DataPipeline-20260204081706-01 (sequential)\n",
      "   ‚úÖ [03/05] Registered: wf-SalesProcess-20260204081706-01 (sequential)\n",
      "   ‚úÖ [04/05] Registered: wf-TechEscalation-20260204081706-01 (review_loop)\n",
      "   ‚úÖ [05/05] Registered: wf-GeneralInquiry-20260204081706-01 (sequential)\n",
      "\n",
      "üìä Summary: 5/05 workflows registered in Azure AI Foundry\n",
      "   Workflow patterns: sequential, review_loop\n",
      "   Each workflow orchestrates 2 agents\n"
     ]
    }
   ],
   "source": [
    "# Import WorkflowAgentDefinition for workflow registration\n",
    "from azure.ai.projects.models import WorkflowAgentDefinition\n",
    "\n",
    "# Define workflow types with patterns\n",
    "WORKFLOW_TYPES = [\n",
    "    (\"CustomerJourney\", \"End-to-end customer support workflow\", \"sequential\"),\n",
    "    (\"DataPipeline\", \"Data analysis and reporting pipeline\", \"sequential\"),\n",
    "    (\"SalesProcess\", \"Sales funnel and conversion workflow\", \"sequential\"),\n",
    "    (\"TechEscalation\", \"Technical issue escalation workflow\", \"review_loop\"),\n",
    "    (\"GeneralInquiry\", \"General purpose inquiry workflow\", \"sequential\"),\n",
    "]\n",
    "\n",
    "# Map workflow types to agent pairs (from created agents)\n",
    "def get_agents_for_workflow(workflow_type: str, agents: list) -> tuple:\n",
    "    \"\"\"Get agent pair for a workflow based on type.\"\"\"\n",
    "    agent_map = {\n",
    "        \"CustomerJourney\": (\"CustomerSupport\", \"TechSupport\"),\n",
    "        \"DataPipeline\": (\"DataAnalyst\", \"GeneralAssistant\"),\n",
    "        \"SalesProcess\": (\"SalesAssistant\", \"CustomerSupport\"),\n",
    "        \"TechEscalation\": (\"TechSupport\", \"DataAnalyst\"),\n",
    "        \"GeneralInquiry\": (\"GeneralAssistant\", \"SalesAssistant\"),\n",
    "    }\n",
    "    primary_type, secondary_type = agent_map.get(workflow_type, (\"GeneralAssistant\", \"GeneralAssistant\"))\n",
    "    \n",
    "    # Find matching agents\n",
    "    primary = next((a for a in agents if a[\"type\"] == primary_type), agents[0] if agents else None)\n",
    "    secondary = next((a for a in agents if a[\"type\"] == secondary_type), agents[-1] if agents else None)\n",
    "    return primary, secondary\n",
    "\n",
    "def build_sequential_workflow_yaml(primary_agent: str, secondary_agent: str) -> str:\n",
    "    \"\"\"Build a sequential workflow YAML with two agents.\"\"\"\n",
    "    return f\"\"\"kind: workflow\n",
    "trigger:\n",
    "  kind: OnConversationStart\n",
    "  id: workflow_start\n",
    "  actions:\n",
    "    - kind: SetVariable\n",
    "      id: set_variable_input\n",
    "      variable: Local.LatestMessage\n",
    "      value: \"=UserMessage(System.LastMessageText)\"\n",
    "    - kind: CreateConversation\n",
    "      id: create_primary_conversation\n",
    "      conversationId: Local.PrimaryConversationId\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: primary_agent\n",
    "      description: Primary Agent\n",
    "      conversationId: \"=Local.PrimaryConversationId\"\n",
    "      agent:\n",
    "        name: {primary_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.LatestMessage\n",
    "    - kind: CreateConversation\n",
    "      id: create_secondary_conversation\n",
    "      conversationId: Local.SecondaryConversationId\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: secondary_agent\n",
    "      description: Secondary Agent\n",
    "      conversationId: \"=Local.SecondaryConversationId\"\n",
    "      agent:\n",
    "        name: {secondary_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.FinalMessage\n",
    "        autoSend: true\"\"\"\n",
    "\n",
    "def build_review_loop_workflow_yaml(primary_agent: str, reviewer_agent: str) -> str:\n",
    "    \"\"\"Build a review loop workflow YAML with primary and reviewer agents.\"\"\"\n",
    "    return f\"\"\"kind: workflow\n",
    "trigger:\n",
    "  kind: OnConversationStart\n",
    "  id: workflow_start\n",
    "  actions:\n",
    "    - kind: SetVariable\n",
    "      id: set_variable_input\n",
    "      variable: Local.LatestMessage\n",
    "      value: \"=UserMessage(System.LastMessageText)\"\n",
    "    - kind: SetVariable\n",
    "      id: set_variable_turncount\n",
    "      variable: Local.TurnCount\n",
    "      value: \"=0\"\n",
    "    - kind: CreateConversation\n",
    "      id: create_primary_conversation\n",
    "      conversationId: Local.PrimaryConversationId\n",
    "    - kind: CreateConversation\n",
    "      id: create_reviewer_conversation\n",
    "      conversationId: Local.ReviewerConversationId\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: primary_agent\n",
    "      description: Primary Agent\n",
    "      conversationId: \"=Local.PrimaryConversationId\"\n",
    "      agent:\n",
    "        name: {primary_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.LatestMessage\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: reviewer_agent\n",
    "      description: Reviewer Agent\n",
    "      conversationId: \"=Local.ReviewerConversationId\"\n",
    "      agent:\n",
    "        name: {reviewer_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.LatestMessage\n",
    "    - kind: SetVariable\n",
    "      id: increment_turncount\n",
    "      variable: Local.TurnCount\n",
    "      value: \"=Local.TurnCount + 1\"\n",
    "    - kind: ConditionGroup\n",
    "      id: completion_check\n",
    "      conditions:\n",
    "        - condition: '=!IsBlank(Find(\"[COMPLETE]\", Upper(Last(Local.LatestMessage).Text)))'\n",
    "          id: check_done\n",
    "          actions:\n",
    "            - kind: EndConversation\n",
    "              id: end_workflow\n",
    "        - condition: \"=Local.TurnCount >= 3\"\n",
    "          id: check_turn_count_exceeded\n",
    "          actions:\n",
    "            - kind: SendActivity\n",
    "              id: send_final\n",
    "              activity: \"Review complete.\"\n",
    "      elseActions:\n",
    "        - kind: GotoAction\n",
    "          id: goto_primary_agent\n",
    "          actionId: primary_agent\"\"\"\n",
    "\n",
    "# Batch create 5 workflows (1 per type) and register in Azure\n",
    "created_workflows = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÑ Batch Workflow Registration (5 Workflows)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not created_agents:\n",
    "    print(\"‚ö†Ô∏è No agents available. Run Part 1 first to create agents.\")\n",
    "else:\n",
    "    with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as project_client:\n",
    "        workflow_count = 0\n",
    "        for workflow_type, description, pattern in WORKFLOW_TYPES:\n",
    "            for i in range(1):  # 1 workflow per type\n",
    "                workflow_name = f\"wf-{workflow_type}-{uuid_suffix}-{i+1:02d}\"\n",
    "                \n",
    "                # Get agents for this workflow\n",
    "                primary_agent, secondary_agent = get_agents_for_workflow(workflow_type, created_agents)\n",
    "                \n",
    "                if not primary_agent or not secondary_agent:\n",
    "                    print(f\"   ‚ö†Ô∏è Skipping {workflow_name}: No agents available\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Build workflow YAML based on pattern\n",
    "                    if pattern == \"review_loop\":\n",
    "                        workflow_yaml = build_review_loop_workflow_yaml(\n",
    "                            primary_agent[\"name\"],\n",
    "                            secondary_agent[\"name\"]\n",
    "                        )\n",
    "                    else:\n",
    "                        workflow_yaml = build_sequential_workflow_yaml(\n",
    "                            primary_agent[\"name\"],\n",
    "                            secondary_agent[\"name\"]\n",
    "                        )\n",
    "                    \n",
    "                    # Register workflow in Azure AI Foundry\n",
    "                    workflow = project_client.agents.create_version(\n",
    "                        agent_name=workflow_name,\n",
    "                        definition=WorkflowAgentDefinition(workflow=workflow_yaml),\n",
    "                    )\n",
    "                    \n",
    "                    workflow_config = {\n",
    "                        \"name\": workflow.name,\n",
    "                        \"id\": workflow.id,\n",
    "                        \"version\": workflow.version,\n",
    "                        \"type\": workflow_type,\n",
    "                        \"pattern\": pattern,\n",
    "                        \"description\": description,\n",
    "                        \"agents\": [primary_agent[\"name\"], secondary_agent[\"name\"]],\n",
    "                        \"created_at\": datetime.now().isoformat(),\n",
    "                    }\n",
    "                    created_workflows.append(workflow_config)\n",
    "                    workflow_count += 1\n",
    "                    print(f\"   ‚úÖ [{workflow_count:02d}/05] Registered: {workflow_name} ({pattern})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Failed to create {workflow_name}: {str(e)[:60]}\")\n",
    "\n",
    "    print(f\"\\nüìä Summary: {len(created_workflows)}/05 workflows registered in Azure AI Foundry\")\n",
    "    print(f\"   Workflow patterns: sequential, review_loop\")\n",
    "    print(f\"   Each workflow orchestrates 2 agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc608fa",
   "metadata": {},
   "source": [
    "## Part 3: Real-time Daemon Simulation\n",
    "\n",
    "Run a **continuous daemon simulation** that sends requests to agents and tracks live metrics. This simulates production traffic for monitoring and testing.\n",
    "\n",
    "### Daemon Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Interval | 5 seconds | Time between batches |\n",
    "| Calls per batch | 3-5 | Random calls per interval |\n",
    "| Threads | 3 | Parallel execution threads |\n",
    "| Duration | 60 seconds | Total simulation time |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7854410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daemon configuration ready\n",
      "   Interval: 5.0s\n",
      "   Calls/batch: 3-5\n",
      "   Duration: 60s\n",
      "   Evaluations enabled: False\n",
      "   Evaluation count: 5\n"
     ]
    }
   ],
   "source": [
    "# Daemon Configuration\n",
    "@dataclass\n",
    "class DaemonConfig:\n",
    "    \"\"\"Configuration for daemon simulation.\"\"\"\n",
    "    interval_seconds: float = 5.0\n",
    "    calls_per_batch_min: int = 3\n",
    "    calls_per_batch_max: int = 5\n",
    "    threads: int = 3\n",
    "    duration_seconds: int = 60\n",
    "    run_evaluations: bool = False  # Enable/disable evaluations during simulation\n",
    "    evaluation_count: int = 5     # Number of evaluation runs\n",
    "\n",
    "@dataclass\n",
    "class DaemonMetrics:\n",
    "    \"\"\"Live metrics for daemon monitoring.\"\"\"\n",
    "    total_calls: int = 0\n",
    "    successful_calls: int = 0\n",
    "    failed_calls: int = 0\n",
    "    total_latency_ms: float = 0\n",
    "    evaluation_runs: int = 0\n",
    "    start_time: Optional[datetime] = None\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        return (self.successful_calls / self.total_calls * 100) if self.total_calls > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def avg_latency_ms(self) -> float:\n",
    "        return (self.total_latency_ms / self.successful_calls) if self.successful_calls > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def runtime(self) -> str:\n",
    "        if not self.start_time:\n",
    "            return \"0s\"\n",
    "        elapsed = (datetime.now() - self.start_time).total_seconds()\n",
    "        return f\"{int(elapsed)}s\"\n",
    "\n",
    "# Sample queries for each agent type\n",
    "QUERY_TEMPLATES = {\n",
    "    \"CustomerSupport\": [\"How do I reset my password?\", \"I need help with my order\", \"What's your return policy?\"],\n",
    "    \"DataAnalyst\": [\"Analyze Q4 sales data\", \"Show revenue trends\", \"Compare regional performance\"],\n",
    "    \"TechSupport\": [\"My app crashes on startup\", \"How do I install the SDK?\", \"Network connection issues\"],\n",
    "    \"SalesAssistant\": [\"What's the pricing for enterprise?\", \"Compare product features\", \"Request a demo\"],\n",
    "    \"GeneralAssistant\": [\"What time is it in Tokyo?\", \"Summarize this article\", \"Help me draft an email\"],\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Daemon configuration ready\")\n",
    "print(f\"   Interval: {DaemonConfig().interval_seconds}s\")\n",
    "print(f\"   Calls/batch: {DaemonConfig().calls_per_batch_min}-{DaemonConfig().calls_per_batch_max}\")\n",
    "print(f\"   Duration: {DaemonConfig().duration_seconds}s\")\n",
    "print(f\"   Evaluations enabled: {DaemonConfig().run_evaluations}\")\n",
    "print(f\"   Evaluation count: {DaemonConfig().evaluation_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DaemonRunner class defined (with Evals API support)\n",
      "   Key changes from previous version:\n",
      "   - Uses openai_client.evals.create() API\n",
      "   - Uses DataSourceConfigCustom for structured data\n",
      "   - Uses builtin evaluators (builtin.violence)\n",
      "   - Async execution with polling for completion\n"
     ]
    }
   ],
   "source": [
    "# Daemon Runner - Simulates continuous production traffic with Evals API\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "\n",
    "# Import Evals API types\n",
    "from openai.types.eval_create_params import DataSourceConfigCustom\n",
    "from openai.types.evals.run_create_response import RunCreateResponse\n",
    "from openai.types.evals.run_retrieve_response import RunRetrieveResponse\n",
    "\n",
    "@dataclass\n",
    "class EvaluationItem:\n",
    "    \"\"\"Single evaluation dataset row.\"\"\"\n",
    "    query: str\n",
    "    context: str = \"\"\n",
    "    ground_truth: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Result of a single evaluation run.\"\"\"\n",
    "    eval_id: str\n",
    "    run_id: str\n",
    "    status: str\n",
    "    result_counts: Dict[str, Any]\n",
    "    output_items: List[Any]\n",
    "    success: bool\n",
    "    latency_ms: float\n",
    "\n",
    "class DaemonRunner:\n",
    "    \"\"\"Simple daemon runner for agent fleet simulation with Evals API evaluations.\"\"\"\n",
    "    \n",
    "    def __init__(self, agents: List[Dict], config: DaemonConfig):\n",
    "        self.agents = agents\n",
    "        self.config = config\n",
    "        self.metrics = DaemonMetrics()\n",
    "        self._stop_requested = False\n",
    "        self._lock = threading.Lock()\n",
    "        self.evaluation_results: List[EvaluationResult] = []\n",
    "    \n",
    "    def _call_agent(self, agent: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single agent call and return metrics.\"\"\"\n",
    "        agent_type = agent.get(\"type\", \"GeneralAssistant\")\n",
    "        query = random.choice(QUERY_TEMPLATES.get(agent_type, QUERY_TEMPLATES[\"GeneralAssistant\"]))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        success = False\n",
    "        error_msg = None\n",
    "        \n",
    "        try:\n",
    "            with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as client:\n",
    "                openai_client = client.get_openai_client()\n",
    "                conversation = openai_client.conversations.create()\n",
    "                response = openai_client.responses.create(\n",
    "                    conversation=conversation.id,\n",
    "                    extra_body={\"agent\": {\"name\": agent[\"name\"], \"type\": \"agent_reference\"}},\n",
    "                    input=query,\n",
    "                )\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:50]\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        with self._lock:\n",
    "            self.metrics.total_calls += 1\n",
    "            if success:\n",
    "                self.metrics.successful_calls += 1\n",
    "                self.metrics.total_latency_ms += latency_ms\n",
    "            else:\n",
    "                self.metrics.failed_calls += 1\n",
    "        \n",
    "        return {\"success\": success, \"latency_ms\": latency_ms, \"agent\": agent[\"name\"], \"error\": error_msg}\n",
    "    \n",
    "    def _run_batch(self, batch_size: int):\n",
    "        \"\"\"Run a batch of agent calls in parallel.\"\"\"\n",
    "        threads = []\n",
    "        for _ in range(batch_size):\n",
    "            agent = random.choice(self.agents)\n",
    "            t = threading.Thread(target=self._call_agent, args=(agent,))\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "    \n",
    "    def _run_evals_api_evaluation(self, model: str = None) -> Optional[EvaluationResult]:\n",
    "        \"\"\"\n",
    "        Run evaluation using the official OpenAI Evals API.\n",
    "        \n",
    "        Key differences from previous implementation:\n",
    "        1. Uses openai_client.evals.create() instead of direct responses.create()\n",
    "        2. Uses DataSourceConfigCustom for structured data source\n",
    "        3. Uses builtin evaluators (e.g., builtin.violence) instead of local YAML\n",
    "        4. Async execution with polling for completion\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use provided model or default\n",
    "        eval_model = model or AZURE_AI_MODEL_DEPLOYMENT_NAME or \"gpt-4o\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            with (\n",
    "                DefaultAzureCredential() as cred,\n",
    "                AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=cred) as project_client,\n",
    "                project_client.get_openai_client() as openai_client,\n",
    "            ):\n",
    "                # 1. Create data source config (key difference: structured schema)\n",
    "                data_source_config = DataSourceConfigCustom(\n",
    "                    type=\"custom\",\n",
    "                    item_schema={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\"query\": {\"type\": \"string\"}},\n",
    "                        \"required\": [\"query\"]\n",
    "                    },\n",
    "                    include_sample_schema=True,\n",
    "                )\n",
    "                \n",
    "                # 2. Define testing criteria with builtin evaluators\n",
    "                testing_criteria = [\n",
    "                    {\n",
    "                        \"type\": \"azure_ai_evaluator\",\n",
    "                        \"name\": \"violence_detection\",\n",
    "                        \"evaluator_name\": \"builtin.violence\",\n",
    "                        \"data_mapping\": {\n",
    "                            \"query\": \"{{item.query}}\",\n",
    "                            \"response\": \"{{sample.output_text}}\"\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"azure_ai_evaluator\",\n",
    "                        \"name\": \"coherence\",\n",
    "                        \"evaluator_name\": \"builtin.coherence\",\n",
    "                        \"initialization_parameters\": {\"deployment_name\": f\"{eval_model}\"},\n",
    "                        \"data_mapping\": {\"query\": \"{{item.query}}\", \"response\": \"{{sample.output_text}}\"},\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"azure_ai_evaluator\",\n",
    "                        \"name\": \"fluency\",\n",
    "                        \"evaluator_name\": \"builtin.fluency\",\n",
    "                        \"initialization_parameters\": {\"deployment_name\": f\"{eval_model}\"},\n",
    "                        \"data_mapping\": {\"query\": \"{{item.query}}\", \"response\": \"{{sample.output_text}}\"},\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"azure_ai_evaluator\",\n",
    "                        \"name\": \"relevance\",\n",
    "                        \"evaluator_name\": \"builtin.relevance\",\n",
    "                        \"initialization_parameters\": {\n",
    "                            \"deployment_name\": f\"{eval_model}\",\n",
    "                            \"is_reasoning_model\": True, # if you use an AOAI reasoning model   \n",
    "                        },\n",
    "                        \"data_mapping\": {\n",
    "                            \"query\": \"{{item.query}}\",\n",
    "                            \"response\": \"{{sample.output_text}}\",\n",
    "                        },\n",
    "                    },\n",
    "                    \n",
    "                    \n",
    "                ]\n",
    "                \n",
    "                # 3. Create evaluation object\n",
    "                eval_object = openai_client.evals.create(\n",
    "                    name=f\"Fleet Evaluation {datetime.now().strftime('%H%M%S')}\",\n",
    "                    data_source_config=data_source_config,\n",
    "                    testing_criteria=testing_criteria,  # type: ignore\n",
    "                )\n",
    "                print(f\"      üìù Evaluation created (id: {eval_object.id})\")\n",
    "                \n",
    "                # 4. Define data source with sample queries\n",
    "                data_source = {\n",
    "                    \"type\": \"azure_ai_target_completions\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"file_content\",\n",
    "                        \"content\": [\n",
    "                            {\"item\": {\"query\": \"What is the capital of France?\",\"response\":\"Paris\"}},\n",
    "                            {\"item\": {\"query\": \"How do I reset my password?\",\"response\":\"To reset your password, go to the settings page and click on 'Reset Password'.\"}},\n",
    "                            {\"item\": {\"query\": \"Explain machine learning briefly.\",\"response\":\"Machine learning is a field of AI that uses algorithms to learn from data and make predictions or decisions without being explicitly programmed.\"}},\n",
    "                        ],\n",
    "                    },\n",
    "                    \"input_messages\": {\n",
    "                        \"type\": \"template\",\n",
    "                        \"template\": [\n",
    "                            {\n",
    "                                \"type\": \"message\",\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": {\"type\": \"input_text\", \"text\": \"{{item.query}}\"}\n",
    "                            }\n",
    "                        ],\n",
    "                    },\n",
    "                    \"target\": {\n",
    "                        \"type\": \"azure_ai_model\",\n",
    "                        \"model\": eval_model,\n",
    "                        \"sampling_params\": {\n",
    "                            \"top_p\": 1.0,\n",
    "                            \"max_completion_tokens\": 256,\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "                \n",
    "                # 5. Create and run evaluation\n",
    "                eval_run: Union[RunCreateResponse, RunRetrieveResponse] = openai_client.evals.runs.create(\n",
    "                    eval_id=eval_object.id,\n",
    "                    name=f\"Run for {eval_model}\",\n",
    "                    data_source=data_source  # type: ignore\n",
    "                )\n",
    "                print(f\"      üöÄ Eval run started (id: {eval_run.id})\")\n",
    "                \n",
    "                # 6. Poll for completion (key difference: async execution)\n",
    "                while eval_run.status not in [\"completed\", \"failed\"]:\n",
    "                    eval_run = openai_client.evals.runs.retrieve(\n",
    "                        run_id=eval_run.id,\n",
    "                        eval_id=eval_object.id\n",
    "                    )\n",
    "                    print(f\"      ‚è≥ Status: {eval_run.status}...\")\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                latency_ms = (time.time() - start_time) * 1000\n",
    "                \n",
    "                # 7. Collect results\n",
    "                if eval_run.status == \"completed\":\n",
    "                    output_items = list(\n",
    "                        openai_client.evals.runs.output_items.list(\n",
    "                            run_id=eval_run.id,\n",
    "                            eval_id=eval_object.id\n",
    "                        )\n",
    "                    )\n",
    "                    print(output_items)\n",
    "                    result = EvaluationResult(\n",
    "                        eval_id=eval_object.id,\n",
    "                        run_id=eval_run.id,\n",
    "                        status=\"completed\",\n",
    "                        result_counts=eval_run.result_counts or {},\n",
    "                        output_items=output_items,\n",
    "                        success=True,\n",
    "                        latency_ms=latency_ms,\n",
    "                    )\n",
    "                    \n",
    "                    with self._lock:\n",
    "                        self.metrics.evaluation_runs += 1\n",
    "                    \n",
    "                    return result\n",
    "                else:\n",
    "                    return EvaluationResult(\n",
    "                        eval_id=eval_object.id,\n",
    "                        run_id=eval_run.id,\n",
    "                        status=\"failed\",\n",
    "                        result_counts={},\n",
    "                        output_items=[],\n",
    "                        success=False,\n",
    "                        latency_ms=latency_ms,\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Evaluation error: {str(e)[:80]}\")\n",
    "            return None\n",
    "    \n",
    "    def _run_evaluations_batch(self, count: int):\n",
    "        \"\"\"Run evaluation batch using Evals API.\"\"\"\n",
    "        print(f\"\\n   üß™ Running {count} evaluation(s) with Evals API...\")\n",
    "        \n",
    "        for i in range(count):\n",
    "            print(f\"\\n   [{i+1}/{count}] Starting evaluation...\")\n",
    "            result = self._run_evals_api_evaluation()\n",
    "            \n",
    "            if result:\n",
    "                self.evaluation_results.append(result)\n",
    "                status = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "                print(f\"      {status} Completed in {result.latency_ms/1000:.1f}s\")\n",
    "                if result.result_counts:\n",
    "                    print(f\"      üìä Results: {result.result_counts}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the daemon simulation loop.\"\"\"\n",
    "        self.metrics.start_time = datetime.now()\n",
    "        end_time = time.time() + self.config.duration_seconds\n",
    "        batch_num = 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üöÄ Starting Daemon Simulation\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        while time.time() < end_time and not self._stop_requested:\n",
    "            batch_num += 1\n",
    "            batch_size = random.randint(self.config.calls_per_batch_min, self.config.calls_per_batch_max)\n",
    "            \n",
    "            self._run_batch(batch_size)\n",
    "            \n",
    "            # Print live metrics\n",
    "            print(f\"\\r   üìä Batch {batch_num:03d} | \"\n",
    "                  f\"Calls: {self.metrics.total_calls} | \"\n",
    "                  f\"Success: {self.metrics.success_rate:.1f}% | \"\n",
    "                  f\"Avg Latency: {self.metrics.avg_latency_ms:.0f}ms | \"\n",
    "                  f\"Runtime: {self.metrics.runtime}\", end=\"\", flush=True)\n",
    "            \n",
    "            time.sleep(self.config.interval_seconds)\n",
    "        \n",
    "        # Run evaluations at the end if enabled\n",
    "        if self.config.run_evaluations:\n",
    "            self._run_evaluations_batch(self.config.evaluation_count)\n",
    "        \n",
    "        print(f\"\\n\\n‚úÖ Daemon simulation completed!\")\n",
    "        return self.metrics\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the daemon gracefully.\"\"\"\n",
    "        self._stop_requested = True\n",
    "\n",
    "print(\"‚úÖ DaemonRunner class defined (with Evals API support)\")\n",
    "print(\"   Key changes from previous version:\")\n",
    "print(\"   - Uses openai_client.evals.create() API\")\n",
    "print(\"   - Uses DataSourceConfigCustom for structured data\")\n",
    "print(\"   - Uses builtin evaluators (builtin.violence)\")\n",
    "print(\"   - Async execution with polling for completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7542837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ Starting Daemon Simulation\n",
      "======================================================================\n",
      "   üìä Batch 005 | Calls: 15 | Success: 100.0% | Avg Latency: 6180ms | Runtime: 57s\n",
      "\n",
      "‚úÖ Daemon simulation completed!\n",
      "\n",
      "======================================================================\n",
      "üìä Final Simulation Metrics\n",
      "======================================================================\n",
      "   Total Calls:      15\n",
      "   Successful:       15\n",
      "   Failed:           0\n",
      "   Success Rate:     100.0%\n",
      "   Avg Latency:      6180ms\n",
      "   Total Runtime:    62s\n"
     ]
    }
   ],
   "source": [
    "# Run the daemon simulation without evaluations\n",
    "RUN_EVALUATIONS = False  \n",
    "\n",
    "if not created_agents:\n",
    "    print(\"‚ö†Ô∏è No agents created. Run Part 1 first.\")\n",
    "else:\n",
    "    config = DaemonConfig(\n",
    "        interval_seconds=5.0,\n",
    "        calls_per_batch_min=2,\n",
    "        calls_per_batch_max=4,\n",
    "        threads=3,\n",
    "        duration_seconds=60,        # 1 minute simulation\n",
    "        run_evaluations=RUN_EVALUATIONS,\n",
    "        evaluation_count=2,        # Run 2 evaluations\n",
    "    )\n",
    "    \n",
    "    #\n",
    "    \n",
    "    daemon = DaemonRunner(agents=created_agents, config=config)\n",
    "    final_metrics = daemon.run()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä Final Simulation Metrics\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total Calls:      {final_metrics.total_calls}\")\n",
    "    print(f\"   Successful:       {final_metrics.successful_calls}\")\n",
    "    print(f\"   Failed:           {final_metrics.failed_calls}\")\n",
    "    print(f\"   Success Rate:     {final_metrics.success_rate:.1f}%\")\n",
    "    print(f\"   Avg Latency:      {final_metrics.avg_latency_ms:.0f}ms\")\n",
    "    print(f\"   Total Runtime:    {final_metrics.runtime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e9dfb",
   "metadata": {},
   "source": [
    "## Part 4: Sample Evaluations (Evals API)\n",
    "\n",
    "The daemon simulation uses the **official OpenAI Evals API** for model evaluation.\n",
    "\n",
    "### How Evals API Works\n",
    "\n",
    "1. **Create Evaluation Object**: Define `data_source_config` and `testing_criteria`\n",
    "2. **Create Eval Run**: Specify `data_source` with queries and target model\n",
    "3. **Poll for Completion**: Wait for `status == \"completed\"`\n",
    "4. **Retrieve Results**: Get `output_items` with detailed evaluation results\n",
    "\n",
    "### How to Enable Evaluations\n",
    "\n",
    "Set `RUN_EVALUATIONS = True` in the daemon run cell to enable evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfbc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ Starting Daemon Simulation\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Batch 005 | Calls: 16 | Success: 100.0% | Avg Latency: 7246ms | Runtime: 76s\n",
      "   üß™ Running 2 evaluation(s) with Evals API...\n",
      "\n",
      "   [1/2] Starting evaluation...\n",
      "      üìù Evaluation created (id: eval_b19dbb85eba0400997cc435c25bd29f8)\n",
      "      üöÄ Eval run started (id: evalrun_17433f24c8bb4d7184e8dd3d1d8ce61c)\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n",
      "      ‚è≥ Status: in_progress...\n"
     ]
    }
   ],
   "source": [
    "# Run the daemon simulation with evaluations\n",
    "RUN_EVALUATIONS = True  \n",
    "\n",
    "if not created_agents:\n",
    "    print(\"‚ö†Ô∏è No agents created. Run Part 1 first.\")\n",
    "else:\n",
    "    config = DaemonConfig(\n",
    "        interval_seconds=5.0,\n",
    "        calls_per_batch_min=2,\n",
    "        calls_per_batch_max=4,\n",
    "        threads=3,\n",
    "        duration_seconds=60,        # 1 minute simulation\n",
    "        run_evaluations=RUN_EVALUATIONS,\n",
    "        evaluation_count=2,        # Run 2 evaluations\n",
    "    )\n",
    "    \n",
    "    #\n",
    "    \n",
    "    daemon = DaemonRunner(agents=created_agents, config=config)\n",
    "    final_metrics = daemon.run()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä Final Simulation Metrics\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total Calls:      {final_metrics.total_calls}\")\n",
    "    print(f\"   Successful:       {final_metrics.successful_calls}\")\n",
    "    print(f\"   Failed:           {final_metrics.failed_calls}\")\n",
    "    print(f\"   Success Rate:     {final_metrics.success_rate:.1f}%\")\n",
    "    print(f\"   Avg Latency:      {final_metrics.avg_latency_ms:.0f}ms\")\n",
    "    print(f\"   Total Runtime:    {final_metrics.runtime}\")\n",
    "    \n",
    "    if RUN_EVALUATIONS:\n",
    "        print(f\"\\nüß™ Evaluation Results:\")\n",
    "        print(f\"   Evaluation Runs:  {final_metrics.evaluation_runs}\")\n",
    "        print(f\"   Templates Used:   4 (violence_detection, coherence, fluency, relevance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ Detailed Evaluation Results (Evals API)\n",
      "======================================================================\n",
      "\n",
      "1. ‚úÖ Eval ID: eval_05f778a7df8247edbdd9c54b3afabeb7\n",
      "   Run ID: evalrun_2a0b833e5e634dfba535d0c5d0b4a830\n",
      "   Status: completed\n",
      "   Latency: 43.3s\n",
      "   Result Counts: ResultCounts(errored=0, failed=1, passed=2, total=3)\n",
      "   Output Items (3):\n",
      "      1. OutputItemListResponse(id='1', created_at=1770193198, datasource_item={'query': 'What is the capital...\n",
      "      2. OutputItemListResponse(id='2', created_at=1770193198, datasource_item={'query': 'How do I reset my p...\n",
      "      3. OutputItemListResponse(id='3', created_at=1770193198, datasource_item={'query': 'Explain machine lea...\n",
      "\n",
      "2. ‚úÖ Eval ID: eval_141a53017905457db97b1e6c1d804396\n",
      "   Run ID: evalrun_51b91afe8b1e4b55b22a0aac23e27227\n",
      "   Status: completed\n",
      "   Latency: 39.0s\n",
      "   Result Counts: ResultCounts(errored=0, failed=1, passed=2, total=3)\n",
      "   Output Items (3):\n",
      "      1. OutputItemListResponse(id='1', created_at=1770193239, datasource_item={'query': 'What is the capital...\n",
      "      2. OutputItemListResponse(id='2', created_at=1770193239, datasource_item={'query': 'How do I reset my p...\n",
      "      3. OutputItemListResponse(id='3', created_at=1770193239, datasource_item={'query': 'Explain machine lea...\n"
     ]
    }
   ],
   "source": [
    "# View evaluation results from Evals API\n",
    "if daemon and daemon.evaluation_results:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üß™ Detailed Evaluation Results (Evals API)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, result in enumerate(daemon.evaluation_results, 1):\n",
    "        status = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "        print(f\"\\n{i}. {status} Eval ID: {result.eval_id}\")\n",
    "        print(f\"   Run ID: {result.run_id}\")\n",
    "        print(f\"   Status: {result.status}\")\n",
    "        print(f\"   Latency: {result.latency_ms/1000:.1f}s\")\n",
    "        \n",
    "        if result.result_counts:\n",
    "            print(f\"   Result Counts: {result.result_counts}\")\n",
    "        \n",
    "        if result.output_items:\n",
    "            print(f\"   Output Items ({len(result.output_items)}):\")\n",
    "            for j, item in enumerate(result.output_items[:3], 1):  # Show first 3\n",
    "                print(f\"      {j}. {str(item)[:100]}...\")\n",
    "else:\n",
    "    print(\"No evaluation results available. Run daemon with RUN_EVALUATIONS=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93c5d0",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Batch Agent Registration** | Created 5 agents with different roles in a single loop |\n",
    "| **Workflow Registration** | Defined 5 workflows for multi-agent orchestration |\n",
    "| **Daemon Simulation** | Ran continuous traffic simulation with live metrics |\n",
    "| **Sample Evaluations** | Ran 2 evaluations using YAML templates (when enabled) |\n",
    "| **Portal Integration** | Agents visible in Azure AI Foundry for monitoring |\n",
    "\n",
    "### Metrics Summary\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `total_calls` | Total API calls made during simulation |\n",
    "| `success_rate` | Percentage of successful calls |\n",
    "| `avg_latency_ms` | Average response time in milliseconds |\n",
    "| `evaluation_runs` | Number of evaluations executed (if enabled) |\n",
    "| `runtime` | Total simulation duration |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Enable Evaluations**: Set `RUN_EVALUATIONS = True` to run sample evaluations\n",
    "2. **Increase Duration**: Change `duration_seconds` for longer simulations\n",
    "3. **Add Tracing**: Connect Application Insights (see `1_foundry_agent_monitoring.ipynb`)\n",
    "4. **Scale Up**: Increase `calls_per_batch_max` for higher load testing\n",
    "5. **View in Portal**: Navigate to Azure AI Foundry to see live traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49b80c",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-services/agents/)\n",
    "- [Agent Fleet Simulation Reference](https://github.com/guming3d/AI-Foundry-Agent-Simulation)\n",
    "- [Azure AI Projects SDK](https://learn.microsoft.com/en-us/python/api/azure-ai-projects/)\n",
    "- [Application Insights for Tracing](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
