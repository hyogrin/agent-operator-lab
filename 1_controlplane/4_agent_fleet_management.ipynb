{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd09c4",
   "metadata": {},
   "source": [
    "# Control Plane: Agent Fleet Management\n",
    "\n",
    "----\n",
    "\n",
    "This notebook demonstrates **batch registration** of agents/workflows and **real-time simulation** with live metrics tracking in Azure AI Foundry.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "- **Batch Register Agents**: Create 10 agents at once for fleet management\n",
    "- **Batch Register Workflows**: Create 10 workflows for multi-agent orchestration\n",
    "- **Real-time Simulation**: Run continuous daemon simulations with live metrics\n",
    "- **Sample Evaluation**: Evaluate agent performance on sample tasks \n",
    "- **Monitor in Portal**: View live metrics and traces in Azure AI Foundry Portal\n",
    "\n",
    "* Code Reference Repository: [@guming3d, AI-Foundry-Agent-Simulation](https://github.com/guming3d/AI-Foundry-Agent-Simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a85aa6",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Part 1: Batch Agent Registration](#part-1-batch-agent-registration)\n",
    "- [Part 2: Batch Workflow Registration](#part-2-batch-workflow-registration)\n",
    "- [Part 3: Real-time Daemon Simulation](#part-3-real-time-daemon-simulation)\n",
    "- [Part 4: Sample Evaluations](#part-4-sample-evaluations)\n",
    "- [Part 5: Portal Monitoring](#part-5-portal-monitoring)\n",
    "- [Wrap-up](#wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f62229",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook reuses the configuration file (`.foundry_config.json`) created by `0_setup/1_setup.ipynb`.\n",
    "\n",
    "- If the file is missing, run the setup notebook first.\n",
    "- Make sure you can authenticate (e.g., `az login`), so `DefaultAzureCredential` can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b55dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and PATH configuration\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import asyncio\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Ensure the notebook kernel can find Azure CLI (`az`) on PATH\n",
    "possible_paths = [\n",
    "    '/opt/homebrew/bin',   # macOS (Apple Silicon)\n",
    "    '/usr/local/bin',      # macOS (Intel) / Linux\n",
    "    '/usr/bin',            # Linux / Codespaces\n",
    "    '/home/linuxbrew/.linuxbrew/bin',  # Linux Homebrew\n",
    "]\n",
    "\n",
    "az_path = None\n",
    "try:\n",
    "    result = subprocess.run(['which', 'az'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        az_path = os.path.dirname(result.stdout.strip())\n",
    "        print(f'üîç Azure CLI found: {result.stdout.strip()}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "paths_to_add: list[str] = []\n",
    "if az_path and az_path not in os.environ.get('PATH', ''):\n",
    "    paths_to_add.append(az_path)\n",
    "else:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and path not in os.environ.get('PATH', ''):\n",
    "            paths_to_add.append(path)\n",
    "\n",
    "if paths_to_add:\n",
    "    os.environ['PATH'] = ':'.join(paths_to_add) + ':' + os.environ.get('PATH', '')\n",
    "    print(f\"‚úÖ Added to PATH: {', '.join(paths_to_add)}\")\n",
    "else:\n",
    "    print('‚úÖ PATH looks good already')\n",
    "\n",
    "print(f\"\\nPATH (first 150 chars): {os.environ['PATH'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d98e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Foundry project settings from .foundry_config.json\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import PromptAgentDefinition\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "config_file = '../0_setup/.foundry_config.json'\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not find '{config_file}'.\")\n",
    "    print('üí° Run 0_setup/1_setup.ipynb first to create it.')\n",
    "    raise e\n",
    "\n",
    "FOUNDRY_NAME = config.get('FOUNDRY_NAME')\n",
    "RESOURCE_GROUP = config.get('RESOURCE_GROUP')\n",
    "LOCATION = config.get('LOCATION')\n",
    "PROJECT_NAME = config.get('PROJECT_NAME', 'proj-default')\n",
    "AZURE_AI_PROJECT_ENDPOINT = config.get('AZURE_AI_PROJECT_ENDPOINT')\n",
    "AZURE_AI_MODEL_DEPLOYMENT_NAME = os.environ.get(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ['FOUNDRY_NAME'] = FOUNDRY_NAME or ''\n",
    "os.environ['LOCATION'] = LOCATION or ''\n",
    "os.environ['RESOURCE_GROUP'] = RESOURCE_GROUP or ''\n",
    "os.environ['AZURE_SUBSCRIPTION_ID'] = config.get('AZURE_SUBSCRIPTION_ID', '')\n",
    "\n",
    "print(f\"‚úÖ Loaded settings from '{config_file}'.\")\n",
    "print(f\"\\nüìå Foundry name: {FOUNDRY_NAME}\")\n",
    "print(f\"üìå Resource group: {RESOURCE_GROUP}\")\n",
    "print(f\"üìå Location: {LOCATION}\")\n",
    "print(f\"üìå Project endpoint: {AZURE_AI_PROJECT_ENDPOINT}\")\n",
    "print(f\"üìå Model deployment: {AZURE_AI_MODEL_DEPLOYMENT_NAME}\")\n",
    "\n",
    "# Initialize credential for Azure services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b6633",
   "metadata": {},
   "source": [
    "## Part 1: Batch Agent Registration\n",
    "\n",
    "Register **10 agents** at once for fleet management. Each agent has a specific role (e.g., CustomerSupport, DataAnalyst, etc.) to simulate a realistic enterprise deployment.\n",
    "\n",
    "| Agent Type | Description | Count |\n",
    "|------------|-------------|-------|\n",
    "| CustomerSupport | Handle customer inquiries | 2 |\n",
    "| DataAnalyst | Analyze business data | 2 |\n",
    "| TechSupport | Technical troubleshooting | 2 |\n",
    "| SalesAssistant | Sales and product queries | 2 |\n",
    "| GeneralAssistant | General purpose queries | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent types with their instructions\n",
    "AGENT_TYPES = [\n",
    "    (\"CustomerSupport\", \"You are a customer support agent. Help customers with inquiries, complaints, and account issues.\"),\n",
    "    (\"DataAnalyst\", \"You are a data analyst. Analyze data, create reports, and provide business insights.\"),\n",
    "    (\"TechSupport\", \"You are a technical support agent. Help with software issues, troubleshooting, and technical guidance.\"),\n",
    "    (\"SalesAssistant\", \"You are a sales assistant. Help with product information, pricing, and purchase recommendations.\"),\n",
    "    (\"GeneralAssistant\", \"You are a general assistant. Help with various tasks and questions.\"),\n",
    "]\n",
    "\n",
    "# Batch create 10 agents (2 per type)\n",
    "uuid_suffix = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "created_agents = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ Batch Agent Registration (10 Agents)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as project_client:\n",
    "    agent_count = 0\n",
    "    for agent_type, instructions in AGENT_TYPES:\n",
    "        for i in range(2):  # 2 agents per type\n",
    "            agent_name = f\"fleet-{agent_type}-{uuid_suffix}-{i+1:02d}\"\n",
    "            try:\n",
    "                agent = project_client.agents.create_version(\n",
    "                    agent_name=agent_name,\n",
    "                    definition=PromptAgentDefinition(\n",
    "                        model=AZURE_AI_MODEL_DEPLOYMENT_NAME,\n",
    "                        instructions=instructions,\n",
    "                    ),\n",
    "                )\n",
    "                created_agents.append({\n",
    "                    \"name\": agent.name,\n",
    "                    \"id\": agent.id,\n",
    "                    \"type\": agent_type,\n",
    "                    \"model\": AZURE_AI_MODEL_DEPLOYMENT_NAME,\n",
    "                })\n",
    "                agent_count += 1\n",
    "                print(f\"   ‚úÖ [{agent_count:02d}/10] Created: {agent_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to create {agent_name}: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\nüìä Summary: {len(created_agents)}/10 agents created successfully\")\n",
    "print(f\"   Agent IDs saved for simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf9053",
   "metadata": {},
   "source": [
    "## Part 2: Batch Workflow Registration\n",
    "\n",
    "Register **10 workflows** to orchestrate multi-agent interactions. Each workflow defines a sequence of agent calls for complex tasks.\n",
    "\n",
    "| Workflow Type | Description | Agents Used |\n",
    "|---------------|-------------|-------------|\n",
    "| CustomerJourney | End-to-end customer support | CustomerSupport ‚Üí TechSupport |\n",
    "| DataPipeline | Data analysis workflow | DataAnalyst ‚Üí GeneralAssistant |\n",
    "| SalesProcess | Sales funnel workflow | SalesAssistant ‚Üí CustomerSupport |\n",
    "| TechEscalation | Technical issue escalation | TechSupport ‚Üí DataAnalyst |\n",
    "| GeneralInquiry | General purpose workflow | GeneralAssistant ‚Üí SalesAssistant |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WorkflowAgentDefinition for workflow registration\n",
    "from azure.ai.projects.models import WorkflowAgentDefinition\n",
    "\n",
    "# Define workflow types with patterns\n",
    "WORKFLOW_TYPES = [\n",
    "    (\"CustomerJourney\", \"End-to-end customer support workflow\", \"sequential\"),\n",
    "    (\"DataPipeline\", \"Data analysis and reporting pipeline\", \"sequential\"),\n",
    "    (\"SalesProcess\", \"Sales funnel and conversion workflow\", \"sequential\"),\n",
    "    (\"TechEscalation\", \"Technical issue escalation workflow\", \"review_loop\"),\n",
    "    (\"GeneralInquiry\", \"General purpose inquiry workflow\", \"sequential\"),\n",
    "]\n",
    "\n",
    "# Map workflow types to agent pairs (from created agents)\n",
    "def get_agents_for_workflow(workflow_type: str, agents: list) -> tuple:\n",
    "    \"\"\"Get agent pair for a workflow based on type.\"\"\"\n",
    "    agent_map = {\n",
    "        \"CustomerJourney\": (\"CustomerSupport\", \"TechSupport\"),\n",
    "        \"DataPipeline\": (\"DataAnalyst\", \"GeneralAssistant\"),\n",
    "        \"SalesProcess\": (\"SalesAssistant\", \"CustomerSupport\"),\n",
    "        \"TechEscalation\": (\"TechSupport\", \"DataAnalyst\"),\n",
    "        \"GeneralInquiry\": (\"GeneralAssistant\", \"SalesAssistant\"),\n",
    "    }\n",
    "    primary_type, secondary_type = agent_map.get(workflow_type, (\"GeneralAssistant\", \"GeneralAssistant\"))\n",
    "    \n",
    "    # Find matching agents\n",
    "    primary = next((a for a in agents if a[\"type\"] == primary_type), agents[0] if agents else None)\n",
    "    secondary = next((a for a in agents if a[\"type\"] == secondary_type), agents[-1] if agents else None)\n",
    "    return primary, secondary\n",
    "\n",
    "def build_sequential_workflow_yaml(primary_agent: str, secondary_agent: str) -> str:\n",
    "    \"\"\"Build a sequential workflow YAML with two agents.\"\"\"\n",
    "    return f\"\"\"kind: workflow\n",
    "trigger:\n",
    "  kind: OnConversationStart\n",
    "  id: workflow_start\n",
    "  actions:\n",
    "    - kind: SetVariable\n",
    "      id: set_variable_input\n",
    "      variable: Local.LatestMessage\n",
    "      value: \"=UserMessage(System.LastMessageText)\"\n",
    "    - kind: CreateConversation\n",
    "      id: create_primary_conversation\n",
    "      conversationId: Local.PrimaryConversationId\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: primary_agent\n",
    "      description: Primary Agent\n",
    "      conversationId: \"=Local.PrimaryConversationId\"\n",
    "      agent:\n",
    "        name: {primary_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.LatestMessage\n",
    "    - kind: CreateConversation\n",
    "      id: create_secondary_conversation\n",
    "      conversationId: Local.SecondaryConversationId\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: secondary_agent\n",
    "      description: Secondary Agent\n",
    "      conversationId: \"=Local.SecondaryConversationId\"\n",
    "      agent:\n",
    "        name: {secondary_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.FinalMessage\n",
    "        autoSend: true\"\"\"\n",
    "\n",
    "def build_review_loop_workflow_yaml(primary_agent: str, reviewer_agent: str) -> str:\n",
    "    \"\"\"Build a review loop workflow YAML with primary and reviewer agents.\"\"\"\n",
    "    return f\"\"\"kind: workflow\n",
    "trigger:\n",
    "  kind: OnConversationStart\n",
    "  id: workflow_start\n",
    "  actions:\n",
    "    - kind: SetVariable\n",
    "      id: set_variable_input\n",
    "      variable: Local.LatestMessage\n",
    "      value: \"=UserMessage(System.LastMessageText)\"\n",
    "    - kind: SetVariable\n",
    "      id: set_variable_turncount\n",
    "      variable: Local.TurnCount\n",
    "      value: \"=0\"\n",
    "    - kind: CreateConversation\n",
    "      id: create_primary_conversation\n",
    "      conversationId: Local.PrimaryConversationId\n",
    "    - kind: CreateConversation\n",
    "      id: create_reviewer_conversation\n",
    "      conversationId: Local.ReviewerConversationId\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: primary_agent\n",
    "      description: Primary Agent\n",
    "      conversationId: \"=Local.PrimaryConversationId\"\n",
    "      agent:\n",
    "        name: {primary_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.LatestMessage\n",
    "    - kind: InvokeAzureAgent\n",
    "      id: reviewer_agent\n",
    "      description: Reviewer Agent\n",
    "      conversationId: \"=Local.ReviewerConversationId\"\n",
    "      agent:\n",
    "        name: {reviewer_agent}\n",
    "      input:\n",
    "        messages: \"=Local.LatestMessage\"\n",
    "      output:\n",
    "        messages: Local.LatestMessage\n",
    "    - kind: SetVariable\n",
    "      id: increment_turncount\n",
    "      variable: Local.TurnCount\n",
    "      value: \"=Local.TurnCount + 1\"\n",
    "    - kind: ConditionGroup\n",
    "      id: completion_check\n",
    "      conditions:\n",
    "        - condition: '=!IsBlank(Find(\"[COMPLETE]\", Upper(Last(Local.LatestMessage).Text)))'\n",
    "          id: check_done\n",
    "          actions:\n",
    "            - kind: EndConversation\n",
    "              id: end_workflow\n",
    "        - condition: \"=Local.TurnCount >= 3\"\n",
    "          id: check_turn_count_exceeded\n",
    "          actions:\n",
    "            - kind: SendActivity\n",
    "              id: send_final\n",
    "              activity: \"Review complete.\"\n",
    "      elseActions:\n",
    "        - kind: GotoAction\n",
    "          id: goto_primary_agent\n",
    "          actionId: primary_agent\"\"\"\n",
    "\n",
    "# Batch create 10 workflows (2 per type) and register in Azure\n",
    "created_workflows = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÑ Batch Workflow Registration (10 Workflows)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not created_agents:\n",
    "    print(\"‚ö†Ô∏è No agents available. Run Part 1 first to create agents.\")\n",
    "else:\n",
    "    with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as project_client:\n",
    "        workflow_count = 0\n",
    "        for workflow_type, description, pattern in WORKFLOW_TYPES:\n",
    "            for i in range(2):  # 2 workflows per type\n",
    "                workflow_name = f\"wf-{workflow_type}-{uuid_suffix}-{i+1:02d}\"\n",
    "                \n",
    "                # Get agents for this workflow\n",
    "                primary_agent, secondary_agent = get_agents_for_workflow(workflow_type, created_agents)\n",
    "                \n",
    "                if not primary_agent or not secondary_agent:\n",
    "                    print(f\"   ‚ö†Ô∏è Skipping {workflow_name}: No agents available\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Build workflow YAML based on pattern\n",
    "                    if pattern == \"review_loop\":\n",
    "                        workflow_yaml = build_review_loop_workflow_yaml(\n",
    "                            primary_agent[\"name\"],\n",
    "                            secondary_agent[\"name\"]\n",
    "                        )\n",
    "                    else:\n",
    "                        workflow_yaml = build_sequential_workflow_yaml(\n",
    "                            primary_agent[\"name\"],\n",
    "                            secondary_agent[\"name\"]\n",
    "                        )\n",
    "                    \n",
    "                    # Register workflow in Azure AI Foundry\n",
    "                    workflow = project_client.agents.create_version(\n",
    "                        agent_name=workflow_name,\n",
    "                        definition=WorkflowAgentDefinition(workflow=workflow_yaml),\n",
    "                    )\n",
    "                    \n",
    "                    workflow_config = {\n",
    "                        \"name\": workflow.name,\n",
    "                        \"id\": workflow.id,\n",
    "                        \"version\": workflow.version,\n",
    "                        \"type\": workflow_type,\n",
    "                        \"pattern\": pattern,\n",
    "                        \"description\": description,\n",
    "                        \"agents\": [primary_agent[\"name\"], secondary_agent[\"name\"]],\n",
    "                        \"created_at\": datetime.now().isoformat(),\n",
    "                    }\n",
    "                    created_workflows.append(workflow_config)\n",
    "                    workflow_count += 1\n",
    "                    print(f\"   ‚úÖ [{workflow_count:02d}/10] Registered: {workflow_name} ({pattern})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Failed to create {workflow_name}: {str(e)[:60]}\")\n",
    "\n",
    "    print(f\"\\nüìä Summary: {len(created_workflows)}/10 workflows registered in Azure AI Foundry\")\n",
    "    print(f\"   Workflow patterns: sequential, review_loop\")\n",
    "    print(f\"   Each workflow orchestrates 2 agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc608fa",
   "metadata": {},
   "source": [
    "## Part 3: Real-time Daemon Simulation\n",
    "\n",
    "Run a **continuous daemon simulation** that sends requests to agents and tracks live metrics. This simulates production traffic for monitoring and testing.\n",
    "\n",
    "### Daemon Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Interval | 5 seconds | Time between batches |\n",
    "| Calls per batch | 3-5 | Random calls per interval |\n",
    "| Threads | 3 | Parallel execution threads |\n",
    "| Duration | 60 seconds | Total simulation time |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daemon Configuration\n",
    "@dataclass\n",
    "class DaemonConfig:\n",
    "    \"\"\"Configuration for daemon simulation.\"\"\"\n",
    "    interval_seconds: float = 5.0\n",
    "    calls_per_batch_min: int = 3\n",
    "    calls_per_batch_max: int = 5\n",
    "    threads: int = 3\n",
    "    duration_seconds: int = 60\n",
    "    run_evaluations: bool = False  # Enable/disable evaluations during simulation\n",
    "    evaluation_count: int = 10     # Number of evaluation runs\n",
    "\n",
    "@dataclass\n",
    "class DaemonMetrics:\n",
    "    \"\"\"Live metrics for daemon monitoring.\"\"\"\n",
    "    total_calls: int = 0\n",
    "    successful_calls: int = 0\n",
    "    failed_calls: int = 0\n",
    "    total_latency_ms: float = 0\n",
    "    evaluation_runs: int = 0\n",
    "    start_time: Optional[datetime] = None\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        return (self.successful_calls / self.total_calls * 100) if self.total_calls > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def avg_latency_ms(self) -> float:\n",
    "        return (self.total_latency_ms / self.successful_calls) if self.successful_calls > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def runtime(self) -> str:\n",
    "        if not self.start_time:\n",
    "            return \"0s\"\n",
    "        elapsed = (datetime.now() - self.start_time).total_seconds()\n",
    "        return f\"{int(elapsed)}s\"\n",
    "\n",
    "# Sample queries for each agent type\n",
    "QUERY_TEMPLATES = {\n",
    "    \"CustomerSupport\": [\"How do I reset my password?\", \"I need help with my order\", \"What's your return policy?\"],\n",
    "    \"DataAnalyst\": [\"Analyze Q4 sales data\", \"Show revenue trends\", \"Compare regional performance\"],\n",
    "    \"TechSupport\": [\"My app crashes on startup\", \"How do I install the SDK?\", \"Network connection issues\"],\n",
    "    \"SalesAssistant\": [\"What's the pricing for enterprise?\", \"Compare product features\", \"Request a demo\"],\n",
    "    \"GeneralAssistant\": [\"What time is it in Tokyo?\", \"Summarize this article\", \"Help me draft an email\"],\n",
    "}\n",
    "\n",
    "# Evaluation templates directory\n",
    "EVALUATION_TEMPLATES_DIR = os.path.join(os.path.dirname(os.path.abspath(\".\")), \"1_controlplane\", \"evaluation_templates\")\n",
    "\n",
    "print(\"‚úÖ Daemon configuration ready\")\n",
    "print(f\"   Interval: {DaemonConfig().interval_seconds}s\")\n",
    "print(f\"   Calls/batch: {DaemonConfig().calls_per_batch_min}-{DaemonConfig().calls_per_batch_max}\")\n",
    "print(f\"   Duration: {DaemonConfig().duration_seconds}s\")\n",
    "print(f\"   Evaluations enabled: {DaemonConfig().run_evaluations}\")\n",
    "print(f\"   Evaluation templates: {EVALUATION_TEMPLATES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daemon Runner - Simulates continuous production traffic\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class EvaluationItem:\n",
    "    \"\"\"Single evaluation dataset row.\"\"\"\n",
    "    query: str\n",
    "    context: str = \"\"\n",
    "    ground_truth: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Result of a single evaluation.\"\"\"\n",
    "    template_id: str\n",
    "    agent_name: str\n",
    "    query: str\n",
    "    response: str\n",
    "    success: bool\n",
    "    latency_ms: float\n",
    "\n",
    "class DaemonRunner:\n",
    "    \"\"\"Simple daemon runner for agent fleet simulation with optional evaluations.\"\"\"\n",
    "    \n",
    "    def __init__(self, agents: List[Dict], config: DaemonConfig, templates_dir: str = None):\n",
    "        self.agents = agents\n",
    "        self.config = config\n",
    "        self.metrics = DaemonMetrics()\n",
    "        self._stop_requested = False\n",
    "        self._lock = threading.Lock()\n",
    "        self.templates_dir = Path(templates_dir) if templates_dir else Path(\"evaluation_templates\")\n",
    "        self.evaluation_results: List[EvaluationResult] = []\n",
    "    \n",
    "    def _load_evaluation_templates(self) -> List[Dict]:\n",
    "        \"\"\"Load evaluation templates from YAML files.\"\"\"\n",
    "        templates = []\n",
    "        if not self.templates_dir.exists():\n",
    "            print(f\"   ‚ö†Ô∏è Templates dir not found: {self.templates_dir}\")\n",
    "            return templates\n",
    "        \n",
    "        for yaml_file in self.templates_dir.glob(\"*.yaml\"):\n",
    "            try:\n",
    "                with open(yaml_file, 'r', encoding='utf-8') as f:\n",
    "                    template = yaml.safe_load(f)\n",
    "                    templates.append(template)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to load {yaml_file.name}: {e}\")\n",
    "        return templates\n",
    "    \n",
    "    def _call_agent(self, agent: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single agent call and return metrics.\"\"\"\n",
    "        agent_type = agent.get(\"type\", \"GeneralAssistant\")\n",
    "        query = random.choice(QUERY_TEMPLATES.get(agent_type, QUERY_TEMPLATES[\"GeneralAssistant\"]))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        success = False\n",
    "        error_msg = None\n",
    "        \n",
    "        try:\n",
    "            with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as client:\n",
    "                openai_client = client.get_openai_client()\n",
    "                conversation = openai_client.conversations.create()\n",
    "                response = openai_client.responses.create(\n",
    "                    conversation=conversation.id,\n",
    "                    extra_body={\"agent\": {\"name\": agent[\"name\"], \"type\": \"agent_reference\"}},\n",
    "                    input=query,\n",
    "                )\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:50]\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        with self._lock:\n",
    "            self.metrics.total_calls += 1\n",
    "            if success:\n",
    "                self.metrics.successful_calls += 1\n",
    "                self.metrics.total_latency_ms += latency_ms\n",
    "            else:\n",
    "                self.metrics.failed_calls += 1\n",
    "        \n",
    "        return {\"success\": success, \"latency_ms\": latency_ms, \"agent\": agent[\"name\"], \"error\": error_msg}\n",
    "    \n",
    "    def _run_evaluation(self, template: Dict, agent: Dict) -> EvaluationResult:\n",
    "        \"\"\"Run a single evaluation against an agent.\"\"\"\n",
    "        dataset_items = template.get(\"dataset\", {}).get(\"items\", [])\n",
    "        if not dataset_items:\n",
    "            return None\n",
    "        \n",
    "        item = random.choice(dataset_items)\n",
    "        query = item.get(\"query\", \"\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        success = False\n",
    "        response_text = \"\"\n",
    "        \n",
    "        try:\n",
    "            with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as client:\n",
    "                openai_client = client.get_openai_client()\n",
    "                conversation = openai_client.conversations.create()\n",
    "                response = openai_client.responses.create(\n",
    "                    conversation=conversation.id,\n",
    "                    extra_body={\"agent\": {\"name\": agent[\"name\"], \"type\": \"agent_reference\"}},\n",
    "                    input=query,\n",
    "                )\n",
    "                response_text = getattr(response, 'output_text', str(response))[:200]\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            response_text = f\"Error: {str(e)[:50]}\"\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        with self._lock:\n",
    "            self.metrics.evaluation_runs += 1\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            template_id=template.get(\"id\", \"unknown\"),\n",
    "            agent_name=agent[\"name\"],\n",
    "            query=query[:50],\n",
    "            response=response_text[:100],\n",
    "            success=success,\n",
    "            latency_ms=latency_ms,\n",
    "        )\n",
    "    \n",
    "    def _run_batch(self, batch_size: int):\n",
    "        \"\"\"Run a batch of agent calls in parallel.\"\"\"\n",
    "        threads = []\n",
    "        for _ in range(batch_size):\n",
    "            agent = random.choice(self.agents)\n",
    "            t = threading.Thread(target=self._call_agent, args=(agent,))\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "    \n",
    "    def _run_evaluations_batch(self, templates: List[Dict], count: int):\n",
    "        \"\"\"Run evaluation batch.\"\"\"\n",
    "        print(f\"\\n   üß™ Running {count} evaluations...\")\n",
    "        for i in range(count):\n",
    "            template = random.choice(templates)\n",
    "            agent = random.choice(self.agents)\n",
    "            result = self._run_evaluation(template, agent)\n",
    "            if result:\n",
    "                self.evaluation_results.append(result)\n",
    "                status = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "                print(f\"      {status} [{i+1}/{count}] {template.get('id', 'unknown')} ‚Üí {agent['name'][:20]}...\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the daemon simulation loop.\"\"\"\n",
    "        self.metrics.start_time = datetime.now()\n",
    "        end_time = time.time() + self.config.duration_seconds\n",
    "        batch_num = 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üöÄ Starting Daemon Simulation\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Load evaluation templates if enabled\n",
    "        templates = []\n",
    "        if self.config.run_evaluations:\n",
    "            templates = self._load_evaluation_templates()\n",
    "            print(f\"   üìã Loaded {len(templates)} evaluation templates\")\n",
    "        \n",
    "        while time.time() < end_time and not self._stop_requested:\n",
    "            batch_num += 1\n",
    "            batch_size = random.randint(self.config.calls_per_batch_min, self.config.calls_per_batch_max)\n",
    "            \n",
    "            self._run_batch(batch_size)\n",
    "            \n",
    "            # Print live metrics\n",
    "            print(f\"\\r   üìä Batch {batch_num:03d} | \"\n",
    "                  f\"Calls: {self.metrics.total_calls} | \"\n",
    "                  f\"Success: {self.metrics.success_rate:.1f}% | \"\n",
    "                  f\"Avg Latency: {self.metrics.avg_latency_ms:.0f}ms | \"\n",
    "                  f\"Runtime: {self.metrics.runtime}\", end=\"\", flush=True)\n",
    "            \n",
    "            time.sleep(self.config.interval_seconds)\n",
    "        \n",
    "        # Run evaluations at the end if enabled\n",
    "        if self.config.run_evaluations and templates:\n",
    "            self._run_evaluations_batch(templates, self.config.evaluation_count)\n",
    "        \n",
    "        print(f\"\\n\\n‚úÖ Daemon simulation completed!\")\n",
    "        return self.metrics\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the daemon gracefully.\"\"\"\n",
    "        self._stop_requested = True\n",
    "\n",
    "print(\"‚úÖ DaemonRunner class defined (with evaluation support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7542837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the daemon simulation with evaluations\n",
    "# Set RUN_EVALUATIONS=True to run 10 sample evaluations after simulation\n",
    "\n",
    "RUN_EVALUATIONS = True  # Toggle to enable/disable evaluations\n",
    "\n",
    "if not created_agents:\n",
    "    print(\"‚ö†Ô∏è No agents created. Run Part 1 first.\")\n",
    "else:\n",
    "    config = DaemonConfig(\n",
    "        interval_seconds=5.0,\n",
    "        calls_per_batch_min=2,\n",
    "        calls_per_batch_max=4,\n",
    "        threads=3,\n",
    "        duration_seconds=60,        # 1 minute simulation\n",
    "        run_evaluations=RUN_EVALUATIONS,\n",
    "        evaluation_count=10,        # Run 10 evaluations\n",
    "    )\n",
    "    \n",
    "    # Use the evaluation templates directory\n",
    "    templates_dir = \"./evaluation_templates\"\n",
    "    \n",
    "    daemon = DaemonRunner(agents=created_agents, config=config, templates_dir=templates_dir)\n",
    "    final_metrics = daemon.run()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä Final Simulation Metrics\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total Calls:      {final_metrics.total_calls}\")\n",
    "    print(f\"   Successful:       {final_metrics.successful_calls}\")\n",
    "    print(f\"   Failed:           {final_metrics.failed_calls}\")\n",
    "    print(f\"   Success Rate:     {final_metrics.success_rate:.1f}%\")\n",
    "    print(f\"   Avg Latency:      {final_metrics.avg_latency_ms:.0f}ms\")\n",
    "    print(f\"   Total Runtime:    {final_metrics.runtime}\")\n",
    "    \n",
    "    if RUN_EVALUATIONS:\n",
    "        print(f\"\\nüß™ Evaluation Results:\")\n",
    "        print(f\"   Evaluation Runs:  {final_metrics.evaluation_runs}\")\n",
    "        print(f\"   Templates Used:   4 (basic-relevance, format-check, response-completeness, semantic-similarity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e9dfb",
   "metadata": {},
   "source": [
    "## Part 4: Sample Evaluations\n",
    "\n",
    "The daemon simulation can optionally run **sample evaluations** using YAML-based templates. These evaluations test agent responses for quality metrics.\n",
    "\n",
    "### Evaluation Templates\n",
    "\n",
    "| Template | Description | Evaluator |\n",
    "|----------|-------------|-----------|\n",
    "| `basic-relevance.yaml` | Checks response relevance to query | `builtin.relevance` |\n",
    "| `format-check.yaml` | Validates response format | `string_check` |\n",
    "| `response-completeness.yaml` | Checks if response is complete | `builtin.response_completeness` |\n",
    "| `semantic-similarity.yaml` | Measures semantic similarity | `builtin.similarity` |\n",
    "\n",
    "### How to Enable Evaluations\n",
    "\n",
    "Set `RUN_EVALUATIONS = True` in the daemon run cell to enable evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View evaluation results from daemon run\n",
    "if daemon and daemon.evaluation_results:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üß™ Detailed Evaluation Results\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, result in enumerate(daemon.evaluation_results, 1):\n",
    "        status = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "        print(f\"\\n{i}. {status} Template: {result.template_id}\")\n",
    "        print(f\"   Agent: {result.agent_name}\")\n",
    "        print(f\"   Query: {result.query}...\")\n",
    "        print(f\"   Response: {result.response[:80]}...\")\n",
    "        print(f\"   Latency: {result.latency_ms:.0f}ms\")\n",
    "else:\n",
    "    print(\"No evaluation results available. Run daemon with RUN_EVALUATIONS=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedbf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all agents created in this session for portal verification\n",
    "print(\"=\" * 70)\n",
    "print(\"üìã Agents Created for Portal Verification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with AIProjectClient(endpoint=AZURE_AI_PROJECT_ENDPOINT, credential=credential) as client:\n",
    "    agents = list(client.agents.list())\n",
    "    fleet_agents = [a for a in agents if a.name.startswith(\"fleet-\")]\n",
    "    \n",
    "    print(f\"\\nüîç Found {len(fleet_agents)} fleet agents in project:\\n\")\n",
    "    for i, agent in enumerate(fleet_agents[:10], 1):\n",
    "        print(f\"   {i:02d}. {agent.name} (id: {agent.id[:20]}...)\")\n",
    "    \n",
    "    if len(fleet_agents) > 10:\n",
    "        print(f\"   ... and {len(fleet_agents) - 10} more\")\n",
    "\n",
    "print(f\"\\nüí° View these agents in Portal: {AZURE_AI_PROJECT_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93c5d0",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Batch Agent Registration** | Created 10 agents with different roles in a single loop |\n",
    "| **Workflow Registration** | Defined 10 workflows for multi-agent orchestration |\n",
    "| **Daemon Simulation** | Ran continuous traffic simulation with live metrics |\n",
    "| **Sample Evaluations** | Ran 10 evaluations using YAML templates (when enabled) |\n",
    "| **Portal Integration** | Agents visible in Azure AI Foundry for monitoring |\n",
    "\n",
    "### Metrics Summary\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `total_calls` | Total API calls made during simulation |\n",
    "| `success_rate` | Percentage of successful calls |\n",
    "| `avg_latency_ms` | Average response time in milliseconds |\n",
    "| `evaluation_runs` | Number of evaluations executed (if enabled) |\n",
    "| `runtime` | Total simulation duration |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Enable Evaluations**: Set `RUN_EVALUATIONS = True` to run sample evaluations\n",
    "2. **Increase Duration**: Change `duration_seconds` for longer simulations\n",
    "3. **Add Tracing**: Connect Application Insights (see `1_foundry_agent_monitoring.ipynb`)\n",
    "4. **Scale Up**: Increase `calls_per_batch_max` for higher load testing\n",
    "5. **View in Portal**: Navigate to Azure AI Foundry to see live traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49b80c",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-services/agents/)\n",
    "- [Agent Fleet Simulation Reference](https://github.com/guming3d/AI-Foundry-Agent-Simulation)\n",
    "- [Azure AI Projects SDK](https://learn.microsoft.com/en-us/python/api/azure-ai-projects/)\n",
    "- [Application Insights for Tracing](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-operator-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
